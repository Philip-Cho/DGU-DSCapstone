{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61cf196e",
   "metadata": {},
   "source": [
    "str to dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a2d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ed8f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/lynn1/OneDrive/바탕 화면/Capstone/data/arxiv-dataset/val.txt') as f:\n",
    "#     data = []\n",
    "    lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2d09894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['article_id', 'article_text', 'abstract_text', 'labels', 'section_names', 'sections'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d980de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the interest in anchoring phenomena and phenomena in confined nematic liquid crystals has largely been driven by their potential use in liquid crystal display devices .',\n",
       " 'the twisted nematic liquid crystal cell serves as an example .',\n",
       " 'it consists of a nematic liquid crystal confined between two parallel walls , both providing homogeneous planar anchoring but with mutually perpendicular easy directions . in this case',\n",
       " 'the orientation of the nematic director is tuned by the application of an external electric or magnetic field .',\n",
       " 'a precise control of the surface alignment extending over large areas is decisive for the functioning of such devices .',\n",
       " 'most studies have focused on nematic liquid crystals in contact with laterally uniform substrates . on the other hand substrate inhomogeneities',\n",
       " 'arise rather naturally as a result of surface treatments such as rubbing .',\n",
       " 'thus the nematic texture near the surface is in fact non - uniform .',\n",
       " 'this non - uniformity , however , is smeared out beyond a decay length proportional to the periodicity of the surface pattern .',\n",
       " 'very often the thickness of the non - uniform surface layer is considerably smaller than both the wavelength of visible light and the thickness of the nematic cell , i.e. , the distance between the two confining parallel walls',\n",
       " '. hence optical properties of the nematic liquid crystal confined between such substrates correspond to those resulting from effective , uniform substrates .',\n",
       " 'more recent developments have demonstrated that surfaces patterned with a large periodicity of some micrometers are of considerable interest from a technological point of view ( see , e.g. , ref .',\n",
       " '@xcite and references therein ) .',\n",
       " 'a new generation of electro - optical devices relies on nematic liquid crystals with patterned orientation of the nematic director over small areas which can be achieved by chemically patterning the confining surfaces .',\n",
       " 'for example , to produce flat - panel displays with wide viewing angles one can use pixels that are divided into sub - pixels , where each sub - pixel is defined by a different orientation of the nematic director , which is induced by the surface structure and subsequently tuned by the electric field .',\n",
       " 'in addition to the technological relevance , nematic liquid crystals in contact with non - uniform substrates provide the opportunity to study basic phenomena such as effective elastic forces between the substrates and phase transitions between various competing nematic textures ( see , e.g. , ref .',\n",
       " '@xcite and references therein ) .    whereas the influence of homogeneous confining substrates on nematic liquid crystals is now well understood ,',\n",
       " 'the phase behavior of nematic liquid crystals in contact with chemically or geometrically patterned substrates is still debated .',\n",
       " 'one might suppose that theoretical calculations based on continuum theories should resolve the properties of nematic liquid crystals in contact with patterned substrates @xcite .',\n",
       " 'however , such calculations are numerically demanding because two- or three - dimensional grids have to be used because of the broken symmetry due to the surface pattern .',\n",
       " 'moreover , it is very challenging to determine metastable states and energy barriers between them which are important for the understanding of bistable nematic devices @xcite . in the present paper',\n",
       " 'we adopt a different strategy which takes the advantage of the finite decay length characterizing the influence of the surface pattern on the nematic liquid crystal in the direction perpendicular to the substrate .',\n",
       " 'we determine first an anchoring energy function and an average surface director orientation of the patterned substrate and obtain an effective free energy for the nematic liquid crystal cell under consideration .',\n",
       " 'we find remarkably good agreement between the phase diagrams of various systems calculated using this effective free energy function on the one hand and the original free energy functional on the other hand .',\n",
       " 'the continuum theory for liquid crystals has its origin dating back to at least the work of oseen @xcite and zocher @xcite .',\n",
       " 'this early version of the continuum theory for nematic liquid crystals played an important role for the further development of the static theory and its more direct formulation by frank @xcite .',\n",
       " 'the frank theory is formulated in terms of the so - called nematic director @xmath0 , @xmath1 , and its possible spatial distortions .',\n",
       " 'the nematic director describes the direction of the locally averaged molecular alignment in liquid crystals . in a nematic liquid crystal',\n",
       " 'the centers of mass of the liquid crystal molecules do not exhibit long - ranged order .',\n",
       " 'the molecules can translate freely while being aligned , on average , parallel to one another and to the nematic director .',\n",
       " 'it is known that if an initially uniform nematic liquid crystal is distorted by external forces , it relaxes back to the uniform state after the disturbing influence is switched off , signaling that the uniform configuration represents a thermodynamically stable state .',\n",
       " 'therefore it is assumed that there is a cost in free energy associated with elastic distortions of the nematic director of the form @xmath2&=&\\\\frac{1}{2}\\\\int_v',\n",
       " 'd^3 r\\\\,\\\\left[k_{11}\\\\left(\\\\nabla\\\\cdot\\\\hat{{\\\\bf n}}\\\\right)^2\\\\right.\\\\nonumber \\\\\\\\&+&\\\\!\\\\!\\\\!\\\\left.k_{22}(\\\\hat{{\\\\bf n}}\\\\cdot(\\\\nabla \\\\times\\\\hat{{\\\\bf n}}))^2 + k_{33}(\\\\hat{{\\\\bf n}}\\\\times(\\\\nabla \\\\times \\\\hat{{\\\\bf n}}))^2\\\\right],\\\\nonumber \\\\\\\\ & & \\\\label{eq1}\\\\end{aligned}\\\\ ] ] where @xmath3 is the volume accessible to the nematic liquid crystal and @xmath4 , @xmath5 , and @xmath6 are elastic constants associated with splay , twist , and bend distortions , respectively .',\n",
       " 'the elastic constants depend on temperature and are commonly of the order @xmath7 to . sometimes , for example , when the relative values of the elastic constants are unknown or when the resulting euler - lagrange equations are complicated , the one - constant approximation @xmath8 is made . in this case',\n",
       " 'the elastic free energy functional reduces to @xmath9&=&\\\\frac{k}{2}\\\\int_v d^3 r\\\\,\\\\left(\\\\nabla\\\\hat{{\\\\bf n}}\\\\right)^2\\\\,.\\\\end{aligned}\\\\ ] ] in the presence of surfaces the bulk free energy @xmath10 must be supplemented by the surface free energy @xmath11 such that the total free energy is given by @xmath12 . in the corresponding equilibrium euler - lagrange equations @xmath13 , @xmath11 leads to appropriate boundary conditions .',\n",
       " 'the description of the nematic director alignment at the surfaces forming the boundaries is called anchoring .',\n",
       " 'in addition to the so - called free boundary condition where there is no anchoring , one considers weak and strong anchoring . if there are no anchoring conditions imposed on @xmath14 at the boundary , the bulk free energy @xmath15 is minimized using standard techniques of the calculus of variations . in the case of strong anchoring it',\n",
       " 'is also sufficient to minimize the bulk free energy but subject to @xmath14 taking prescribed fixed values at the boundary . in the case of weak',\n",
       " 'anchoring the total free energy @xmath16 , which includes the surface free energy @xmath11 , has to be minimized .',\n",
       " 'the most commonly used expression for the surface free energy is of the form proposed by rapini and papoular by @xcite : @xmath17=\\\\frac{1}{2}\\\\int_{s } d^2 r\\\\,w({\\\\bf r } ) ( \\\\hat{{\\\\bf n}}\\\\cdot{\\\\nu})^2.\\\\ ] ] the integral runs over the boundary and @xmath18 is the corresponding anchoring strength that characterizes the surface .',\n",
       " 'the local unit vector perpendicular to the surface is denoted as @xmath14 . for negative @xmath19',\n",
       " ', this contribution favors an orientation of the molecules perpendicular to the surface , while positive @xmath19 favor degenerate planar orientations at the surface .',\n",
       " 'the absolute value of the anchoring strength is commonly of the order @xmath20 to .',\n",
       " 'here we consider a nematic liquid crystal confined between a patterned substrate at @xmath21 and a flat substrate at @xmath22 where the @xmath23 axis is normal to the flat substrate . as figs .',\n",
       " '[ fig1 ] ( a ) , ( c ) , and ( d ) illustrate , the lower substrate is characterized by geometrical and/or chemical patterns of periodicity @xmath24 along the @xmath25 axis . moreover , the system is translationally invariant in the @xmath26 direction . within the one - constant approximation [ eq .  ( [ eq2 ] ) ] the total free energy functional is given by @xmath27=}\\\\nonumber \\\\\\\\&&\\\\frac{kl}{2}\\\\int\\\\limits_0^p dx\\\\ , \\\\int\\\\limits_{z_0(x)}^d dz\\\\ , [ \\\\nabla\\\\theta(x , z)]^2+f_s[\\\\theta(x , z_0(x))]\\\\ , , \\\\label{eq4}\\\\end{aligned}\\\\ ] ] where @xmath28 is the extension of the system in @xmath26 direction , @xmath29 is the surface profile of the patterned substrate , and @xmath30 . at the upper surface',\n",
       " 'strong anchoring @xmath31 is imposed .',\n",
       " 'twist is not considered , i.e. , @xmath32 .',\n",
       " 'of course , the analysis can be straightforwardly extended to the case of different splay and bend constants @xmath4 and @xmath6 , respectively , but this aspect of the problem is important only if the analysis is supposed to yield quantitative results for a specific nematic liquid crystal .',\n",
       " 'the surface contribution @xmath33 $ ] includes the anchoring energy for which the rapini - papoular form [ eq .  ( [ eq3 ] ) ] is adopted :    .',\n",
       " 'the upper flat substrate induces strong anchoring , i.e. , @xmath31 , while the lower substrate is characterized by a surface pattern of period @xmath24 in @xmath25 direction .',\n",
       " 'the system is translational invariant in @xmath26 direction perpendicular to the plane of the figure . in ( a ) the lower sinusoidally grating surface ( groove depth @xmath34 )',\n",
       " 'is endowed with an alternating stripe pattern of locally homeotropic anchoring ( white bars ) and homogeneous planar anchoring ( black bars ) .',\n",
       " 'the period of the chemical pattern is half the period of the surface grating @xmath24 . in ( c ) and',\n",
       " '( d ) a pure geometrically structured lower substrate and pure chemically patterned lower substrate , respectively , are shown .',\n",
       " 'the anchoring direction at the substrates is schematically represented by black rods .',\n",
       " 'quantitatively reliable predictions of the phase behavior of a nematic liquid crystal confined between two substrates at an _ arbitrary _ mean distance @xmath35 in ( a ) can be achieved if the effective free energy function [ eqs .',\n",
       " '( [ eq6 ] ) - ( [ eq8 ] ) ] is analyzed . to this end',\n",
       " \"the free energy functional of the nematic liquid crystal confined between two substrates at a _ single _ and rather small mean distance @xmath36 in ( b ) has to be minimized as is discussed in the main text.,width=302 ]    @xmath37=}\\\\nonumber \\\\\\\\&&\\\\!\\\\!\\\\!\\\\!\\\\!\\\\frac{l}{2}\\\\int^p_0 dx\\\\ , w(x)\\\\frac{\\\\left(-\\\\sin(\\\\theta_0(x))z'_0(x)+\\\\cos(\\\\theta_0(x))\\\\right)^2 } { \\\\sqrt{1+(z'_0(x))^2}},\\\\end{aligned}\\\\ ] ]    where @xmath38 .\",\n",
       " 'equations ( [ eq4 ] ) and ( [ eq5 ] ) together with the boundary condition for the nematic director at the upper surface completely specify the free energy functional for the system under consideration .',\n",
       " 'the euler - langrange equations resulting from the stationary conditions of the total free energy with respect to the nematic director field can be solved numerically on a sufficiently fine two - dimensional grid using an iterative method . in order to obtain both stable and metastable configurations , different types of initial configurations',\n",
       " 'however , due to the pattern of the lower surface the determination of the director field and the phase diagram turns out to be a challenging numerical problem in particular in the case of large cell widths @xmath35 .',\n",
       " 'moreover , the energy barrier between two metastable states can not be determined this way .      here',\n",
       " 'we map the free energy functional @xmath39 $ ] [ eq .',\n",
       " '( [ eq4 ] ) ] of a nematic liquid crystal cell with _ arbitrary _',\n",
       " 'width @xmath35 and arbitrary anchoring angle @xmath40 at the upper surface ( see fig .',\n",
       " '[ fig1 ]  ( a ) ) onto the effective free energy function @xmath41 where the average surface director orientation @xmath42 @xcite at the lower patterned surface is given by @xmath43 the effective surface free energy function @xmath44 characterizing the anchoring energy at the patterned surface can be written as @xmath45 in order to calculate @xmath42 and @xmath44 explicitly , we first determine numerically the minimum of the free energy @xmath46 of the nematic liquid crystal cell for a _ single _ and rather small value @xmath47 and arbitrary anchoring angle @xmath48 at the upper surface ( see fig .',\n",
       " '[ fig1 ]  ( b ) ) .',\n",
       " 'thereafter the phase behavior , energy barriers between metastable states , and effective anchoring angles for the system of interest ( fig .',\n",
       " '[ fig1 ]  ( a ) ) can be obtained for _ arbitrary _ values of @xmath35 and @xmath40 from @xmath49 as a function of the single variable @xmath42 .',\n",
       " 'such a calculation is considerably less challenging than minimizing the original free energy functional @xmath39 $ ] with respect to @xmath50 on a two - dimensional @xmath51 grid .',\n",
       " 'however , the effective free energy method is applicable only if eq .',\n",
       " '( [ eq7 ] ) can be inverted in order to obtain @xmath52 which is needed as input into eq .',\n",
       " '( [ eq8 ] ) .',\n",
       " 'the condition for this inversion follows from eq .',\n",
       " '( [ eq7 ] ) : @xmath53 moreover , @xmath54 is practically independent of the cell width @xmath36 provided @xmath55 implying that the interfacial region above the lower substrate does not extend to the upper substrate .    before studying the nematic liquid crystal in contact with the patterned substrates shown in fig .  [ fig1 ]',\n",
       " 'it is instructive to analyze first the nematic liquid crystal confined between two homogeneous flat substrates at a distance @xmath35 .',\n",
       " 'the upper surface induces strong anchoring , i.e. , .',\n",
       " 'the free energy functional defined in eq .',\n",
       " '( [ eq4 ] ) follows as @xmath56&\\\\!\\\\!=\\\\!\\\\!&\\\\frac{klp}{2}\\\\int\\\\limits_0^d dz\\\\ , \\\\left(\\\\frac{d \\\\theta(z)}{d z}\\\\right)^2+f_s(\\\\theta_0).\\\\end{aligned}\\\\ ] ] the solution of the euler - langrange equation @xmath57 subject to the boundary condition at the upper surface @xmath58 interpolates linearly between the top and bottom surfaces :    @xmath59    where @xmath60 follows from the boundary condition at the lower surface . with this solution of the euler - lagrange equation',\n",
       " 'the minimized free energy function reads @xmath61 it follows directly from eqs .',\n",
       " '( [ eq6 ] ) - ( [ eq8 ] ) that @xmath62 , @xmath63 , and @xmath64 hence in the case of homogeneous confining substrates the effective free energy function [ eq .',\n",
       " '( [ eq13 ] ) ] agrees exactly with the minimized free energy function of the original system [ eq .',\n",
       " '( [ eq12 ] ) ] .',\n",
       " 'in the previous section we have shown that we can describe a nematic liquid crystal confined between two homogeneous substrates by the effective free energy function [ eq .  ( [ eq6 ] ) ] . in this subsection',\n",
       " 'we apply this approach to the particular case of a nematic liquid crystal confined between a chemically patterned sinusoidal surface and a flat substrate with strong homeotropic anchoring ( see fig .',\n",
       " '[ fig1 ]  ( a ) ) . the surface profile of the grating surface',\n",
       " 'is given by @xmath65 , where @xmath34 is the groove depth and @xmath66 is the period .',\n",
       " 'as figure [ fig1 ] ( a ) illustrates , the surface exhibits a pattern consisting of alternating stripes with locally homeotropic and homogeneous planar anchoring .',\n",
       " 'the projection of the widths of the stripes onto the @xmath25 axis is @xmath67 and the anchoring strength is specified by a periodic step function : @xmath68 and @xmath69 for values of @xmath25 on the homeotropic and planar stripes , respectively .',\n",
       " 'figure [ fig2 ] ( a ) displays @xmath70 ( dashed line ) and @xmath71 ( solid line ) for @xmath72 , @xmath73 , and @xmath74 . the shapes of @xmath70 as a function of @xmath48 and @xmath71 as a function of @xmath42 are rather similar because @xmath75 ( see eq .',\n",
       " '( [ eq8 ] ) ) for this set of model parameters .',\n",
       " 'figure [ fig2 ] ( b ) displays the phase diagram plotted as a function of the anchoring angle @xmath40 at the upper substrate and the mean separation of the substrates @xmath35 .',\n",
       " 'the calculations demonstrate the existence of two ( stable or metastable ) nematic director configurations : the homeotropic ( h ) phase , in which the director field is almost uniform and parallel to the anchoring direction imposed at the upper surface , i.e. , @xmath76 , and the hybrid aligned nematic ( han ) phase , in which the director field varies from @xmath77 at the upper surface to nearly planar orientation through the cell .',\n",
       " 'note that there are two han textures : han@xmath78 and han@xmath79 corresponding to positive and negative average surface angles at the lower surface ( see also fig .  [',\n",
       " 'fig : diagram ] below ) . for small anchoring angles @xmath40 the han phases',\n",
       " 'are stable provided the cell width is larger than @xmath80 ( more precisely , the han@xmath78 texture is stable for @xmath81 while the han@xmath79 texture is stable for @xmath82 and they coexist at @xmath83 ) . for smaller distances between the substrates',\n",
       " '@xmath84 the han phases are no longer stable because distortions of the director field are too costly in the presence of the dominating strong anchoring at the upper surface . the comparison of the phase boundary of thermal equilibrium as obtained from the effective free energy method [ eqs .',\n",
       " '( [ eq6 ] ) - ( [ eq8 ] ) , and solid line in fig .  [ fig2 ]  ( b ) ] and the direct minimization of the underlying free energy functional [ eqs .',\n",
       " '( [ eq4 ] ) and ( [ eq5 ] ) , and diamonds in fig .  [ fig2 ]  ( b ) ] demonstrate the reliability of the effective free energy method .',\n",
       " '( see fig .',\n",
       " '[ fig1 ]  ( b ) ) .',\n",
       " '@xmath28 is the extension of the cell in the invariant @xmath26 direction , @xmath85 is the isotropic elastic constant , and the anchoring strength on the homeotropic stripes ( white bars in fig .',\n",
       " '[ fig1 ]  ( b ) ) and planar anchoring stripes ( black bars in fig .',\n",
       " '[ fig1 ]  ( b ) ) are @xmath72 and @xmath73 , respectively .',\n",
       " '( b ) phase diagram of the same system as a function of the anchoring angle at the upper flat substrate @xmath40 and the cell width @xmath35 ( see fig .  [ fig1 ]  ( a ) ) .',\n",
       " 'the solid line denotes first order phase transitions between a homeotropic ( h ) and hybrid aligned nematic ( han@xmath78 and han@xmath79 ) phases . at @xmath86 and @xmath87 there is a triple point where the han@xmath78 , han@xmath79 , and h states coexist .',\n",
       " 'the solid circle marks the critical point at @xmath88 and @xmath89 .',\n",
       " 'the limits of metastability of the han@xmath78 ( 1 ) and the h ( 2 ) state are denoted by the dot - dashed lines .',\n",
       " 'the limit of metastability of the h state for @xmath82 and the han@xmath79 state are not shown for clearness .',\n",
       " 'the lines and the solid circle follow from analyzing the effective free energy function [ eqs .',\n",
       " '( [ eq6 ] ) - ( [ eq8 ] ) ] while the diamonds represent the phase boundary of thermal equilibrium as obtained from a direct minimization of the underlying free energy functional [ eqs .',\n",
       " '( [ eq4 ] ) and ( [ eq5])].,width=302 ]     ( see fig .  [ fig1 ]  ( b ) ) . the anchoring strength on the homeotropic stripes and planar anchoring stripes are @xmath90 and @xmath91 , respectively .',\n",
       " '( b ) phase diagram of the same system as a function of the anchoring angle at the upper flat substrate @xmath40 and the cell width @xmath35 ( see fig .',\n",
       " '[ fig1 ]  ( a ) ) .',\n",
       " 'the solid line denotes the first order phase transition between a homogeneous ( h ) and hybrid aligned nematic ( han@xmath78 ) phase .',\n",
       " 'the solid circle marks the critical point at @xmath92 and @xmath93 .',\n",
       " 'the limits of metastability of the han@xmath78 ( 1 ) and the h ( 2 ) state are denoted by the dot - dashed lines .',\n",
       " 'the lines follow from analyzing the effective free energy function [ eqs .',\n",
       " '( [ eq6 ] ) - ( [ eq8 ] ) ] while the diamonds and the solid circle represent the phase boundary and a critical point as obtained from a direct minimization of the underlying free energy functional [ eqs .',\n",
       " '( [ eq4 ] ) and ( [ eq5 ] ) ] . for @xmath94',\n",
       " 'the phase transition as well as the limits of metastability can not be determined using the effective free energy function because @xmath95 is not known in the region close to its maximum . for clearness ,',\n",
       " 'only the phase diagram for positive @xmath40 is shown ( c.f .',\n",
       " '[ fig2]).,width=302 ]    we note that the phase transition between the h and han textures is first order despite the fact that the effective surface free energy favors monostable planar anchoring , i.e. , @xmath44 exhibits only a minimum at @xmath96 in the interval @xmath97 $ ] .',\n",
       " 'a first order phase transition in a nematic liquid crystal device with a monostable anchoring condition on a homogeneous lower substrate has been predicted for the special case @xmath98 in refs .',\n",
       " '@xcite using the empirical expression @xmath99 as input into eq .',\n",
       " '( [ eq13 ] ) . for @xmath100',\n",
       " 'this surface free energy has two minima at @xmath101 and @xmath102 in the interval @xmath103 $ ] , and as such is bistable , while for @xmath104 , only the minimum @xmath101 exists , i.e. , the surface is monostable . to study the stability limit of the h phase',\n",
       " 'we expand @xmath105 in eq .',\n",
       " '( [ eq13 ] ) around @xmath101 up to sixth order : @xmath106 the h phase corresponds to a local minimum of @xmath105 in eq .',\n",
       " '( [ eq13 ] ) if @xmath107 , where @xmath108 a standard bifurcation analysis reveals that the transition from the h phase to the han phase can be either first order or continuous .',\n",
       " 'the transition is continuous if @xmath109 , first order if @xmath110 , and @xmath111 corresponds to a tricritical point . in the case of a first order phase transition',\n",
       " 'the phase boundary of thermal equilibrium is given by @xmath112 we emphasize that the order of the phase transition depends only on the surface free energy @xmath113 close to @xmath101 for @xmath98 .',\n",
       " 'therefore it is possible to have a first order phase transition even with a monostable surface characterized by a monotonic surface free energy such as the one shown in fig .',\n",
       " '[ fig2 ]  ( a ) for @xmath114 $ ] or the empirical equation ( [ eq14 ] ) with @xmath104 as well the more general expression @xcite @xmath115\\\\end{aligned}\\\\ ] ] with appropriate parameters @xmath116 and @xmath117 .',\n",
       " 'first order phase transitions between the h and han texture are of particular interest for bistable liquid crystal displays . in a bistable liquid crystal display',\n",
       " 'the two molecular configurations corresponding to light and dark states are locally stable in the thermodynamic space when the applied voltage is removed @xcite . therefore , power is needed only to switch from one stable state to another , in contrast to monostable liquid crystal displays which require power to switch and to maintain the light and the dark states .',\n",
       " 'we now turn our attention to the case that it is not possible to evaluate @xmath118 from @xmath119 [ eq .  ( [ eq7 ] ) ] because the condition for this inversion is not satisfied [ eq .  ( [ eq9 ] ) ] . to this end',\n",
       " 'we have chosen the parameters @xmath90 , @xmath91 , and @xmath74 for the system shown in fig .',\n",
       " '[ fig1 ]  ( a ) .',\n",
       " 'figure [ fig3 ] ( a ) display ( dashed line ) and ( solid line ) while the corresponding phase diagram is shown in fig .',\n",
       " '[ fig3 ]  ( b ) . as is apparent from the solid line in fig .',\n",
       " '[ fig3 ]  ( a ) it is not possible to determine the effective surface free energy function for the all values of @xmath42 because the upper flat substrate at @xmath36 is too far away from the lower patterned substrate in order to induce all possible average anchoring orientations @xmath42 . in other words , the anchoring energy at the patterned substrate is too large to be balanced by the elastic energy for the chosen mean distance @xmath36 between the substrates ( see fig .',\n",
       " '[ fig1 ]  ( b ) ) . nevertheless fig .',\n",
       " '[ fig3 ]  ( b ) demonstrates that even this partial information about the effective surface free energy function can be used to calculate the phase diagram for cell widths sufficiently larger than the width at the critical point @xmath120 .      in the last subsection we have shown that with a suitable chemical and geometrical surface morphology on one of the interior surfaces of a liquid crystal cell',\n",
       " ', two stable nematic director configurations can be supported .',\n",
       " 'the zenithally bistable nematic devices that have been studied recently @xcite consist of a nematic liquid crystal confined between a chemically homogeneous grating surface ( see fig .',\n",
       " '[ fig1 ]  ( c ) ) and a flat substrate with strong homeotropic anchoring .',\n",
       " \"the profile of the asymmetric surface grating is given by @xmath121 where @xmath34 is the groove depth , @xmath66 the period , and @xmath122 is the `` blazing '' parameter describing the asymmetry of the surface profile\",\n",
       " '. such a grating surface has been studied by brown _',\n",
       " '@xcite who found a first order transition between the han state , characterized by a low pretilt angle ( @xmath123 ) , and the h state , characterized by a high pretilt angle ( @xmath124 ) . strictly speaking',\n",
       " ', the h state does not correspond to the homeotropic texture ( see fig .',\n",
       " '[ fig5 ]  ( b ) below ) , but we keep the same notation as in the previous section for consistency . here',\n",
       " 'we study phase transitions of a nematic liquid crystal in contact with the blazed surface in a more detail using the effective free energy method discussed in sec .',\n",
       " 'ii  c.    in fig .',\n",
       " '[ fig : blazed ]  ( a ) the minimized free energy @xmath70 ( dashed line ) and the calculated effective surface free energy @xmath71 ( solid line ) are shown for the anchoring strength @xmath125 on the grating surface , the groove depth @xmath126 , and the blazing parameter @xmath127 .',\n",
       " 'the effective surface free energy is asymmetric with respect to @xmath128 because of the asymmetry of the grating surface . as a consequence',\n",
       " 'also the phase diagram , plotted as a function of the anchoring angle on the upper surface @xmath40 and the distance @xmath129 is asymmetric ( fig .  [',\n",
       " 'fig : blazed ]  ( b ) ) .',\n",
       " 'however , the topology of the phase diagram is the same as in the case of a symmetric substrate ( see figs .',\n",
       " '[ fig2 ] ( b ) , [ fig3 ] ( b ) , and for a more general discussion sec .',\n",
       " 'iii  c below ) .',\n",
       " '( c ) and eq .',\n",
       " '( [ eq : blazed ] ) ) .',\n",
       " 'the locally homeotropic anchoring strength on the blazed surface is @xmath130 , the groove depth is @xmath131 and the blazing parameter is @xmath127 .',\n",
       " '@xmath28 is the extension of the cell in the invariant @xmath26 direction and @xmath85 is the isotropic elastic constant .',\n",
       " 'the effective surface free energy ( as well as ) is periodic with the period @xmath132 but it is asymmetric with respect to @xmath133 .',\n",
       " '( b ) phase diagram of the same system as a function of the anchoring angle at the upper flat substrate @xmath40 and the cell width @xmath35 with the same line code as in figs .',\n",
       " '[ fig2 ] and [ fig3 ] .',\n",
       " 'the triple point ( where the han@xmath78 , han@xmath79 and h phases coexist ) is at @xmath134 and @xmath135 , and the critical points ( solid circle ) are at @xmath136 , @xmath137 and @xmath138 , @xmath139',\n",
       " '. for clearness , only the limits of metastability of the han@xmath79 ( 1 ) phase and the h ( 2 ) phase ( for negative @xmath40 ) are shown.,width=302 ]    ( c ) and eq .',\n",
       " '( [ eq : blazed ] ) ) with local homeotropic anchoring and a flat surface with strong homeotropic anchoring ( @xmath98 ) as a function of the groove depth @xmath140 and the cell width @xmath129 .',\n",
       " 'the anchoring strength on the blazed surface is @xmath130 .',\n",
       " 'the lines correspond to different values of @xmath122 and denote first order transitions between homeotropic ( h ) and hybrid aligned ( han@xmath79 ) phases . for small @xmath140 , the lines extend to @xmath141 corresponding to a first order _ anchoring _ transition between planar and homeotropic phases . upon increasing',\n",
       " '@xmath140 the first order transition lines end at critical points which are not shown in the figure .',\n",
       " '( b ) the energy barrier at the first order transitions with the same line code as in ( a ) .',\n",
       " 'the lines have been obtained from the total effective free energy ( see eq .',\n",
       " '( [ eq6 ] ) ) , while the diamonds correspond to the energy barriers between the planar and homeotropic effective anchoring which follow from considering only the surface contribution @xmath44.,width=302 ]    ) ) for the hybrid aligned ( han@xmath79 ) phase in ( a ) and the homeotropic ( h ) phase in ( b ) on the lines of the first order transitions ( see fig .  [ fig4 ] ) for a nematic liquid crystal confined between the blazed surface ( see fig .  [ fig1 ]  ( c ) and eq .',\n",
       " '( [ eq : blazed ] ) ) with local homeotropic anchoring and a flat surface with strong homeotropic anchoring ( @xmath98 ) .',\n",
       " 'the diamonds in ( a ) and ( b ) correspond to the two minima of the effective surface energy function @xmath142 .',\n",
       " 'the model parameters and the line code are the same as in fig .',\n",
       " ', width=302 ]    we now concentrate on the most interesting ( from a practical point of view ) case of strong homeotropic anchoring ( @xmath86 ) at the upper homogeneous surface .',\n",
       " 'figure [ fig4 ] ( a ) displays the phase diagram for a few values of the blazing parameter @xmath122 and a fixed value of the local homeotropic anchoring strength on the grating surface @xmath130 . for a fixed cell width @xmath129 , asymmetry ( @xmath143 ) leads to a decrease of the groove depth @xmath140 at which there is a first order transition between the han and the h phases , as compared to the nematic liquid crystal cell with the symmetric surface grating ( @xmath144 ) . upon increasing the groove depth',\n",
       " '@xmath140 the transition line ends at a critical point ( not shown in the figure ) , while it diverges as @xmath145 .',\n",
       " 'the groove depth @xmath146 corresponds to an _ anchoring _ ( or surface ) transition between low tilt and high tilt _ surface _',\n",
       " 'states which are the homeotropic and planar effective anchoring states , respectively , in the case @xmath144 .',\n",
       " 'the effective free energy method allows one to calculate an energy barrier @xmath147 between the two bistable states , which is _ not _',\n",
       " 'feasible by the direct numerical minimization of the free energy functional .',\n",
       " 'the results of our calculations are shown in fig .',\n",
       " '[ fig4 ]  ( b ) .',\n",
       " 'the asymmetry of the surface grating leads to a decrease of the energy barrier . with increasing groove depth @xmath140 the energy barrier decreases and eventually vanishes upon approaching the critical point .',\n",
       " 'the diamonds in fig .',\n",
       " '[ fig4 ]  ( b ) denote the energy barriers for the abovementioned anchoring transitions of a nematic liquid crystal in contact with a single grating surface .',\n",
       " 'the energy barrier between two bistable states is an important quantity for the design of a zenithally bistable nematic device .',\n",
       " 'too small energy barrier , as compared to @xmath148 , would cause spontaneous switching between the two states because of thermal fluctuations , while enlarging the energy barrier leads to an increase of the power consumption . using the calculated values of @xmath147 ( see fig .',\n",
       " '[ fig4 ]  ( b ) ) one can estimate the energy barrier in a real nematic liquid crystal cell .',\n",
       " 'for instance , for a cell of area @xmath149 and of width @xmath150 , and taking the typical values @xmath151 and @xmath152 , one obtains @xmath153 for @xmath154 and @xmath155 , which seems to be an acceptable value .',\n",
       " 'another important quantity in zenithally bistable nematic devices is the average director orientation at the grating surface in the two degenerate states .',\n",
       " 'the average surface director in the han ( @xmath156 ) and h ( @xmath157 ) states is shown in fig .',\n",
       " '[ fig5 ] for the same model parameters as in fig .',\n",
       " '[ fig4 ] and for the values of @xmath140 and @xmath129 corresponding to the coexistence line .',\n",
       " 'the asymmetry of the surface grating leads to a decrease of @xmath156 and an increase of @xmath158 . for a fixed value of @xmath122',\n",
       " ', the difference between the two angles decreases with increasing the groove depth @xmath140 and finally vanishes upon approaching the critical point ( not shown in the figure ) .',\n",
       " 'hence the asymmetry of the surface grating leads to a decrease of the groove depth at which the bistability is observed , which improves optical properties @xcite , and to a decrease of the energy barrier , which lowers the power consumption of a zenithally bistable nematic device . on the other hand ,',\n",
       " 'also the difference between the two bistable states decreases which impairs optical properties of such a device .',\n",
       " 'in section [ iiia ] we have discussed phase diagrams in the @xmath159 plane which can be described in terms of the surface free energy given by eq .  .',\n",
       " 'however , it is instructive to consider a more general situation that the surface free energy follows from a truncation of the fourier expansion given in eq .  .',\n",
       " 'to be able to study both symmetric and asymmetric surfaces we assume a natural generalization of eq .',\n",
       " ', namely @xmath160\\\\,,\\\\end{aligned}\\\\ ] ] which reduces to eq .   in the case of a symmetric surface characterized by @xmath161 .',\n",
       " 'the angle @xmath162 that minimizes @xmath163 ( see eq .  ) is a function of @xmath40 and @xmath35 .',\n",
       " 'the derivative @xmath164 is the susceptibility of the system that diverges at the critical thickness @xmath165 and remains finite and positive for @xmath107 . from eq .   the conditions for the critical angle @xmath166 follow as @xmath167 implying that @xmath120 and @xmath166 depend only on the form of @xmath113 .',\n",
       " 'plane for the surface free energy given by eq . .',\n",
       " '@xmath168 and @xmath169 denote different ( usually non - uniform ) textures , the thick lines correspond to first order transitions , and black circles mark critical points .',\n",
       " 'when @xmath11 ( defined on the unit circle @xmath170 ) has one minimum and one maximum the phase diagram can be either of type ( a ) or ( b ) .',\n",
       " 'when @xmath11 has two minima and two maxima the phase diagram is of type ( b ) unless the minima are of equal depth , in which case it is of type ( c ) .',\n",
       " 'note that the lines @xmath171 are identified with each other and the phase diagram can be considered as being on a cylindrical surface.,scaledwidth=49.0% ]    the extremes of @xmath11 given by eq .',\n",
       " 'can be found easily only in the case of symmetric or antisymmetric ( @xmath172 ) surface and the same concerns the position of critical point . in this work , however , we are interested rather in possible topologies of the phase diagram in the @xmath159 plane , which result from eqs . and , and not in the exact location of critical points or transition lines . to draw schematic phase diagrams we consider @xmath11 as a function defined on the unit circle @xmath170',\n",
       " 'depending on the parameters @xmath116 and @xmath117 , @xmath113 has either one minimum and one maximum or two minima and two maxima .',\n",
       " 'this conclusion applies also to the function @xmath173 , thus , there can be either one or two critical points in the phase diagram ( see eqs . and ) . in the limit of large @xmath35',\n",
       " ', there is always a first order phase transition between two non - uniform textures corresponding to the opposite orientations of the director at @xmath21 .',\n",
       " 'this is because @xmath174 is not a periodic function of @xmath175 at fixed @xmath40 . to find @xmath40 at the transition we expand @xmath11 around its deepest minimum ( denoted @xmath176 ) , which leads to the approximate free energy : @xmath177 where @xmath178 is the extrapolation length .',\n",
       " 'since @xmath176 and @xmath179 are equivalent minima of @xmath11 , and both @xmath176 and @xmath40 are allowed to vary in the interval @xmath180 $ ] , the transition occurs at @xmath181 if @xmath182 or at @xmath183 if @xmath184 . with the above information we can now draw schematically the phase diagram ( see fig .  [ fig : diagram ] ) .',\n",
       " 'since the vertical lines at @xmath171 are to be identified with each other the phase diagram can be considered on a cylindrical surface .',\n",
       " 'away from the transition lines there is a smooth evolution from one texture to another .',\n",
       " 'this means that at fixed @xmath35 it is possible to transform smoothly the @xmath185 texture into the @xmath186 texture , i.e. , without crossing the transition line , even for @xmath187 .',\n",
       " 'we note that in some range of parameters , the phase diagram for @xmath11 with one minimum is topologically indistinguishable from that for @xmath11 with two minima ( fig .',\n",
       " '[ fig : diagram ]  ( b ) ) ; in both cases there are two critical points and a triple point .',\n",
       " 'if @xmath11 has two equal minima ( e.g. , at @xmath101 and @xmath188 in the case of symmetric surface ) the triple point disappears and the first order transition lines extend to @xmath189 , as shown in fig .',\n",
       " '[ fig : diagram ]  ( c ) .',\n",
       " 'we have studied the phase behavior of a nematic liquid crystal confined between a flat and a patterned substrate ( fig .',\n",
       " '[ fig1 ] ) using the frank - oseen model [ eq .',\n",
       " '( [ eq4 ] ) ] and the rapini - papoular surface free energy [ eq .',\n",
       " '( [ eq5 ] ) ] . an expression for the effective free energy function of the system [ eq .',\n",
       " '( [ eq6 ] ) ] was derived by determining an effective surface free energy characterizing the anchoring energy at the patterned surface [ eq .  ( [ eq8 ] ) ] . using the effective free energy function ,',\n",
       " 'we have determined the phase behavior of the nematic liquid crystal confined between a flat surface with strong anchoring and a chemically patterned sinusoidal surface ( fig .',\n",
       " '[ fig1 ] ( a ) ) , finding first order transitions between a homeotropic texture ( h ) and hybrid aligned nematic ( han ) textures ( figs .',\n",
       " '[ fig2 ] ( b ) and [ fig3 ] ( b ) ) .',\n",
       " 'it is possible to have a first order phase transition even with a monostable surface characterized by a monotonic surface free energy function ( fig .',\n",
       " '[ fig2 ]  ( a ) , @xmath114 $ ] ) .',\n",
       " 'in addition we have performed direct minimizations of the original free energy functional [ eqs .',\n",
       " '( [ eq4 ] ) and ( [ eq5 ] ) ] on a two - dimensional grid and found remarkably good agreement with the phase boundaries resulting from the effective energy function analysis ( figs',\n",
       " '.  [ fig2 ] ( b ) and [ fig3 ] ( b ) ) .',\n",
       " 'hence quantitatively reliable predictions of the phase behavior can be achieved using the effective free energy method .    using this method ,',\n",
       " 'we have also studied the phase behavior ( fig .',\n",
       " '[ fig : blazed ] ( b ) ) of a nematic liquid crystal confined between a chemically uniform , asymmetrically grooved substrate ( fig .',\n",
       " '[ fig1 ]  ( c ) and eq .',\n",
       " '( [ eq : blazed ] ) ) with locally homeotropic anchoring and a flat substrate with strong homeotropic anchoring , which is a typical setup for a zenithally bistable nematic device @xcite .',\n",
       " 'the asymmetry of the grating substrate leads to a decrease of the groove depth at which a first order transition between the h and han phases occurs ( fig .',\n",
       " '[ fig4 ]  ( a ) ) .',\n",
       " 'moreover , we have determined the energy barrier between the two coexisting states ( fig .',\n",
       " '[ fig4 ]  ( b ) ) .',\n",
       " 'our calculations show that the energy barrier decreases with increasing the asymmetry of the grating surface but it is well above @xmath190 for a typical nematic liquid crystal cell .',\n",
       " 'in addition , the average director orientation at the grating surface in two bistable states has been calculated ( fig .',\n",
       " '[ fig5 ] ) .',\n",
       " 'the difference between the two bistable states vanishes with increasing substrate asymmetry , which has a negative effect on the optical properties of a zenithally bistable nematic device .',\n",
       " 'we have also generalized the model of the effective surface free energy considered by parry - jones _',\n",
       " '@xcite to the case of asymmetric structured substrates and obtained three possible types of the phase diagram in the plane spanned by the orientation of the director at the homogeneous surface and the thickness of the nematic cell .',\n",
       " 'the asymmetry of the substrate causes only a shift of transition lines and critical points , compared to the symmetric case , but does not change the topology of the phase diagram .',\n",
       " 'finally , we have verified that this model allows one to reproduce qualitatively the phase diagram of a nematic liquid crystal confined between a homogeneous planar substrate and an asymmetrically grooved surface ( fig .',\n",
       " '[ fig : blazed ]  ( b ) and fig .',\n",
       " '[ fig : diagram ]  ( b ) ) .']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['article_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "571b4e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S> we study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary . by first evaluating an effective surface free energy function characterizing the patterned substrate we derive an expression for the effective free energy of the confined nematic liquid crystal . </S>',\n",
       " '<S> then we determine phase diagrams involving a homogeneous state in which the nematic director is almost uniform and a hybrid aligned nematic state in which the orientation of the director varies through the cell . </S>',\n",
       " '<S> direct minimization of the free energy functional were performed in order to test the predictions of the effective free energy method . </S>',\n",
       " '<S> we find remarkably good agreement between the phase boundaries calculated from the two approaches . </S>',\n",
       " '<S> in addition the effective energy method allows one to determine the energy barriers between two states in a bistable nematic device . </S>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['abstract_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc65f3a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"article_id\": \"1405.3379\", \"article_text\": [\"additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models .\", \"it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models .\", \"many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0 , see e.g. @xcite . in the last years\", \"many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded .\", \"this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.g. @xcite for the general case and @xcite for additive models .\", \"therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e.g. @xcite .\", \"such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to @xcite .    in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied .\", \"our leading example covers learning rates for quantile regression based on the lipschitz continuous but non - differentiable pinball loss function , which is also called check function in the literature , see e.g. @xcite and @xcite for parametric quantile regression and @xcite , @xcite , and @xcite for kernel based quantile regression .\", \"we will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own .\", \"of course , a practical approach might be to fit both models and compare their risks evaluated for test data .\", \"for the same reason we will also not cover sparsity .\", \"consistency of support vector machines generated by additive kernels for additive models was considered in @xcite . in this paper\", \"we establish learning rates for these algorithms .\", \"let us recall the framework with a complete separable metric space @xmath3 as the input space and a closed subset @xmath4 of @xmath5 as the output space .\", \"a borel probability measure @xmath6 on @xmath7 is used to model the learning problem and an independent and identically distributed sample @xmath8 is drawn according to @xmath6 for learning .\", \"a loss function @xmath9 is used to measure the quality of a prediction function @xmath10 by the local error @xmath11 .\", \"_ throughout the paper we assume that @xmath12 is measurable , @xmath13 , convex with respect to the third variable , and uniformly lipschitz continuous satisfying @xmath14 with a finite constant @xmath15 .\", \"_    support vector machines ( svms ) considered here are kernel - based regularization schemes in a reproducing kernel hilbert space ( rkhs ) @xmath0 generated by a mercer kernel @xmath16 . with a shifted loss function @xmath17 introduced for dealing\", \"even with heavy - tailed distributions as @xmath18 , they take the form @xmath19 where for a general borel measure @xmath20 on @xmath21 , the function @xmath22 is defined by @xmath23 where @xmath24 is a regularization parameter .\", \"the idea to shift a loss function has a long history , see e.g. @xcite in the context of m - estimators .\", \"it was shown in @xcite that @xmath22 is also a minimizer of the following optimization problem involving the original loss function @xmath12 if a minimizer exists : @xmath25    the additive model we consider consists of the _ input space decomposition _\", \"@xmath26 with each @xmath27 a complete separable metric space and a _ hypothesis space _\", \"@xmath28 where @xmath29 is a set of functions @xmath30 each of which is also identified as a map @xmath31 from @xmath3 to @xmath5 .\", \"hence the functions from @xmath32 take the additive form @xmath33 .\", \"we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity @xmath34 is an element of the set @xmath35 which is a subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 each quantity @xmath38 is an element of the full input space @xmath36 , where @xmath39 .\", \"because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols .\", \"the additive kernel @xmath40 is defined in terms of mercer kernels @xmath41 on @xmath27 as @xmath42 it generates an rkhs @xmath0 which can be written in terms of the rkhs @xmath43 generated by @xmath41 on @xmath27 corresponding to the form ( [ additive ] ) as @xmath44 with norm given by @xmath45 the norm of @xmath46 satisfies @xmath47    to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels .\", \"the first example deals with gaussian rbf kernels .\", \"all proofs will be given in section [ proofsection ] .\", \"[ gaussadd ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and @xmath52.\\\\\\\\ ] ] the additive kernel @xmath53 is given by @xmath54 furthermore , the product kernel @xmath55 is the standard gaussian kernel given by @xmath56 define a gaussian function @xmath57 on @xmath58 ^ 2 $ ] depending only on one variable by @xmath59 then @xmath60 but @xmath61 where @xmath62 denotes the rkhs generated by the standard gaussian rbf kernel @xmath63 .\", \"the second example is about sobolev kernels .\", \"[ sobolvadd ] let @xmath64 , @xmath65 $ ] and @xmath58^s.$ ] let @xmath66 : = \\\\\\\\bigl\\\\\\\\{u\\\\\\\\in l_2([0,1 ] ) ; d^\\\\\\\\alpha u \\\\\\\\in l_2([0,1 ] ) \\\\\\\\mbox{~for~all~}|\\\\\\\\alpha|\\\\\\\\le 1\\\\\\\\bigr\\\\\\\\}\\\\\\\\ ] ] be the sobolev space consisting of all square integrable univariate functions whose derivative is also square integrable .\", \"it is an rkhs with a mercer kernel @xmath67 defined on @xmath68 ^ 2 $ ] .\", \"if we take all the mercer kernels @xmath69 to be @xmath67 , then @xmath70 $ ] for each @xmath71 .\", \"the additive kernel @xmath72 is also a mercer kernel and defines an rkhs @xmath73\\\\\\\\right\\\\\\\\}.\\\\\\\\ ] ] however , the multivariate sobolev space @xmath74^s)$ ] , consisting of all square integrable functions whose partial derivatives are all square integrable , contains discontinuous functions and is not an rkhs .\", \"denote the marginal distribution of @xmath6 on @xmath27 as @xmath75 . under the assumption that @xmath76 for each @xmath71 and that @xmath43 is dense in @xmath29 in the @xmath77-metric , it was proved in @xcite that @xmath78 in probability as long as @xmath79 satisfies @xmath80 and @xmath81 .\", \"the rest of the paper has the following structure .\", \"section [ ratessection ] contains our main results on learning rates for svms based on additive kernels . learning rates for quantile regression\", \"are treated as important special cases .\", \"section [ comparisonsection ] contains a comparison of our results with other learning rates published recently .\", \"section [ proofsection ] contains all the proofs and some results which can be interesting in their own .\", \"in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in @xcite .\", \"the rates are about asymptotic behaviors of the excess risk @xmath82 and take the form @xmath83 with @xmath84 .\", \"they will be stated under three kinds of conditions involving the hypothesis space @xmath0 , the measure @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 .\", \"the first condition is about the approximation ability of the hypothesis space @xmath0 .\", \"since the output function @xmath19 is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space @xmath0 with respect to the optimal risk @xmath86 measured by the following approximation error .\", \"[ defapprox ] the approximation error of the triple @xmath87 is defined as @xmath88    to estimate the approximation error , we make an assumption about the minimizer of the risk @xmath89    for each @xmath90 , define the integral operator @xmath91 associated with the kernel @xmath41 by @xmath92 we mention that @xmath93 is a compact and positive operator on @xmath94 . hence we can find its normalized eigenpairs @xmath95 such that @xmath96 is an orthonormal basis of @xmath94 and @xmath97 as @xmath98 . fix @xmath99 .\", \"then we can define the @xmath100-th power @xmath101 of @xmath93 by @xmath102 this is a positive and bounded operator and its range is well - defined .\", \"the assumption @xmath103 means @xmath104 lies in this range .\", \"[ assumption1 ] we assume @xmath105 and @xmath106 where for some @xmath107 and each @xmath108 , @xmath109 is a function of the form @xmath110 with some @xmath111 .\", \"the case @xmath112 of assumption [ assumption1 ] means each @xmath113 lies in the rkhs @xmath43 .\", \"a standard condition in the literature ( e.g. , @xcite ) for achieving decays of the form @xmath114 for the approximation error ( [ approxerrordef ] ) is @xmath115 with some @xmath116 . here\", \"the operator @xmath117 is defined by @xmath118 in general , this can not be written in an additive form .\", \"however , the hypothesis space ( [ additive ] ) takes an additive form @xmath119 .\", \"so it is natural for us to impose an additive expression @xmath120 for the target function @xmath121 with the component functions @xmath113 satisfying the power condition @xmath110 .\", \"the above natural assumption leads to a technical difficulty in estimating the approximation error : the function @xmath113 has no direct connection to the marginal distribution @xmath122 projected onto @xmath27 , hence existing methods in the literature ( e.g. , @xcite ) can not be applied directly .\", \"note that on the product space @xmath123 , there is no natural probability measure projected from @xmath6 , and the risk on @xmath124 is not defined .    our idea to overcome the difficulty is to introduce an intermediate function @xmath125 .\", \"it may not minimize a risk ( which is not even defined ) .\", \"however , it approximates the component function @xmath113 well .\", \"when we add up such functions @xmath126 , we get a good approximation of the target function @xmath121 , and thereby a good estimate of the approximation error .\", \"this is the first novelty of the paper .\", \"[ approxerrorthm ] under assumption [ assumption1 ] , we have @xmath127 where @xmath128 is the constant given by @xmath129      the second condition for our learning rates is about the capacity of the hypothesis space measured by @xmath130-empirical covering numbers .    let @xmath131 be a set of functions on @xmath21 and @xmath132 for every @xmath133 the * covering number of @xmath131 * with respect to the empirical metric @xmath134 , given by @xmath135 is defined as @xmath136 and the * @xmath130-empirical covering number * of @xmath137 is defined as @xmath138    [ assumption2 ] we assume @xmath139 and that for some @xmath140 , @xmath141 and every @xmath142 , the @xmath130-empirical covering number of the unit ball of @xmath43 satisfies @xmath143    the second novelty of this paper is to observe that the additive nature of the hypothesis space yields the following nice bound with a dimension - independent power exponent for the covering numbers of the balls of the hypothesis space @xmath0 , to be proved in section [ samplesection ] .\", \"[ capacitythm ] under assumption [ assumption2 ] , for any @xmath144 and @xmath145 , we have @xmath146    the bound for the covering numbers stated in theorem [ capacitythm ] is special : the power @xmath147 is independent of the number @xmath148 of the components in the additive model .\", \"it is well - known @xcite in the literature of function spaces that the covering numbers of balls of the sobolev space @xmath149 on the cube @xmath150^s$ ] of the euclidean space @xmath151 with regularity index @xmath152 has the following asymptotic behavior with @xmath153 : @xmath154 here the power @xmath155 depends linearly on the dimension @xmath148 .\", \"similar dimension - dependent bounds for the covering numbers of the rkhss associated with gaussian rbf - kernels can be found in @xcite .\", \"the special bound in theorem [ capacitythm ] demonstrates an advantage of the additive model in terms of capacity of the additive hypothesis space .\", \"the third condition for our learning rates is about the noise level in the measure @xmath6 with respect to the hypothesis space . before stating the general condition\", \", we consider a special case for quantile regression , to illustrate our general results .\", \"let @xmath156 be a quantile parameter .\", \"the quantile regression function @xmath157 is defined by its value @xmath158 to be a @xmath159-quantile of @xmath160 , i.e. , a value @xmath161 satisfying @xmath162 the regularization scheme for quantile regression considered here takes the form ( [ algor ] ) with the loss function @xmath12 given by the pinball loss as @xmath163    a noise condition on @xmath6 for quantile regression is defined in @xcite as follows . to this end , let @xmath164 be a probability measure on @xmath165 and @xmath166 . then a real number @xmath167 is called @xmath159-quantile of @xmath164 , if and only if @xmath167 belongs to the set @xmath168\\\\\\\\bigr ) \\\\\\\\ge\", \"\\\\\\\\tau     \\\\\\\\mbox{~~and~~ } q\\\\\\\\bigl([t , \\\\\\\\infty)\\\\\\\\bigr ) \\\\\\\\ge 1-\\\\\\\\tau\\\\\\\\bigr\\\\\\\\}\\\\\\\\,.\\\\\\\\ ] ] it is well - known that @xmath169 is a compact interval .\", \"[ noisecond ] let @xmath166 .    1 .\", \"a probability measure @xmath164 on @xmath165 is said to have a * @xmath159-quantile of type @xmath170 * , if there exist a @xmath159-quantile @xmath171 and a constant @xmath172 such that , for all @xmath173 $ ] , we have @xmath174 2 .\", \"let @xmath175 $ ] .\", \"we say that a probability measure @xmath20 on @xmath176 has a * @xmath159-quantile of @xmath177-average type @xmath170 * if the conditional probability measure @xmath178 has @xmath179-almost surely a @xmath159-quantile of type @xmath170 and the function @xmath180 where @xmath181 is the constant defined in part ( 1 ) , satisfies @xmath182 .\", \"one can show that a distribution @xmath164 having a @xmath159-quantile of type @xmath170 has a unique @xmath159-quantile @xmath183 .\", \"moreover , if @xmath164 has a lebesgue density @xmath184 then @xmath164 has a @xmath159-quantile of type @xmath170 if @xmath184 is bounded away from zero on @xmath185 $ ] since we can use @xmath186\\\\\\\\}$ ] in ( [ tauquantileoftype2formula ] ) .\", \"this assumption is general enough to cover many distributions used in parametric statistics such as gaussian , student s @xmath187 , and logistic distributions ( with @xmath188 ) , gamma and log - normal distributions ( with @xmath189 ) , and uniform and beta distributions ( with @xmath190 $ ] ) .\", \"the following theorem , to be proved in section [ proofsection ] , gives a learning rate for the regularization scheme ( [ algor ] ) in the special case of quantile regression .\", \"[ quantilethm ] suppose that @xmath191 almost surely for some constant @xmath192 , and that each kernel @xmath41 is @xmath193 with @xmath194 for some @xmath195 .\", \"if assumption [ assumption1 ] holds with @xmath112 and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath198 and @xmath199 , with confidence at least @xmath200 we have @xmath201 where @xmath202 is a constant independent of @xmath203 and @xmath204 and @xmath205    please note that the exponent @xmath206 given by ( [ quantilerates2 ] ) for the learning rate in ( [ quantilerates ] ) is independent of the quantile level @xmath159 , of the number @xmath148 of additive components in @xmath207 , and of the dimensions @xmath208 and @xmath209 further note that @xmath210 , if @xmath211 , and @xmath212 if @xmath213 . because @xmath214 can be arbitrarily close to @xmath215 , the learning rate , which is independent of the dimension @xmath216 and given by theorem [ quantilethm ] , is close to @xmath217 for large values of @xmath177 and is close to @xmath218 or better , if @xmath211 .      to state our general learning rates\", \", we need an assumption on a _ variance - expectation bound _ which is similar to definition [ noisecond ] in the special case of quantile regression .\", \"[ assumption3 ] we assume that there exist an exponent @xmath219 $ ] and a positive constant @xmath220 such that @xmath221    assumption [ assumption3 ] always holds true for @xmath222 . if the triple @xmath223 satisfies some conditions , the exponent @xmath224 can be larger .\", \"for example , when @xmath12 is the pinball loss ( [ pinloss ] ) and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath225 for some @xmath196 $ ] and @xmath226 as defined in @xcite , then @xmath227 .\", \"[ mainratesthm ] suppose that @xmath228 is bounded by a constant @xmath229 almost surely . under assumptions [ assumption1 ] to [ assumption3 ] ,\", \"if we take @xmath198 and @xmath230 for some @xmath231 , then for any @xmath232 , with confidence at least @xmath200 we have @xmath233 where @xmath234 is given by @xmath235 and @xmath202 is constant independent of @xmath203 or @xmath204 ( to be given explicitly in the proof ) .\", \"we now add some theoretical and numerical comparisons on the goodness of our learning rates with those from the literature . as already mentioned in the introduction\", \", some reasons for the popularity of additive models are flexibility , increased interpretability , and ( often ) a reduced proneness of the curse of high dimensions .\", \"hence it is important to check , whether the learning rate given in theorem [ mainratesthm ] under the assumption of an additive model favourably compares to ( essentially ) optimal learning rates without this assumption . in other words ,\", \"we need to demonstrate that the main goal of this paper is achieved by theorem [ quantilethm ] and theorem [ mainratesthm ] , i.e. that an svm based on an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , provided the assumption of an additive model is satisfied .\", \"our learning rate in theorem [ quantilethm ] is new and optimal in the literature of svm for quantile regression .\", \"most learning rates in the literature of svm for quantile regression are given for projected output functions @xmath236 , while it is well known that projections improve learning rates @xcite . here the projection operator @xmath237 is defined for any measurable function @xmath10 by @xmath238 sometimes this is called clipping .\", \"such results are given in @xcite .\", \"for example , under the assumptions that @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , the approximation error condition ( [ approxerrorb ] ) is satisfied for some @xmath239 , and that for some constants @xmath240 , the sequence of eigenvalues @xmath241 of the integral operator @xmath117 satisfies @xmath242 for every @xmath243 , it was shown in @xcite that with confidence at least @xmath200 , @xmath244 where @xmath245 here the parameter @xmath246 measures the capacity of the rkhs @xmath247 and it plays a similar role as half of the parameter @xmath147 in assumption 2 . for a @xmath193 kernel and @xmath112\", \", one can choose @xmath246 and @xmath147 to be arbitrarily small and the above power index @xmath248 can be taken as @xmath249 .\", \"the learning rate in theorem [ quantilethm ] may be improved by relaxing assumption 1 to a sobolev smoothness condition for @xmath121 and a regularity condition for the marginal distribution @xmath250 .\", \"for example , one may use a gaussian kernel @xmath251 depending on the sample size @xmath203 and @xcite achieve the approximation error condition ( [ approxerrorb ] ) for some @xmath252 .\", \"this is done for quantile regression in @xcite .\", \"since we are mainly interested in additive models , we shall not discuss such an extension .\", \"[ gaussmore ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and the additive kernel @xmath72 be given by ( [ gaussaddform ] ) with @xmath253 in example [ gaussadd ] as @xmath52.\\\\\\\\ ] ] if the function @xmath121 is given by ( [ gaussfcn ] ) , @xmath191 almost surely for some constant @xmath192 , and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath145 and @xmath199 , ( [ quantilerates ] ) holds with confidence at least @xmath200 .    it is unknown whether the above learning rate can be derived by existing approaches in the literature ( e.g. @xcite ) even after projection .\", \"note that the kernel in the above example is independent of the sample size .\", \"it would be interesting to see whether there exists some @xmath99 such that the function @xmath57 defined by ( [ gaussfcn ] ) lies in the range of the operator @xmath254 .\", \"the existence of such a positive index would lead to the approximation error condition ( [ approxerrorb ] ) , see @xcite .    let us now add some numerical comparisons on the goodness of our learning rates given by theorem [ mainratesthm ] with those given by @xcite .\", \"their corollary 4.12 gives ( essentially ) minmax optimal learning rates for ( clipped ) svms in the context of nonparametric quantile regression using one gaussian rbf kernel on the whole input space under appropriate smoothness assumptions of the target function .\", \"let us consider the case that the distribution @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , where @xmath255 , and assume that both corollary 4.12 in @xcite and our theorem [ mainratesthm ] are applicable .\", \"i.e. , we assume in particular that @xmath6 is a probability measure on @xmath256 $ ] and that the marginal distribution @xmath257 has a lebesgue density @xmath258 for some @xmath259 . furthermore , suppose that the optimal decision function @xmath260 has ( to make theorem [ mainratesthm ] applicable with @xmath261 $ ] ) the additive structure @xmath207 with each @xmath104 as stated in assumption [ assumption1 ] , where @xmath262 and @xmath263 , with minimal risk @xmath86 and additionally fulfills ( to make corollary 4.12 in @xcite applicable ) @xmath264 where @xmath265 $ ] and @xmath266 denotes a besov space with smoothness parameter @xmath267 .\", \"the intuitive meaning of @xmath248 is , that increasing values of @xmath248 correspond to increased smoothness .\", \"we refer to ( * ? ? ? * and p. 44 ) for details on besov spaces .\", \"it is well - known that the besov space @xmath268 contains the sobolev space @xmath269 for @xmath270 , @xmath271 , and @xmath272 , and that @xmath273 .\", \"we mention that if all @xmath41 are suitably chosen wendland kernels , their reproducing kernel hilbert spaces @xmath43 are sobolev spaces , see ( * ? ? ?\", \"* thm . 10.35 , p. 160 ) .\", \"furthermore , we use the same sequence of regularizing parameters as in ( * ? ? ?\", \"4.9 , cor . 4.12 ) , i.e. , @xmath274 where @xmath275 , @xmath276 , @xmath277 $ ] , and @xmath278 is some user - defined positive constant independent of @xmath279 . for\", \"reasons of simplicity , let us fix @xmath280 .\", \"then ( * ? ? ?\", \"4.12 ) gives learning rates for the risk of svms for @xmath159-quantile regression , if a single gaussian rbf - kernel on @xmath281 is used for @xmath159-quantile functions of @xmath177-average type @xmath170 with @xmath255 , which are of order @xmath282 hence the learning rate in theorem [ quantilethm ] is better than the one in ( * ? ? ?\", \"4.12 ) in this situation , if @xmath283 provided the assumption of the additive model is valid .\", \"table [ table1 ] lists the values of @xmath284 from ( [ explicitratescz2 ] ) for some finite values of the dimension @xmath216 , where @xmath285 .\", \"all of these values of @xmath284 are positive with the exceptions if @xmath286 or @xmath287 .\", \"this is in contrast to the corresponding exponent in the learning rate by ( * ? ?\", \"* cor . 4.12 ) , because @xmath288    table [ table2 ] and figures [ figure1 ] to [ figure2 ] give additional information on the limit @xmath289 .\", \"of course , higher values of the exponent indicates faster rates of convergence .\", \"it is obvious , that an svm based on an additive kernel has a significantly faster rate of convergence in higher dimensions @xmath216 compared to svm based on a single gaussian rbf kernel defined on the whole input space , of course under the assumption that the additive model is valid .\", \"the figures seem to indicate that our learning rate from theorem [ mainratesthm ] is probably not optimal for small dimensions . however , the main focus of the present paper is on high dimensions .\", \".[table1 ] the table lists the limits of the exponents @xmath290 from ( * ? ? ?\", \"* cor . 4.12 ) and @xmath291 from theorem [ mainratesthm ] , respectively , if the regularizing parameter @xmath292 is chosen in an optimal manner for the nonparametric setup , i.e. @xmath293 , with @xmath294 for @xmath295 and @xmath296 .\", \"recall that @xmath297 $ ] .\", \"[ cols= \\\\\" > , > , > , > \\\\\" , ]\"], \"abstract_text\": [\"<S> additive models play an important role in semiparametric statistics . </S>\", \"<S> this paper gives learning rates for regularized kernel based methods for additive models . </S>\", \"<S> these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . </S>\", \"<S> additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .    * </S>\", \"<S> key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine . </S>\"], \"labels\": null, \"section_names\": [\"introduction\", \"main results on learning rates\", \"comparison of learning rates\"], \"sections\": [[\"additive models @xcite provide an important family of models for semiparametric regression or classification . some reasons for the success of additive models are their increased flexibility when compared to linear or generalized linear models and their increased interpretability when compared to fully nonparametric models .\", \"it is well - known that good estimators in additive models are in general less prone to the curse of high dimensionality than good estimators in fully nonparametric models .\", \"many examples of such estimators belong to the large class of regularized kernel based methods over a reproducing kernel hilbert space @xmath0 , see e.g. @xcite . in the last years\", \"many interesting results on learning rates of regularized kernel based models for additive models have been published when the focus is on sparsity and when the classical least squares loss function is used , see e.g. @xcite , @xcite , @xcite , @xcite , @xcite , @xcite and the references therein . of course , the least squares loss function is differentiable and has many nice mathematical properties , but it is only locally lipschitz continuous and therefore regularized kernel based methods based on this loss function typically suffer on bad statistical robustness properties , even if the kernel is bounded .\", \"this is in sharp contrast to kernel methods based on a lipschitz continuous loss function and on a bounded loss function , where results on upper bounds for the maxbias bias and on a bounded influence function are known , see e.g. @xcite for the general case and @xcite for additive models .\", \"therefore , we will here consider the case of regularized kernel based methods based on a general convex and lipschitz continuous loss function , on a general kernel , and on the classical regularizing term @xmath1 for some @xmath2 which is a smoothness penalty but not a sparsity penalty , see e.g. @xcite .\", \"such regularized kernel based methods are now often called support vector machines ( svms ) , although the notation was historically used for such methods based on the special hinge loss function and for special kernels only , we refer to @xcite .    in this paper we address the open question , whether an svm with an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , if the assumption of an additive model is satisfied .\", \"our leading example covers learning rates for quantile regression based on the lipschitz continuous but non - differentiable pinball loss function , which is also called check function in the literature , see e.g. @xcite and @xcite for parametric quantile regression and @xcite , @xcite , and @xcite for kernel based quantile regression .\", \"we will not address the question how to check whether the assumption of an additive model is satisfied because this would be a topic of a paper of its own .\", \"of course , a practical approach might be to fit both models and compare their risks evaluated for test data .\", \"for the same reason we will also not cover sparsity .\", \"consistency of support vector machines generated by additive kernels for additive models was considered in @xcite . in this paper\", \"we establish learning rates for these algorithms .\", \"let us recall the framework with a complete separable metric space @xmath3 as the input space and a closed subset @xmath4 of @xmath5 as the output space .\", \"a borel probability measure @xmath6 on @xmath7 is used to model the learning problem and an independent and identically distributed sample @xmath8 is drawn according to @xmath6 for learning .\", \"a loss function @xmath9 is used to measure the quality of a prediction function @xmath10 by the local error @xmath11 .\", \"_ throughout the paper we assume that @xmath12 is measurable , @xmath13 , convex with respect to the third variable , and uniformly lipschitz continuous satisfying @xmath14 with a finite constant @xmath15 .\", \"_    support vector machines ( svms ) considered here are kernel - based regularization schemes in a reproducing kernel hilbert space ( rkhs ) @xmath0 generated by a mercer kernel @xmath16 . with a shifted loss function @xmath17 introduced for dealing\", \"even with heavy - tailed distributions as @xmath18 , they take the form @xmath19 where for a general borel measure @xmath20 on @xmath21 , the function @xmath22 is defined by @xmath23 where @xmath24 is a regularization parameter .\", \"the idea to shift a loss function has a long history , see e.g. @xcite in the context of m - estimators .\", \"it was shown in @xcite that @xmath22 is also a minimizer of the following optimization problem involving the original loss function @xmath12 if a minimizer exists : @xmath25    the additive model we consider consists of the _ input space decomposition _\", \"@xmath26 with each @xmath27 a complete separable metric space and a _ hypothesis space _\", \"@xmath28 where @xmath29 is a set of functions @xmath30 each of which is also identified as a map @xmath31 from @xmath3 to @xmath5 .\", \"hence the functions from @xmath32 take the additive form @xmath33 .\", \"we mention , that there is strictly speaking a notational problem here , because in the previous formula each quantity @xmath34 is an element of the set @xmath35 which is a subset of the full input space @xmath36 , @xmath37 , whereas in the definition of sample @xmath8 each quantity @xmath38 is an element of the full input space @xmath36 , where @xmath39 .\", \"because these notations will only be used in different places and because we do not expect any misunderstandings , we think this notation is easier and more intuitive than specifying these quantities with different symbols .\", \"the additive kernel @xmath40 is defined in terms of mercer kernels @xmath41 on @xmath27 as @xmath42 it generates an rkhs @xmath0 which can be written in terms of the rkhs @xmath43 generated by @xmath41 on @xmath27 corresponding to the form ( [ additive ] ) as @xmath44 with norm given by @xmath45 the norm of @xmath46 satisfies @xmath47    to illustrate advantages of additive models , we provide two examples of comparing additive with product kernels .\", \"the first example deals with gaussian rbf kernels .\", \"all proofs will be given in section [ proofsection ] .\", \"[ gaussadd ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and @xmath52.\\\\\\\\ ] ] the additive kernel @xmath53 is given by @xmath54 furthermore , the product kernel @xmath55 is the standard gaussian kernel given by @xmath56 define a gaussian function @xmath57 on @xmath58 ^ 2 $ ] depending only on one variable by @xmath59 then @xmath60 but @xmath61 where @xmath62 denotes the rkhs generated by the standard gaussian rbf kernel @xmath63 .\", \"the second example is about sobolev kernels .\", \"[ sobolvadd ] let @xmath64 , @xmath65 $ ] and @xmath58^s.$ ] let @xmath66 : = \\\\\\\\bigl\\\\\\\\{u\\\\\\\\in l_2([0,1 ] ) ; d^\\\\\\\\alpha u \\\\\\\\in l_2([0,1 ] ) \\\\\\\\mbox{~for~all~}|\\\\\\\\alpha|\\\\\\\\le 1\\\\\\\\bigr\\\\\\\\}\\\\\\\\ ] ] be the sobolev space consisting of all square integrable univariate functions whose derivative is also square integrable .\", \"it is an rkhs with a mercer kernel @xmath67 defined on @xmath68 ^ 2 $ ] .\", \"if we take all the mercer kernels @xmath69 to be @xmath67 , then @xmath70 $ ] for each @xmath71 .\", \"the additive kernel @xmath72 is also a mercer kernel and defines an rkhs @xmath73\\\\\\\\right\\\\\\\\}.\\\\\\\\ ] ] however , the multivariate sobolev space @xmath74^s)$ ] , consisting of all square integrable functions whose partial derivatives are all square integrable , contains discontinuous functions and is not an rkhs .\", \"denote the marginal distribution of @xmath6 on @xmath27 as @xmath75 . under the assumption that @xmath76 for each @xmath71 and that @xmath43 is dense in @xmath29 in the @xmath77-metric , it was proved in @xcite that @xmath78 in probability as long as @xmath79 satisfies @xmath80 and @xmath81 .\", \"the rest of the paper has the following structure .\", \"section [ ratessection ] contains our main results on learning rates for svms based on additive kernels . learning rates for quantile regression\", \"are treated as important special cases .\", \"section [ comparisonsection ] contains a comparison of our results with other learning rates published recently .\", \"section [ proofsection ] contains all the proofs and some results which can be interesting in their own .\"], [\"in this paper we provide some learning rates for the support vector machines generated by additive kernels for additive models which helps improve the quantitative understanding presented in @xcite .\", \"the rates are about asymptotic behaviors of the excess risk @xmath82 and take the form @xmath83 with @xmath84 .\", \"they will be stated under three kinds of conditions involving the hypothesis space @xmath0 , the measure @xmath6 , the loss @xmath12 , and the choice of the regularization parameter @xmath85 .\", \"the first condition is about the approximation ability of the hypothesis space @xmath0 .\", \"since the output function @xmath19 is from the hypothesis space , the learning rates of the learning algorithm depend on the approximation ability of the hypothesis space @xmath0 with respect to the optimal risk @xmath86 measured by the following approximation error .\", \"[ defapprox ] the approximation error of the triple @xmath87 is defined as @xmath88    to estimate the approximation error , we make an assumption about the minimizer of the risk @xmath89    for each @xmath90 , define the integral operator @xmath91 associated with the kernel @xmath41 by @xmath92 we mention that @xmath93 is a compact and positive operator on @xmath94 . hence we can find its normalized eigenpairs @xmath95 such that @xmath96 is an orthonormal basis of @xmath94 and @xmath97 as @xmath98 . fix @xmath99 .\", \"then we can define the @xmath100-th power @xmath101 of @xmath93 by @xmath102 this is a positive and bounded operator and its range is well - defined .\", \"the assumption @xmath103 means @xmath104 lies in this range .\", \"[ assumption1 ] we assume @xmath105 and @xmath106 where for some @xmath107 and each @xmath108 , @xmath109 is a function of the form @xmath110 with some @xmath111 .\", \"the case @xmath112 of assumption [ assumption1 ] means each @xmath113 lies in the rkhs @xmath43 .\", \"a standard condition in the literature ( e.g. , @xcite ) for achieving decays of the form @xmath114 for the approximation error ( [ approxerrordef ] ) is @xmath115 with some @xmath116 . here\", \"the operator @xmath117 is defined by @xmath118 in general , this can not be written in an additive form .\", \"however , the hypothesis space ( [ additive ] ) takes an additive form @xmath119 .\", \"so it is natural for us to impose an additive expression @xmath120 for the target function @xmath121 with the component functions @xmath113 satisfying the power condition @xmath110 .\", \"the above natural assumption leads to a technical difficulty in estimating the approximation error : the function @xmath113 has no direct connection to the marginal distribution @xmath122 projected onto @xmath27 , hence existing methods in the literature ( e.g. , @xcite ) can not be applied directly .\", \"note that on the product space @xmath123 , there is no natural probability measure projected from @xmath6 , and the risk on @xmath124 is not defined .    our idea to overcome the difficulty is to introduce an intermediate function @xmath125 .\", \"it may not minimize a risk ( which is not even defined ) .\", \"however , it approximates the component function @xmath113 well .\", \"when we add up such functions @xmath126 , we get a good approximation of the target function @xmath121 , and thereby a good estimate of the approximation error .\", \"this is the first novelty of the paper .\", \"[ approxerrorthm ] under assumption [ assumption1 ] , we have @xmath127 where @xmath128 is the constant given by @xmath129      the second condition for our learning rates is about the capacity of the hypothesis space measured by @xmath130-empirical covering numbers .    let @xmath131 be a set of functions on @xmath21 and @xmath132 for every @xmath133 the * covering number of @xmath131 * with respect to the empirical metric @xmath134 , given by @xmath135 is defined as @xmath136 and the * @xmath130-empirical covering number * of @xmath137 is defined as @xmath138    [ assumption2 ] we assume @xmath139 and that for some @xmath140 , @xmath141 and every @xmath142 , the @xmath130-empirical covering number of the unit ball of @xmath43 satisfies @xmath143    the second novelty of this paper is to observe that the additive nature of the hypothesis space yields the following nice bound with a dimension - independent power exponent for the covering numbers of the balls of the hypothesis space @xmath0 , to be proved in section [ samplesection ] .\", \"[ capacitythm ] under assumption [ assumption2 ] , for any @xmath144 and @xmath145 , we have @xmath146    the bound for the covering numbers stated in theorem [ capacitythm ] is special : the power @xmath147 is independent of the number @xmath148 of the components in the additive model .\", \"it is well - known @xcite in the literature of function spaces that the covering numbers of balls of the sobolev space @xmath149 on the cube @xmath150^s$ ] of the euclidean space @xmath151 with regularity index @xmath152 has the following asymptotic behavior with @xmath153 : @xmath154 here the power @xmath155 depends linearly on the dimension @xmath148 .\", \"similar dimension - dependent bounds for the covering numbers of the rkhss associated with gaussian rbf - kernels can be found in @xcite .\", \"the special bound in theorem [ capacitythm ] demonstrates an advantage of the additive model in terms of capacity of the additive hypothesis space .\", \"the third condition for our learning rates is about the noise level in the measure @xmath6 with respect to the hypothesis space . before stating the general condition\", \", we consider a special case for quantile regression , to illustrate our general results .\", \"let @xmath156 be a quantile parameter .\", \"the quantile regression function @xmath157 is defined by its value @xmath158 to be a @xmath159-quantile of @xmath160 , i.e. , a value @xmath161 satisfying @xmath162 the regularization scheme for quantile regression considered here takes the form ( [ algor ] ) with the loss function @xmath12 given by the pinball loss as @xmath163    a noise condition on @xmath6 for quantile regression is defined in @xcite as follows . to this end , let @xmath164 be a probability measure on @xmath165 and @xmath166 . then a real number @xmath167 is called @xmath159-quantile of @xmath164 , if and only if @xmath167 belongs to the set @xmath168\\\\\\\\bigr ) \\\\\\\\ge\", \"\\\\\\\\tau     \\\\\\\\mbox{~~and~~ } q\\\\\\\\bigl([t , \\\\\\\\infty)\\\\\\\\bigr ) \\\\\\\\ge 1-\\\\\\\\tau\\\\\\\\bigr\\\\\\\\}\\\\\\\\,.\\\\\\\\ ] ] it is well - known that @xmath169 is a compact interval .\", \"[ noisecond ] let @xmath166 .    1 .\", \"a probability measure @xmath164 on @xmath165 is said to have a * @xmath159-quantile of type @xmath170 * , if there exist a @xmath159-quantile @xmath171 and a constant @xmath172 such that , for all @xmath173 $ ] , we have @xmath174 2 .\", \"let @xmath175 $ ] .\", \"we say that a probability measure @xmath20 on @xmath176 has a * @xmath159-quantile of @xmath177-average type @xmath170 * if the conditional probability measure @xmath178 has @xmath179-almost surely a @xmath159-quantile of type @xmath170 and the function @xmath180 where @xmath181 is the constant defined in part ( 1 ) , satisfies @xmath182 .\", \"one can show that a distribution @xmath164 having a @xmath159-quantile of type @xmath170 has a unique @xmath159-quantile @xmath183 .\", \"moreover , if @xmath164 has a lebesgue density @xmath184 then @xmath164 has a @xmath159-quantile of type @xmath170 if @xmath184 is bounded away from zero on @xmath185 $ ] since we can use @xmath186\\\\\\\\}$ ] in ( [ tauquantileoftype2formula ] ) .\", \"this assumption is general enough to cover many distributions used in parametric statistics such as gaussian , student s @xmath187 , and logistic distributions ( with @xmath188 ) , gamma and log - normal distributions ( with @xmath189 ) , and uniform and beta distributions ( with @xmath190 $ ] ) .\", \"the following theorem , to be proved in section [ proofsection ] , gives a learning rate for the regularization scheme ( [ algor ] ) in the special case of quantile regression .\", \"[ quantilethm ] suppose that @xmath191 almost surely for some constant @xmath192 , and that each kernel @xmath41 is @xmath193 with @xmath194 for some @xmath195 .\", \"if assumption [ assumption1 ] holds with @xmath112 and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath198 and @xmath199 , with confidence at least @xmath200 we have @xmath201 where @xmath202 is a constant independent of @xmath203 and @xmath204 and @xmath205    please note that the exponent @xmath206 given by ( [ quantilerates2 ] ) for the learning rate in ( [ quantilerates ] ) is independent of the quantile level @xmath159 , of the number @xmath148 of additive components in @xmath207 , and of the dimensions @xmath208 and @xmath209 further note that @xmath210 , if @xmath211 , and @xmath212 if @xmath213 . because @xmath214 can be arbitrarily close to @xmath215 , the learning rate , which is independent of the dimension @xmath216 and given by theorem [ quantilethm ] , is close to @xmath217 for large values of @xmath177 and is close to @xmath218 or better , if @xmath211 .      to state our general learning rates\", \", we need an assumption on a _ variance - expectation bound _ which is similar to definition [ noisecond ] in the special case of quantile regression .\", \"[ assumption3 ] we assume that there exist an exponent @xmath219 $ ] and a positive constant @xmath220 such that @xmath221    assumption [ assumption3 ] always holds true for @xmath222 . if the triple @xmath223 satisfies some conditions , the exponent @xmath224 can be larger .\", \"for example , when @xmath12 is the pinball loss ( [ pinloss ] ) and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath225 for some @xmath196 $ ] and @xmath226 as defined in @xcite , then @xmath227 .\", \"[ mainratesthm ] suppose that @xmath228 is bounded by a constant @xmath229 almost surely . under assumptions [ assumption1 ] to [ assumption3 ] ,\", \"if we take @xmath198 and @xmath230 for some @xmath231 , then for any @xmath232 , with confidence at least @xmath200 we have @xmath233 where @xmath234 is given by @xmath235 and @xmath202 is constant independent of @xmath203 or @xmath204 ( to be given explicitly in the proof ) .\"], [\"we now add some theoretical and numerical comparisons on the goodness of our learning rates with those from the literature . as already mentioned in the introduction\", \", some reasons for the popularity of additive models are flexibility , increased interpretability , and ( often ) a reduced proneness of the curse of high dimensions .\", \"hence it is important to check , whether the learning rate given in theorem [ mainratesthm ] under the assumption of an additive model favourably compares to ( essentially ) optimal learning rates without this assumption . in other words ,\", \"we need to demonstrate that the main goal of this paper is achieved by theorem [ quantilethm ] and theorem [ mainratesthm ] , i.e. that an svm based on an additive kernel can provide a substantially better learning rate in high dimensions than an svm with a general kernel , say a classical gaussian rbf kernel , provided the assumption of an additive model is satisfied .\", \"our learning rate in theorem [ quantilethm ] is new and optimal in the literature of svm for quantile regression .\", \"most learning rates in the literature of svm for quantile regression are given for projected output functions @xmath236 , while it is well known that projections improve learning rates @xcite . here the projection operator @xmath237 is defined for any measurable function @xmath10 by @xmath238 sometimes this is called clipping .\", \"such results are given in @xcite .\", \"for example , under the assumptions that @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , the approximation error condition ( [ approxerrorb ] ) is satisfied for some @xmath239 , and that for some constants @xmath240 , the sequence of eigenvalues @xmath241 of the integral operator @xmath117 satisfies @xmath242 for every @xmath243 , it was shown in @xcite that with confidence at least @xmath200 , @xmath244 where @xmath245 here the parameter @xmath246 measures the capacity of the rkhs @xmath247 and it plays a similar role as half of the parameter @xmath147 in assumption 2 . for a @xmath193 kernel and @xmath112\", \", one can choose @xmath246 and @xmath147 to be arbitrarily small and the above power index @xmath248 can be taken as @xmath249 .\", \"the learning rate in theorem [ quantilethm ] may be improved by relaxing assumption 1 to a sobolev smoothness condition for @xmath121 and a regularity condition for the marginal distribution @xmath250 .\", \"for example , one may use a gaussian kernel @xmath251 depending on the sample size @xmath203 and @xcite achieve the approximation error condition ( [ approxerrorb ] ) for some @xmath252 .\", \"this is done for quantile regression in @xcite .\", \"since we are mainly interested in additive models , we shall not discuss such an extension .\", \"[ gaussmore ] let @xmath48 , @xmath49 $ ] and @xmath50 ^ 2.$ ] let @xmath51 and the additive kernel @xmath72 be given by ( [ gaussaddform ] ) with @xmath253 in example [ gaussadd ] as @xmath52.\\\\\\\\ ] ] if the function @xmath121 is given by ( [ gaussfcn ] ) , @xmath191 almost surely for some constant @xmath192 , and @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 for some @xmath196 $ ] , then by taking @xmath197 , for any @xmath145 and @xmath199 , ( [ quantilerates ] ) holds with confidence at least @xmath200 .    it is unknown whether the above learning rate can be derived by existing approaches in the literature ( e.g. @xcite ) even after projection .\", \"note that the kernel in the above example is independent of the sample size .\", \"it would be interesting to see whether there exists some @xmath99 such that the function @xmath57 defined by ( [ gaussfcn ] ) lies in the range of the operator @xmath254 .\", \"the existence of such a positive index would lead to the approximation error condition ( [ approxerrorb ] ) , see @xcite .    let us now add some numerical comparisons on the goodness of our learning rates given by theorem [ mainratesthm ] with those given by @xcite .\", \"their corollary 4.12 gives ( essentially ) minmax optimal learning rates for ( clipped ) svms in the context of nonparametric quantile regression using one gaussian rbf kernel on the whole input space under appropriate smoothness assumptions of the target function .\", \"let us consider the case that the distribution @xmath6 has a @xmath159-quantile of @xmath177-average type @xmath170 , where @xmath255 , and assume that both corollary 4.12 in @xcite and our theorem [ mainratesthm ] are applicable .\", \"i.e. , we assume in particular that @xmath6 is a probability measure on @xmath256 $ ] and that the marginal distribution @xmath257 has a lebesgue density @xmath258 for some @xmath259 . furthermore , suppose that the optimal decision function @xmath260 has ( to make theorem [ mainratesthm ] applicable with @xmath261 $ ] ) the additive structure @xmath207 with each @xmath104 as stated in assumption [ assumption1 ] , where @xmath262 and @xmath263 , with minimal risk @xmath86 and additionally fulfills ( to make corollary 4.12 in @xcite applicable ) @xmath264 where @xmath265 $ ] and @xmath266 denotes a besov space with smoothness parameter @xmath267 .\", \"the intuitive meaning of @xmath248 is , that increasing values of @xmath248 correspond to increased smoothness .\", \"we refer to ( * ? ? ? * and p. 44 ) for details on besov spaces .\", \"it is well - known that the besov space @xmath268 contains the sobolev space @xmath269 for @xmath270 , @xmath271 , and @xmath272 , and that @xmath273 .\", \"we mention that if all @xmath41 are suitably chosen wendland kernels , their reproducing kernel hilbert spaces @xmath43 are sobolev spaces , see ( * ? ? ?\", \"* thm . 10.35 , p. 160 ) .\", \"furthermore , we use the same sequence of regularizing parameters as in ( * ? ? ?\", \"4.9 , cor . 4.12 ) , i.e. , @xmath274 where @xmath275 , @xmath276 , @xmath277 $ ] , and @xmath278 is some user - defined positive constant independent of @xmath279 . for\", \"reasons of simplicity , let us fix @xmath280 .\", \"then ( * ? ? ?\", \"4.12 ) gives learning rates for the risk of svms for @xmath159-quantile regression , if a single gaussian rbf - kernel on @xmath281 is used for @xmath159-quantile functions of @xmath177-average type @xmath170 with @xmath255 , which are of order @xmath282 hence the learning rate in theorem [ quantilethm ] is better than the one in ( * ? ? ?\", \"4.12 ) in this situation , if @xmath283 provided the assumption of the additive model is valid .\", \"table [ table1 ] lists the values of @xmath284 from ( [ explicitratescz2 ] ) for some finite values of the dimension @xmath216 , where @xmath285 .\", \"all of these values of @xmath284 are positive with the exceptions if @xmath286 or @xmath287 .\", \"this is in contrast to the corresponding exponent in the learning rate by ( * ? ?\", \"* cor . 4.12 ) , because @xmath288    table [ table2 ] and figures [ figure1 ] to [ figure2 ] give additional information on the limit @xmath289 .\", \"of course , higher values of the exponent indicates faster rates of convergence .\", \"it is obvious , that an svm based on an additive kernel has a significantly faster rate of convergence in higher dimensions @xmath216 compared to svm based on a single gaussian rbf kernel defined on the whole input space , of course under the assumption that the additive model is valid .\", \"the figures seem to indicate that our learning rate from theorem [ mainratesthm ] is probably not optimal for small dimensions . however , the main focus of the present paper is on high dimensions .\", \".[table1 ] the table lists the limits of the exponents @xmath290 from ( * ? ? ?\", \"* cor . 4.12 ) and @xmath291 from theorem [ mainratesthm ] , respectively , if the regularizing parameter @xmath292 is chosen in an optimal manner for the nonparametric setup , i.e. @xmath293 , with @xmath294 for @xmath295 and @xmath296 .\", \"recall that @xmath297 $ ] .\", \"[ cols= \\\\\" > , > , > , > \\\\\" , ]\"]]}\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd57aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
