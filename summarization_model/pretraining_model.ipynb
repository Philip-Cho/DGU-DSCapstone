{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cbb2b48",
   "metadata": {},
   "source": [
    "https://cryptosalamander.tistory.com/141?category=1218889  \n",
    "https://colab.research.google.com/drive/1lYBYtaXqt9S733OXdXvrvC09ysKFN30W#scrollTo=6S0xPged3n8S  \n",
    "https://huggingface.co/blog/how-to-train  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c14bfe",
   "metadata": {},
   "source": [
    "--model_name_or_path : 기존 모델의 weight를 불러온 뒤, 전이학습을 할 때 사용된다.  \n",
    "--model_type : pretrained weight 없이 구조만 가져와서 처음부터 학습을 진행할 시 사용한다.   \n",
    "--tokenizer_name: pretrained tokenizer를 불러오거나, tokenizer가 저장된 폴더의 경로를 전달하여 tokenizer를 불러온다. 기본적으로, tokenizer_config.json, vocab.txt, special_tokens_map.json을 불러오며, 이는 이전 포스팅에서 학습한 tokenizer를 저장한 경로에 존재한다.  \n",
    "--max_seq_length : input으로 사용될 문장의 길이 한도를 의미한다.  \n",
    "--train_file : 학습에 사용될 Corpus 데이터를 전달하면 된다.   \n",
    "--validation_file : validation 과정에 사용될 Corpus 데이터 경로를 전달하면 된다.  \n",
    "--validation_split_percentage : 별도의 validation_file이 없을 경우, train set에서 몇퍼센트를 추출하여 validation_set으로 활용한다. 10정도 입력하면 적절하다.  \n",
    "--cache_dir : 전처리된 임시 파일, 로그, checkpoint등이 저장될 경로, 입력하지 않으면 기본값인 nltk_data cache로 가지만, 지정해주면 해당 경로에 저장된다. 생각보다 용량이 커질 수 있으므로 용량이 넉넉한 파티션에 지정하는 것이 좋다.  \n",
    "--preprocessing_num_workers : 전처리에 사용할 프로세스의 개수이다. CPU Core수에 맞게 적절히 세팅해주면 좋다. 기본값으로 돌릴 시 47시간 걸리던 전처리가, num_workers를 16으로 해줬더니 10시간으로 줄어들었다. 하지만 너무 클 경우 CPU와 메모리에 부하가 크므로, CPU 사양을 고려하여 설정해야한다.  \n",
    "--line_by_line : 문장이 라인 단위로 쪼개져있는 Corpus에서 사용한다. 기본적으로 False 상태로 되어있다. 기본적으로 huggingface에서는 line_by_line이 꺼져있으면, 문장들을 합친 뒤 512를 넘기지 않도록 적절히 chunks를 만들고 이를 이용해 학습을 진행한다. line_by_line은 왠만하면 키지 않는 것이 좋다. 오용될 경우, Corpus에 불필요한 [PAD]가 매우 많이 발생하기 때문이다.  \n",
    "--pad_to_max_length : 512보다 짧은 문장에 padding을 추가하여 512로 만들어준다. 기본적으로 켜주는게 좋지만, 잘못된 방식으로 line_by_line과 함께 사용될 경우, 전체 데이터 단어 1개에 [PAD] 토큰 99개가 붙는 최악의 상황이 발생할 수 있다.   \n",
    "--per_device_train_batch_size : 말 그대로 train batch size  \n",
    "--per_device_eval_batch_size : eval batch size  \n",
    "--save_steps : 몇 step 마다 저장할 것인지, 기본값은 5000이지만, Corpus가 크다면 넉넉히 설정하는게 좋다. 너무 자주 저장되면 checkpoints 용량도 어마어마하게 크기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7b807",
   "metadata": {},
   "source": [
    "$python ./examples/pytorch/language-modeling/run_mlm.py \\\n",
    "  --max_seq_length 512 \\\n",
    "  --pad_to_max_length \\\n",
    "  --logging_dir ./tensorboard-metrics \\\n",
    "  --cache_dir /mnt/disks/sdc/cache_dir \\\n",
    "  --train_file ./train.txt \\\n",
    "  --tokenizer_name ./mytokenizer/ \\\n",
    "  --preprocessing_num_workers 8 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --model_type bert \\\n",
    "  --validation_split_percentage 10 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --output_dir my-bert \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --per_device_eval_batch_size 8 \\\n",
    "  --save_steps 500000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172a259",
   "metadata": {},
   "source": [
    "python ./transformers/examples/pytorch/language-modeling/run_mlm.py  --max_seq_length 512  --pad_to_max_length  --logging_dir ./logging  --cache_dir ./cache  --train_file data\\open_research\\sample_sents.txt  --tokenizer_name ./output/pretrained_tokenizer  --do_train  --do_eval  --model_type bart  --validation_split_percentage 10  --overwrite_output_dir  --output_dir ./output/bart  --num_train_epochs 3  --per_gpu_train_batch_size 4  --per_gpu_eval_batch_size 1  --save_steps 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738ab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "python ./examples/pytorch/language-modeling/run_mlm.py --max_seq_length 512 \\\n",
    "  --pad_to_max_length \\\n",
    "  --logging_dir ./tensorboard-metrics \\\n",
    "  --cache_dir /mnt/disks/sdc/cache_dir \\\n",
    "  --train_file ./train.txt \\\n",
    "  --tokenizer_name ./mytokenizer/ \\\n",
    "  --preprocessing_num_workers 8 \\\n",
    "  --do_train \\\n",
    "  --do_eval \\\n",
    "  --model_type bert \\\n",
    "  --validation_split_percentage 10 \\\n",
    "  --overwrite_output_dir \\\n",
    "  --output_dir my-bert \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --per_device_train_batch_size 8 \\\n",
    "  --per_device_eval_batch_size 8 \\\n",
    "  --save_steps 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147560e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9261332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('%d GPU(s)' %torch.cuda.device_count())\n",
    "else:\n",
    "    print('CPU')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "366547b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus dataset 가져옴(torch Dataset 상속)\n",
    "class CorpusDataset(Dataset):\n",
    "    def __init__(self, txt:str='sample_sents.txt'):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            './output/pretrained_tokenizer/vocab.json',\n",
    "            './output/pretrained_tokenizer/merges.txt'\n",
    "        )  \n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        \n",
    "        self.data = []\n",
    "        with open('data/open_research/'+txt, encoding='utf-8') as f:\n",
    "            self.data = f.readlines()\n",
    "            self.data = list(map(lambda x: x.strip(), data))\n",
    "        self.data = [x.ids for x in tokenizer.encode_batch(self.data)]\n",
    "    \n",
    "    def __len__(self):      # 데이터셋 길이(샘플 수) return\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):      # idx번째 data(tensor) return\n",
    "        return torch.tensor(self.data[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8756e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  366,  279,  974, 2010, 1479,  287, 1456,  455,  348, 1456, 3584,\n",
       "         582,   88,  628, 3838, 1308,    3])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CorpusDataset('sample_sents.txt')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dd724eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1c239775f10>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return mini batch\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)  # num_workers: 서버 개수\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7760b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "pytorch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
