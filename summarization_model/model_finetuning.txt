model pretraining(run_mlm.py)
python transformers/examples/pytorch/language_modeling/run_mlm.py
--model_type bart --train_file PATH --validation_split_percentage 10 --tokenizer_name PATH --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --do_eval --output_dir PATH --save_steps 500000


model finetuning(run_summarization.py)
python transformers/examples/pytorch/summarization/run_summarization.py
--model_name_or_path PATH --tokenizer_name PATH --do_train --do_eval --train_file PATH(csv/jsonl) --validation_split_percentage 10 --output_dir PATH --num_train_epochs 3 --per_gpu_train_batch_size 16 --per_gpu_eval_batch_size 16 --save_steps 50000 --overwrite_output_dir true --evaluate_during_training

preprocessing_num_workers: 

--model_type bart --train_file PATH --validation_split_percentage 10 --tokenizer_name PATH --per_device_train_batch_size 16 --per_device_eval_batch_size 16 --do_train --do_eval --output_dir PATH --save_steps 500000

