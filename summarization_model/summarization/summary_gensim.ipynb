{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"summary_gensim.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ToY9AxjInKWcD-5h-h9tLov8S84vzaHx","authorship_tag":"ABX9TyMX/FpuBXCWMFiGJMsO+Vyb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZwWeQ6IxLlm","executionInfo":{"status":"ok","timestamp":1650958117275,"user_tz":-540,"elapsed":42196,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"c7fbceaf-d90a-44c7-b338-999fafb8c0e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fasttext\n","  Downloading fasttext-0.9.2.tar.gz (68 kB)\n","\u001b[?25l\r\u001b[K     |████▊                           | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 68 kB 3.1 MB/s \n","\u001b[?25hCollecting pybind11>=2.2\n","  Using cached pybind11-2.9.2-py2.py3-none-any.whl (213 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext) (57.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext) (1.21.6)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3141815 sha256=10693f93dfcf7245916291eba8896d18bcc98ef34894438cbce6186a55d5ef90\n","  Stored in directory: /root/.cache/pip/wheels/4e/ca/bf/b020d2be95f7641801a6597a29c8f4f19e38f9c02a345bab9b\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.2 pybind11-2.9.2\n"]}],"source":["pip install fasttext"]},{"cell_type":"code","source":["pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JyiAPS4gxb-8","executionInfo":{"status":"ok","timestamp":1650958154909,"user_tz":-540,"elapsed":3312,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"0a840af3-0aec-47be-faaf-b6206cab11b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import gensim\n","from urllib.request import urlretrieve, urlopen\n","import gzip\n","import zipfile"],"metadata":{"id":"-AH7Oxp4xjOd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import re\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from urllib.request import urlretrieve\n","import zipfile\n","from sklearn.metrics.pairwise import cosine_similarity\n","import networkx as nx"],"metadata":{"id":"neyKtLWT0M09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')\n","\n","stop_words = stopwords.words('english')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VYpiKD6l1yJT","executionInfo":{"status":"ok","timestamp":1650959478164,"user_tz":-540,"elapsed":431,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"1e1f3528-0161-471c-c1ea-408696f0a9ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6D_nzgBF2xpA","executionInfo":{"status":"ok","timestamp":1650959713739,"user_tz":-540,"elapsed":11652,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"55a1252e-9e67-4ca5-e140-35238e762999"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["file_path = '/content/gdrive/MyDrive/Colab Notebooks/script.txt'"],"metadata":{"id":"nU55ro3O3i4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file = open(file_path, 'r')"],"metadata":{"id":"qVfiG_7P4iq2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = file.read() \n","# 변수 x 출력 \n","print(x) \n","# file 객체 닫기 \n","file.close()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kKtlZNpB4m_Q","executionInfo":{"status":"ok","timestamp":1650960037101,"user_tz":-540,"elapsed":1075,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"5c95c729-d75e-46d7-c02e-87db58be5178"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Welcome back. In this video, \n","we'll talk about how to compute derivatives for you \n","to implement gradient descent for logistic regression. \n","The key takeaways will be what you need to implement. \n","That is, the key equations you need in order to \n","implement gradient descent for logistic regression. \n","In this video, I want to do this computation using the computation graph. \n","I have to admit, using the computation graph is a little bit of \n","an overkill for deriving gradient descent for logistic regression, \n","but I want to start explaining things this \n","way to get you familiar with these ideas so that, \n","hopefully, it will make a bit more sense when we talk about full-fledged neural networks. \n","To that, let's dive into gradient descent for logistic regression. \n","To recap, we had set up logistic regression as follows, \n","your predictions, Y_hat, is defined as follows, \n","where z is that. \n","If we focus on just one example for now, then the loss, \n","or respect to that one example, \n","is defined as follows, \n","where A is the output of logistic regression, \n","and Y is the ground truth label. \n","Let's write this out as a computation graph and for this example, \n","let's say we have only two features, X1 and X2. \n","In order to compute Z, \n","we'll need to input W1, \n","W2, and B, in addition to the feature values X1, X2. \n","These things, in a computational graph, \n","get used to compute Z, which is W1, \n","X1 + W2 X2 + B, \n","rectangular box around that. \n","Then, we compute Y_hat, \n","or A = Sigma_of_Z, \n","that's the next step in the computation graph, and then, finally, \n","we compute L, AY, \n","and I won't copy the formula again. \n","In logistic regression, what we want to do is to modify the parameters, \n","W and B, in order to reduce this loss. \n","We've described the forward propagation steps of how you actually \n","compute the loss on a single training example, \n","now let's talk about how you can go backwards to compute the derivatives. \n","Here's a cleaned-up version of the diagram. \n","Because what we want to do is compute derivatives with respect to this loss, \n","the first thing we want to do when going backwards is to \n","compute the derivative of this loss with respect to, \n","the script over there, with respect to this variable A. \n","So, in the code, \n","you just use DA to denote this variable. \n","It turns out that if you are familiar with calculus, \n","you could show that this ends up being -Y_over_A+1-Y_over_1-A. \n","And the way you do that is you take the formula for the loss and, \n","if you're familiar with calculus, \n","you can compute the derivative with respect to the variable, \n","lowercase A, and you get this formula. \n","But if you're not familiar with calculus, don't worry about it. \n","We'll provide the derivative formulas, \n","what else you need, throughout this course. \n","If you are an expert in calculus, \n","I encourage you to look up the formula for the loss from \n","their previous slide and try taking derivative with respect to A using calculus, \n","but if you don't know enough calculus to do that, don't worry about it. \n","Now, having computed this quantity of DA and \n","the derivative or your final alpha variable with respect to A, \n","you can then go backwards. \n","It turns out that you can show DZ which, \n","this is the part called variable name, \n","this is going to be the derivative of the loss, \n","with respect to Z, or for L, \n","you could really write the loss including A and Y explicitly as parameters or not, right? \n","Either type of notation is equally acceptable. \n","We can show that this is equal to A-Y. \n","Just a couple of comments only for those of you experts in calculus, \n","if you're not expert in calculus, don't worry about it. \n","But it turns out that this, DL DZ, \n","this can be expressed as DL_DA_times_DA_DZ, \n","and it turns out that DA DZ, \n","this turns out to be A_times_1-A, \n","and DL DA we have previously worked out over here, \n","if you take these two quantities, DL DA, \n","which is this term, together with DA DZ, \n","which is this term, and just take these two things and multiply them. \n","You can show that the equation simplifies to A-Y. \n","That's how you derive it, \n","and that this is really the chain rule that have briefly eluded to the form. \n","Feel free to go through that calculation yourself if you are knowledgeable in calculus, \n","but if you aren't, all you need to know is that you can compute \n","DZ as A-Y and we've already done that calculus for you. \n","Then, the final step in that computation is to go back to \n","compute how much you need to change W and B. \n","In particular, you can show that the derivative with respect to W1 and in quotes, \n","call this DW1, that this is equal to X1_times_DZ. \n","Then, similarly, DW2, which is how much you want to change W2, \n","is X2_times_DZ and B, \n","excuse me, DB is equal to DZ. \n","If you want to do gradient descent with respect to just this one example, \n","what you would do is the following; \n","you would use this formula to compute DZ, \n","and then use these formulas to compute DW1, DW2, \n","and DB, and then you perform these updates. \n","W1 gets updated as W1 minus, \n","learning rate alpha, times DW1. \n","W2 gets updated similarly, \n","and B gets set as B minus the learning rate times DB. \n","And so, this will be one step of grade with respect to a single example. \n","You see in how to compute derivatives and implement \n","gradient descent for logistic regression with respect to a single training example. \n","But training logistic regression model, \n","you have not just one training example given training sets of M training examples. \n","In the next video, \n","let's see how you can take these ideas and apply them to learning, \n","not just from one example, \n","but from an entire training set.\n","\n","\n"]}]},{"cell_type":"code","source":["from gensim.summarization import summarize\n","\n","print(summarize(x, ratio = 0.1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XEjHs6qQ4wJh","executionInfo":{"status":"ok","timestamp":1650960633250,"user_tz":-540,"elapsed":6,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"868b3ef1-6d8a-4e4a-d000-48e68d1065cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["an overkill for deriving gradient descent for logistic regression, \n","Let's write this out as a computation graph and for this example, \n","compute the loss on a single training example, \n","now let's talk about how you can go backwards to compute the derivatives.\n","Because what we want to do is compute derivatives with respect to this loss, \n","compute the derivative of this loss with respect to, \n","you can compute the derivative with respect to the variable, \n","the derivative or your final alpha variable with respect to A, \n","If you want to do gradient descent with respect to just this one example, \n","You see in how to compute derivatives and implement \n","gradient descent for logistic regression with respect to a single training example.\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize "],"metadata":{"id":"BRwJLEd2DmrX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T4eVS9tuEQvO","executionInfo":{"status":"ok","timestamp":1650963224858,"user_tz":-540,"elapsed":1520,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"74a1c020-5614-4821-ffb6-fb985979c5a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize, word_tokenize \n","from nltk.corpus import stopwords\n","from collections import defaultdict\n","from string import punctuation\n","from heapq import nlargest\n","\n","stopwords = set(stopwords.words('english') + list(punctuation))\n","#stopwords                        ， do, I, am, is, are  ，                 。       (punctuation)          。\n","\n","max_cut = 0.9\n","min_cut = 0.1\n","#                  。                     。                           。\n","\n","\n","def compute_frequencies(word_sent):\n","    \"\"\"\n","               \n","    :param word_sent:            \n","    :return:     freq[], freq[w]   w     \n","    \"\"\"\n","    freq = defaultdict(int)#defaultdict    dict          default     int    0\n","    #          ：\n","    for s in word_sent:\n","        for word in s:\n","            if word not in stopwords:\n","                freq[word] += 1\n","    #        m            \n","    m = float(max(freq.values()))\n","    #         m\n","    for w in list(freq.keys()):\n","        freq[w] = freq[w]/m\n","        if freq[w] >= max_cut or freq[w] <= min_cut:\n","            del freq[w]\n","    #       \n","    # {key:  , value:    }\n","    return freq\n","\n","def summarize(text, n):\n","    \"\"\"\n","             \n","    text       \n","    n         \n","             \n","    \"\"\"\n","\n","    #          \n","    sents = sent_tokenize(text)\n","    assert n <= len(sents)\n","\n","    #      \n","    word_sent = [word_tokenize(s.lower()) for s in sents]\n","    # self._freq           \n","    freq = compute_frequencies(word_sent)\n","    #ranking             \n","    ranking = defaultdict(int)\n","    for i, word in enumerate(word_sent):\n","        for w in word:\n","            if w in freq:\n","                ranking[i] += freq[w]\n","    sents_idx = rank(ranking, n)\n","    return [sents[j] for j in sents_idx]\n","\n","\"\"\"\n","            \n","          n      \n","      heapq     \n","               \n","       n        \n","\"\"\"\n","def rank(ranking, n):\n","    return nlargest(n, ranking, key=ranking.get)\n","\n","#    \n","if __name__ == '__main__':\n","    res = summarize(x, 4)\n","    for i in range(len(res)):\n","        print(\"* \" + res[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUAwG-3oFXZF","executionInfo":{"status":"ok","timestamp":1650963503695,"user_tz":-540,"elapsed":509,"user":{"displayName":"최은진","userId":"03726309623727482734"}},"outputId":"0cfcc463-ab18-40a5-fcbd-3dfcb563325a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["* But it turns out that this, DL DZ, \n","this can be expressed as DL_DA_times_DA_DZ, \n","and it turns out that DA DZ, \n","this turns out to be A_times_1-A, \n","and DL DA we have previously worked out over here, \n","if you take these two quantities, DL DA, \n","which is this term, together with DA DZ, \n","which is this term, and just take these two things and multiply them.\n","* Because what we want to do is compute derivatives with respect to this loss, \n","the first thing we want to do when going backwards is to \n","compute the derivative of this loss with respect to, \n","the script over there, with respect to this variable A.\n","* If you are an expert in calculus, \n","I encourage you to look up the formula for the loss from \n","their previous slide and try taking derivative with respect to A using calculus, \n","but if you don't know enough calculus to do that, don't worry about it.\n","* If you want to do gradient descent with respect to just this one example, \n","what you would do is the following; \n","you would use this formula to compute DZ, \n","and then use these formulas to compute DW1, DW2, \n","and DB, and then you perform these updates.\n"]}]}]}