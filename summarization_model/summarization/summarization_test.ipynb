{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "778ba3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, AutoTokenizer, AutoModel, BartConfig\n",
    "from transformers import pipeline\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import torch\n",
    "from transformers import LEDTokenizer, LEDForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37aead7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = BartConfig(vocab_size=50000)\n",
    "# model = BartForConditionalGeneration(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba041ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_sci3: our_tokenizer, pretrained bart-base + finetuning(scitldr)\n",
    "model_sci3 = BartForConditionalGeneration.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_scitldr_3epoch\") \n",
    "# model_cnn: bart-base + finetuning(cnn)\n",
    "model_cnn = BartForConditionalGeneration.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_cnn_base\")\n",
    "# model_cnnsci: bart-large-cnn + finetunig(scitldr, 3epoch)\n",
    "model_cnnl_sci = BartForConditionalGeneration.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_bart_cnnsci\")\n",
    "# model_cnn_arxiv: bart-base + finetuning(cnn, arxiv)\n",
    "model_cnn_arxiv = BartForConditionalGeneration.from_pretrained('C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_cnn_arxiv_base')\n",
    "# model_led: long-sequence\n",
    "model_led = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "# tokenizer_led\n",
    "tokenizer_led = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "# open-research corpus\n",
    "tokenizer_ours1 = BartTokenizer.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/tokenizer\")\n",
    "# model_facebook: bart-base(no finetuning)\n",
    "model_facebook = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
    "# model_facebook_cnn: bart-large-cnn\n",
    "model_facebook_cnn = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "# tokenizer_facebook_b: facebook-base\n",
    "tokenizer_facebook_b = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "# tokenzier_facebook_l: facebook-large-cnn\n",
    "tokenizer_facebook_l = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d26c09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_ours2 = BartTokenizer.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/tokenizer2\")\n",
    "# model_sci5 = BartForConditionalGeneration.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_scitldr_5epoch_base\")\n",
    "# model_xsum = BartForConditionalGeneration.from_pretrained(\"C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/finetuning_xsum\")\n",
    "# new_tokenizer1 = BartTokenizer.from_pretrained('C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/new_tokenizer')\n",
    "# new_tokenizer2 = AutoTokenizer.from_pretrained('C:/Users/lynn1/OneDrive/바탕 화면/Capstone/output/new_tokenizer2_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3f27486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text, model, tokenizer, max_length=100, sci=False):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", truncation=False, padding='max_length')\n",
    "    summary_ids = model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs['attention_mask'], max_length=max_length)\n",
    "    summary = tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \n",
    "    if sci==True and 'We propose' in summary or 'We present' in summary:\n",
    "        summary = summary.replace('We propose', 'This lecture is about')\n",
    "        summary = summary.replace('We present', 'This lecture is about')\n",
    "    summary = summary.replace('\\n', ' ')\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "df8a8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer.encode(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\")\n",
    "\n",
    "# # Global attention on the first token (cf. Beltagy et al. 2020)\n",
    "# global_attention_mask = torch.zeros_like(inputs)\n",
    "# global_attention_mask[:, 0] = 1\n",
    "\n",
    "# # Generate Summary\n",
    "# summary_ids = model.generate(inputs, global_attention_mask=global_attention_mask, num_beams=3, max_length=32)\n",
    "# print(tokenizer.decode(summary_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "61efb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
    "# model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\", return_dict_in_generate=True).to(\"cuda\")\n",
    "# # tokenizer = LEDTokenizer.from_pretrained(\"HHousen/distil-led-large-cnn-16384\")\n",
    "# # model = LEDForConditionalGeneration.from_pretrained(\"HHousen/distil-led-large-cnn-16384\", return_dict_in_generate=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5c43f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cnn\n",
    "# script = script.replace('\\n', '')\n",
    "# input_ids = tokenizer(script, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# global_attention_mask = torch.zeros_like(input_ids)\n",
    "# # set global_attention_mask on first token\n",
    "# global_attention_mask[:, 0] = 1\n",
    "# sequences = model.generate(input_ids, global_attention_mask=global_attention_mask, num_beams=3, max_length=100)\n",
    "# summary = tokenizer.batch_decode(sequences[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d64a1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # arxiv\n",
    "# script = script.replace('\\n', '')\n",
    "# input_ids = tokenizer(script, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# global_attention_mask = torch.zeros_like(input_ids)\n",
    "# # set global_attention_mask on first token\n",
    "# global_attention_mask[:, 0] = 1\n",
    "# sequences = model.generate(input_ids, global_attention_mask=global_attention_mask, num_beams=3, max_length=200)\n",
    "# summary = tokenizer.batch_decode(sequences[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "# summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da668cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids = tokenizer(ARTICLE_TO_SUMMARIZE, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# global_attention_mask = torch.zeros_like(input_ids)\n",
    "# # set global_attention_mask on first token\n",
    "# global_attention_mask[:, 0] = 1\n",
    "\n",
    "# sequences = model.generate(input_ids, global_attention_masmask, num_beams=3, max_length=32)\n",
    "\n",
    "# summary = tokenizer.batch_decode(sequences[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)k=global_attention_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a49af32",
   "metadata": {},
   "source": [
    "# text\n",
    "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f608c8",
   "metadata": {},
   "source": [
    "- model_sci3: our_tokenizer, pretrained bart-base + finetuning(scitldr)  \n",
    "- model_cnn: bart-base + finetuning(cnn)\n",
    "- model_cnnsci: bart-large-cnn + finetunig(scitldr)\n",
    "- model_cnn_arxiv: bart-base + finetuning(cnn, arxiv)\n",
    "- model_led: long-sequence\n",
    "- tokenizer_led\n",
    "- open-research corpus\n",
    "- model_facebook: bart-base(no finetuning)\n",
    "- model_facebook_cnn: bart-large-cnn\n",
    "- tokenizer_facebook_b: facebook-base\n",
    "- tokenzier_facebook_l: facebook-large-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f985b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs. The blackouts were expected to last through at least midday tomorrow.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print(summarize(text, model_facebook_cnn, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2caf9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_facebook, tokenizer_facebook_b))  # 본문과 똑같이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da2d5261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " India singer 7 While school Tuesday school electric shutouts following strong wind'sen's is Tuesday's's's'sen's school's public's Tuesday's's'senen school is's found page'sday's State's comeout's statement's G's for Tuesday's public school's Tuesday's public'sday'sen found's J's� found's\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_sci3, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e496e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_sci3_base, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c99bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearly 800 thousand customers were scheduled to be affected by the shutoffs . PG&E stated it scheduled the blackouts in response to forecasts for high winds .\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_cnn, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e90e896f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearly 800,000 customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_cnnsci, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0f4c9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about 800 thousand customers were scheduled to be affected by the blackouts which were expected to last through at least midday tomorrow .   the aim is to reduce the risk of wildfires . \n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_cnn_arxiv, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82d07a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " blackouts were scheduled to be imposed by the state of california today in response to a forecast for high winds and dry conditions .   the blackouts were scheduled to last through at least midday tomorrow. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. the aim is to reduce the risk of wildfires.   \n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_led, tokenizer_led))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611843a",
   "metadata": {},
   "source": [
    "## script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37497228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key takeaways will be what you need to implement. That is, the key equations you need in order to implement gradient descent for logistic regression. In this video, I want to do this computation using the computation graph. In order to compute Z, we'll need to input W1, W2, and B, in addition to the feature values X1, X2.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print(summarize(script, model_facebook_cnn, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1cc90518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome back. In this video, we'll talk about how to compute derivatives for you to implement gradient descent for logistic regression. The key takeaways from this video will be what you need to implement. That is, the key equations you need in order to execute gradient descent. I want to do this computation using the computation graph. I have to admit, using the heap-based computation graph is a little bit of an overkill for deriving gradient descent by itself. But I\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_facebook, tokenizer_facebook_b))  # 본문과 똑같이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29321628",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/2731207.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_sci3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_facebook_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/1448872487.py\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(text, model, tokenizer, max_length, sci)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msummary_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;31m# if model is encoder decoder encoder_outputs are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;31m# and added to `model_kwargs`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e498faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We'll talk about how to compute derivatives for you to implement gradient descent for logistic regression.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3_base, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c931f633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this video, we'll talk about how to compute derivatives for you to implement gradient descent for logistic regression . The key takeaways will be what you need to implement . In this example, let's say we have only two features, X1 and X2 .\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_cnn, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b2f0e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to compute derivatives for gradient descent for logistic regression using the computation graph in order to implement gradient descent. The key takeaways will be what you need to implement and the key equations you need in order for you to do it.    --  \n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_cnnsci, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3e00c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in this video , we show how to compute derivatives for you to implement gradient descent for logistic regression .   the key takeaways will be what you need to implement . \n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_cnn_arxiv, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "71249efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in this video , we discuss how to compute derivatives for you to implement gradient descent for logistic regression. That is , the key equations you need in order to implement gradient descent for logistic regression. in this video , we want to do this computation using the computation graph. in this video , we want to do this computation using the computation graph. in this video , we want to do this computation using the computation graph. in this video , we want to do this computation using the computation graph\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_led, tokenizer_led))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db81be",
   "metadata": {},
   "source": [
    "# text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a0114b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers have achieved state-of-the-art results in a wide range of natural language tasks. While powerful, the memory and computational requirements of self-attention grow quadratically with sequence length. Longformer is able to build contextual representations of the entire context using multiple layers of attention.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "print(summarize(text2, model_facebook_cnn, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ce27f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers (Vaswani et al., 2017) have achieved state-of-the-art    results in a wide range of natural language tasks including generative language modeling  ,   (Dai et al,' 2019; Radford et al, 2019) and discriminative ... language understanding (Devlin and al., 2019).  Â  This success is partly due to the self-attention component which enables the network to capture contextual \n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_facebook, tokenizer_facebook_b))  # 본문과 똑같이 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a9b02798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/778612928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_sci3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_facebook_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10476/1448872487.py\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(text, model, tokenizer, max_length, sci)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'max_length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msummary_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;31m# if model is encoder decoder encoder_outputs are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;31m# and added to `model_kwargs`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    795\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 796\u001b[1;33m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_sci3, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c30b77dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about Longformer, a modified Transformer architecture with a self-attention operation that scales linearly with the sequence length.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_sci3_base, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7b585a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers have achieved state-of-the-art results in a wide range of natural language tasks . This success is partly due to the self-attention component which enables the network to capture contextual information from the entire sequence . Longformer is able to build contextual representations of the entire context using multiple layers of attention .\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_cnn, tokenizer_facebook_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2cdec345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Transformer-like architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for processing long documents, especially for natural language tasks such as long document classification, question answering (QA), and coreference resolution.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_cnnsci, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b1bc166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we present a modified transformer architecture with a self-attention operation that scales linearly with the sequence length , making it versatile for processing long documents .   this is an advantage for natural language tasks such as    long document classification , question answering (QA ) , and coreference resolution , where existing approaches partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of pretrained models . \n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_cnn_arxiv, tokenizer_facebook_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c04cd420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " transformer has achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling and discriminative language understanding .   this success is partly due to the self-attention component which enables the network to capture contextual     information from the entire sequence .   while powerful , the memory and computational requirements of     self-attention grow quadratically with sequence length , making it infeasible ( or very expensive\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text2, model_led, tokenizer_led))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d8208",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f4638205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PG&E stated it scheduled the blackouts in response to forecasts for high winds amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were scheduled to be affected by the shutoffs which were expected to\n"
     ]
    }
   ],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "print(summarize(text, model, tokenizer_facebook, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a097f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " about 800 thousand customers were scheduled to be affected by the blackouts which were expected to last through at least midday tomorrow .   the aim is to reduce the risk of wildfires . \n"
     ]
    }
   ],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "print(summarize(text, model_cnn_arxiv, tokenizer_facebook_l, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ffefe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in this video , we show how to compute derivatives for you to implement gradient descent for logistic regression .   the key takeaways will be what you need to implement . that is , the key equations you need in order to implement\n"
     ]
    }
   ],
   "source": [
    "# model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "print(summarize(script, model_cnn_arxiv, tokenizer_facebook_l, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f1827602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " amletsorrow.� toor� toorrowir.,un...alu scheduled. conc. to.,.ir.و.ow.\n"
     ]
    }
   ],
   "source": [
    "# SCITLDR로 training 시킨 우리 model + new_tokenizer\n",
    "print(summarize(text, model_sci3, new_tokenizer2, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9e922d1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about a scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow. . in response to forecasts for high winds amid dry. to forecasts.\n"
     ]
    }
   ],
   "source": [
    "# SCITLDR로 training 시킨 우리 model + old_tokenizer\n",
    "print(summarize(text, model_sci5, tokenizer_ours1, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f21b734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-ano plans tap trends More Sc coachto record without). Knicks public Gann TV\n"
     ]
    }
   ],
   "source": [
    "print(summarize(text, model_xsum, tokenizer_facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "399df4e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1268 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5860/3878961018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_facebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_facebook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5860/2094490616.py\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(text, model, tokenizer, max_length, sci)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msummary_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msummary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1154\u001b[0m             \u001b[1;31m# if model is encoder decoder encoder_outputs are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[1;31m# and added to `model_kwargs`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    522\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 798\u001b[1;33m         \u001b[0membed_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    799\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0membed_pos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_key_values_length\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         )\n\u001b[1;32m--> 137\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositions\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_1\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2042\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2044\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_facebook, tokenizer_facebook))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36682978",
   "metadata": {},
   "source": [
    "마침표로 끝나지 않으면 마침표까지 지우도록 전처리  \n",
    ".. 처리  \n",
    "., 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c24f7a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1775"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_tokenizer.tokenize(script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0490e579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about a computation graph to compute derivatives for you to implement gradient descent for logistic regression.., we talk about full-fledged neural networks. the computation graph and for this example., and B, in addition to the feature values X1, X2. to compute Z and B. and B to compute,. using the computation and for theoretical regression as follows, your predictions, Y_hat, is defined as follows.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3, tokenizer_ours2, max_length=100, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1cc0af44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about a computation graph to compute derivatives for you to implement gradient descent for logistic regression.., we talk about full-fledged neural networks. the computation graph and for this example., and B, in addition to the feature values X1, X2. to compute Z and B. and B to compute,. using the computation and for theoretical regression as follows, your predictions, Y_hat, is defined as follows.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3, tokenizer_ours2, max_length=100, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "8900ceb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This lecture is about a computation graph to compute derivatives for you to implement gradient descent for logistic regression.., we talk about full-fledged neural networks. the computation graph and for this example., and B, in addition to the feature values X1, X2. to compute Z and B. and B to compute,. using the computation and for theoretical regression as follows, your predictions, Y_hat, is defined as follows, where z is that. If we focus on just one example for now, then the loss, or respect to that one example, and Y is the ground truth label. with these ideas so that. on.  to the the feature to input W1, W2, WL. in addition. of the feature demonstr X1 and B and B, which is that the loss. for now.-f and the loss to on just more example, which the loss with respect to the output of logistic regression, where A is the assumptions of theoretical regression, andre is that, and show. that, and. as follows. from the feature. into gradient descent, and using the game.). is. , on,, in the loss and to that more example, the loss for now.. by. tasks.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3, tokenizer_ours, max_length=300, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5786081b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ienthancod doression. In this video, we'lll talk about howod compute derivatives for you to implement gradient descent for logistic regressionH The key takeaw\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci3, new_tokenizer1, sci=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e3a1388f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer_facebook.encode(script)\n",
    "len(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf00f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = '''Transformers (Vaswani et al., 2017) have achieved state-of-the-art\n",
    "    results in a wide range of natural language tasks including generative language modeling\n",
    "    (Dai et al., 2019; Radford et al., 2019) and discriminative ... language understanding (Devlin et al., 2019).\n",
    "    This success is partly due to the self-attention component which enables the network to capture contextual\n",
    "    information from the entire sequence. While powerful, the memory and computational requirements of\n",
    "    self-attention grow quadratically with sequence length, making it infeasible (or very expensive) to\n",
    "    process long sequences. To address this limitation, we present Longformer, a modified Transformer\n",
    "    architecture with a self-attention operation that scales linearly with the sequence length, making it\n",
    "    versatile for processing long documents (Fig 1). This is an advantage for natural language tasks such as\n",
    "    long document classification, question answering (QA), and coreference resolution, where existing approaches\n",
    "    partition or shorten the long context into smaller sequences that fall within the typical 512 token limit\n",
    "    of BERT-style pretrained models. Such partitioning could potentially result in loss of important\n",
    "    cross-partition information, and to mitigate this problem, existing methods often rely on complex\n",
    "    architectures to address such interactions. On the other hand, our proposed Longformer is able to build\n",
    "    contextual representations of the entire context using multiple layers of attention, reducing the need for\n",
    "    task-specific architectures.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39be7c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"Welcome back. In this video,\n",
    "we'll talk about how to compute derivatives for you\n",
    "to implement gradient descent for logistic regression.\n",
    "The key takeaways will be what you need to implement.\n",
    "That is, the key equations you need in order to\n",
    "implement gradient descent for logistic regression.\n",
    "In this video, I want to do this computation using the computation graph.\n",
    "I have to admit, using the computation graph is a little bit of\n",
    "an overkill for deriving gradient descent for logistic regression,\n",
    "but I want to start explaining things this\n",
    "way to get you familiar with these ideas so that,\n",
    "hopefully, it will make a bit more sense when we talk about full-fledged neural networks.\n",
    "To that, let's dive into gradient descent for logistic regression.\n",
    "To recap, we had set up logistic regression as follows,\n",
    "your predictions, Y_hat, is defined as follows,\n",
    "where z is that.\n",
    "If we focus on just one example for now, then the loss,\n",
    "or respect to that one example,\n",
    "is defined as follows,\n",
    "where A is the output of logistic regression,\n",
    "and Y is the ground truth label.\n",
    "Let's write this out as a computation graph and for this example,\n",
    "let's say we have only two features, X1 and X2.\n",
    "In order to compute Z,\n",
    "we'll need to input W1,\n",
    "W2, and B, in addition to the feature values X1, X2.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2b3b116",
   "metadata": {},
   "outputs": [],
   "source": [
    "script = \"\"\"Welcome back. In this video,\n",
    "we'll talk about how to compute derivatives for you\n",
    "to implement gradient descent for logistic regression.\n",
    "The key takeaways will be what you need to implement.\n",
    "That is, the key equations you need in order to\n",
    "implement gradient descent for logistic regression.\n",
    "In this video, I want to do this computation using the computation graph.\n",
    "I have to admit, using the computation graph is a little bit of\n",
    "an overkill for deriving gradient descent for logistic regression,\n",
    "but I want to start explaining things this\n",
    "way to get you familiar with these ideas so that,\n",
    "hopefully, it will make a bit more sense when we talk about full-fledged neural networks.\n",
    "To that, let's dive into gradient descent for logistic regression.\n",
    "To recap, we had set up logistic regression as follows,\n",
    "your predictions, Y_hat, is defined as follows,\n",
    "where z is that.\n",
    "If we focus on just one example for now, then the loss,\n",
    "or respect to that one example,\n",
    "is defined as follows,\n",
    "where A is the output of logistic regression,\n",
    "and Y is the ground truth label.\n",
    "Let's write this out as a computation graph and for this example,\n",
    "let's say we have only two features, X1 and X2.\n",
    "In order to compute Z,\n",
    "we'll need to input W1,\n",
    "W2, and B, in addition to the feature values X1, X2.\n",
    "These things, in a computational graph,\n",
    "get used to compute Z, which is W1,\n",
    "X1 + W2 X2 + B,\n",
    "rectangular box around that.\n",
    "Then, we compute Y_hat,\n",
    "or A = Sigma_of_Z,\n",
    "that's the next step in the computation graph, and then, finally,\n",
    "we compute L, AY,\n",
    "and I won't copy the formula again.\n",
    "In logistic regression, what we want to do is to modify the parameters,\n",
    "W and B, in order to reduce this loss.\n",
    "We've described the forward propagation steps of how you actually\n",
    "compute the loss on a single training example,\n",
    "now let's talk about how you can go backwards to compute the derivatives.\n",
    "Here's a cleaned-up version of the diagram.\n",
    "Because what we want to do is compute derivatives with respect to this loss,\n",
    "the first thing we want to do when going backwards is to\n",
    "compute the derivative of this loss with respect to,\n",
    "the script over there, with respect to this variable A.\n",
    "So, in the code,\n",
    "you just use DA to denote this variable.\n",
    "It turns out that if you are familiar with calculus,\n",
    "you could show that this ends up being -Y_over_A+1-Y_over_1-A.\n",
    "And the way you do that is you take the formula for the loss and,\n",
    "if you're familiar with calculus,\n",
    "you can compute the derivative with respect to the variable,\n",
    "lowercase A, and you get this formula.\n",
    "But if you're not familiar with calculus, don't worry about it.\n",
    "We'll provide the derivative formulas,\n",
    "what else you need, throughout this course.\n",
    "If you are an expert in calculus,\n",
    "I encourage you to look up the formula for the loss from\n",
    "their previous slide and try taking derivative with respect to A using calculus,\n",
    "but if you don't know enough calculus to do that, don't worry about it.\n",
    "Now, having computed this quantity of DA and\n",
    "the derivative or your final alpha variable with respect to A,\n",
    "you can then go backwards.\n",
    "It turns out that you can show DZ which,\n",
    "this is the part called variable name,\n",
    "this is going to be the derivative of the loss,\n",
    "with respect to Z, or for L,\n",
    "you could really write the loss including A and Y explicitly as parameters or not, right?\n",
    "Either type of notation is equally acceptable.\n",
    "We can show that this is equal to A-Y.\n",
    "Just a couple of comments only for those of you experts in calculus,\n",
    "if you're not expert in calculus, don't worry about it.\n",
    "But it turns out that this, DL DZ,\n",
    "this can be expressed as DL_DA_times_DA_DZ,\n",
    "and it turns out that DA DZ,\n",
    "this turns out to be A_times_1-A,\n",
    "and DL DA we have previously worked out over here,\n",
    "if you take these two quantities, DL DA,\n",
    "which is this term, together with DA DZ,\n",
    "which is this term, and just take these two things and multiply them.\n",
    "You can show that the equation simplifies to A-Y.\n",
    "That's how you derive it,\n",
    "and that this is really the chain rule that have briefly eluded to the form.\n",
    "Feel free to go through that calculation yourself if you are knowledgeable in calculus,\n",
    "but if you aren't, all you need to know is that you can compute\n",
    "DZ as A-Y and we've already done that calculus for you.\n",
    "Then, the final step in that computation is to go back to\n",
    "compute how much you need to change W and B.\n",
    "In particular, you can show that the derivative with respect to W1 and in quotes,\n",
    "call this DW1, that this is equal to X1_times_DZ.\n",
    "Then, similarly, DW2, which is how much you want to change W2,\n",
    "is X2_times_DZ and B,\n",
    "excuse me, DB is equal to DZ.\n",
    "If you want to do gradient descent with respect to just this one example,\n",
    "what you would do is the following;\n",
    "you would use this formula to compute DZ,\n",
    "and then use these formulas to compute DW1, DW2,\n",
    "and DB, and then you perform these updates.\n",
    "W1 gets updated as W1 minus,\n",
    "learning rate alpha, times DW1.\n",
    "W2 gets updated similarly,\n",
    "and B gets set as B minus the learning rate times DB.\n",
    "And so, this will be one step of grade with respect to a single example.\n",
    "You see in how to compute derivatives and implement\n",
    "gradient descent for logistic regression with respect to a single training example.\n",
    "But training logistic regression model,\n",
    "you have not just one training example given training sets of M training examples.\n",
    "In the next video,\n",
    "let's see how you can take these ideas and apply them to learning,\n",
    "not just from one example,\n",
    "but from an entire training set.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30c09f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_ours.tokenize(script))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "efb31f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A method to do this computation using the computation graph, which is W1,,\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_sci5, tokenizer_ours, max_length=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670e3f55",
   "metadata": {},
   "source": [
    "We propose -> This video is about... ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d64be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our series of letters from World War Two, I want to make a bit more sense when you need to implement. in order to reduce this loss.. on Wednesday. at the end of the year. that it will be the first time. to be the next step in a computation graph. the loss of DA. the world's first takeaways. it is a little bit of a video.\n"
     ]
    }
   ],
   "source": [
    "print(summarize(script, model_xsum, tokenizer_xsum, max_length=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd4a65e",
   "metadata": {},
   "source": [
    "scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bd7bf537",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d3463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1",
   "language": "python",
   "name": "pytorch_1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
