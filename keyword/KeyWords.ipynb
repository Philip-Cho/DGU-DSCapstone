{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import os\n",
    "    \n",
    "# 1-1) Load the stopwords\n",
    "def get_stopwords(stopwords_path):\n",
    "    stop_words_1 = stopwords.words('english')   # stopwords 179개\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [word.strip() for word in stop_words]   # stopwords 854개\n",
    "    return stop_words\n",
    "        \n",
    "# 1-2) Load the script  \n",
    "def get_script(script_path):\n",
    "    with open(script_path) as f:\n",
    "        text = f.read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sent.replace('\\n', ' ') for sent in sentences] \n",
    "    return sentences\n",
    "\n",
    "# 3-1) Build the sentence graph\n",
    "def build_sent_graph(sents, tfidf):  # 문장 리스트 -> tf-idf matrix -> sentence graph\n",
    "    graph_sentence = []\n",
    "    tfidf_mat = tfidf.fit_transform(sents).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence\n",
    "\n",
    "# 3-2) Build the word graph\n",
    "def build_word_graph(sent, cnt_vec):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sent).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    graph_word = np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "    idx2word = {vocab[word] : word for word in vocab}\n",
    "    return graph_word, idx2word\n",
    "\n",
    "# 4) Calculate the ranks of each sentence or word\n",
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0   # diagonal 부분 -> 0으로 바꿔줌(diagonal matrix)\n",
    "        link_sum = np.sum(A[:, id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    \n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "# 5-1) Get the list of keywords\n",
    "def get_keywords(sorted_word_idx, idx2word, word_num=5):\n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_idx[:word_num]:\n",
    "        index.append(idx)      \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "# 5-2) Get the list of keysentences\n",
    "def get_keysents(sorted_sent_idx, sentences, sent_num=2):\n",
    "    keysents=[]\n",
    "    index=[]\n",
    "    for idx in sorted_sent_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "    for idx in index:\n",
    "        keysents.append(sentences[idx])\n",
    "\n",
    "    return keysents\n",
    "\n",
    "# 6) Final: Get the sentence with blank, answer sentence, answer word\n",
    "def keysents_blank(keywords:list, keysents:list):\n",
    "    keysent=''   # blank 만들 keysent\n",
    "    keysent_blank=''   # blank 만든 keysent\n",
    "    keyword_keysent=''   # keysent의 blank에 들어갈 keyword\n",
    "    lowest_weight=23   # 가장 작은 weight(초기값: 최대 weight+1)\n",
    "    \n",
    "    for sent in keysents:\n",
    "        sent_weight = keysents.index(sent) + 1 \n",
    "        \n",
    "        keyword=''\n",
    "        for word in keywords:\n",
    "            if word in sent:\n",
    "                keyword = word\n",
    "                break   # keywords 리스트는 앞의 index일수록 순위가 높은 키워드 -> 문장에 존재하면 break    \n",
    "        if keyword!='':\n",
    "            word_weight = keywords.index(keyword) + 1\n",
    "        else:\n",
    "            word_weight = 23\n",
    "            \n",
    "        weight = sent_weight + word_weight\n",
    "        if weight<lowest_weight:\n",
    "            lowest_weight = weight\n",
    "            keysent = sent\n",
    "            keyword_keysent = keyword\n",
    "    \n",
    "    keysent_blank = keysent.replace(keyword_keysent, '__________')\n",
    "    \n",
    "    return {'sentence_blank':keysent_blank, 'sentence':keysent, 'answer':keyword_keysent}\n",
    "\n",
    "def preprocess_sents(sentences, stop_words):\n",
    "    # 2) Preprocess the sentences\n",
    "    sents_after=[]   # stop_words 제거, lower()한 list of sentences\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        sents_after.append(' '.join([word.lower() for word in words if word.lower() not in stop_words and len(word)>1]))\n",
    "        sents_after = [s for s in sents_after if s!=''] \n",
    "    return sents_after\n",
    "\n",
    "def run(script_path='scripts_for_stopwords/075-Clustering algorithms.flac.txt'):\n",
    "    # 1-3) Set the algorithm\n",
    "    sent_ngram = 1\n",
    "    word_ngram = 1\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, sent_ngram))\n",
    "    cnt_vec = CountVectorizer(ngram_range=(1, word_ngram))\n",
    "    \n",
    "    stopwords_path = 'stop_words_english.txt'\n",
    "    script_path = script_path\n",
    "    \n",
    "    stop_words = get_stopwords(stopwords_path)\n",
    "    sentences = get_script(script_path)\n",
    "    sents_after = preprocess_sents(sentences, stop_words)\n",
    "    \n",
    "    sent_graph = build_sent_graph(sents_after, tfidf)\n",
    "    word_graph, idx2word = build_word_graph(sents_after, cnt_vec)\n",
    "    \n",
    "    sent_rank_idx = get_ranks(sent_graph)  # 문장 가중치 그래프\n",
    "    sorted_sent_idx = sorted(sent_rank_idx,   # 문장 가중치 그래프-가중치 작은 차순 정렬\n",
    "                             key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "    word_rank_idx = get_ranks(word_graph)  # 단어 가중치 그래프\n",
    "    sorted_word_idx = sorted(word_rank_idx, \n",
    "                             key=lambda k: word_rank_idx[k], reverse=True)\n",
    "\n",
    "    keywords = get_keywords(sorted_word_idx, idx2word, word_num=10)\n",
    "    keysents = get_keysents(sorted_sent_idx, sentences, sent_num=10)\n",
    "#     print(keywords)\n",
    "#     print(keysents_blank(keywords, keysents))\n",
    "    \n",
    "    return keywords, keysents_blank(keywords, keysents)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run(script_path='scripts_for_stopwords/075-Clustering algorithms.flac.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wordfile(script_path):\n",
    "    keywords_list = run(script_path)[0]\n",
    "    with open('stop_words_english.txt', 'r', encoding='utf-8') as f:\n",
    "        stopwords_list = f.readlines()\n",
    "        stopwords_list = [word.strip() for word in stopwords_list]\n",
    "    \n",
    "    sciwords_list = [word for word in keywords_list if word not in stopwords_list]\n",
    "    \n",
    "    file = open('sciwords.txt', 'a', encoding='utf-8')\n",
    "    for word in keywords_list:\n",
    "        if word not in stopwords_list:\n",
    "            file.write(word+'\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## stopwords 작업하면서 데이터사이언스 관련 단어집 만들기  \n",
    "키워드 50개 추출 -> stopwords.txt에 포함되는 것 제거 -> sciwords.txt에 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['network', 'layer', 'learning', 'neurons', 'biases', 'function', 'sigmoid', 're', 'networks', 'digits', 'input', 'structure', 'image', 'feed', 'neuron', 'neural', 'videos', 'component', 'visual', 'outputs', 'code', 'machine', 'pixels', 'activations', 'weight', 'pixel', 'series', 'values', 'biological', 'inactive', 'weights', 'active', 'final', 'pattern', 'connections', 'lisha', 'bias', 'edge', 'linear', 'patterns', 'background', 'distinct', 'weighted', 'handwritten', 'sum', 'partners', 'sort', 'question', 'experiment', 'break'], {'sentence_blank': \"There's also a couple __________s in between called the hidden __________s  Which for the time being?\", 'sentence': \"There's also a couple layers in between called the hidden layers  Which for the time being?\", 'answer': 'layer'})\n"
     ]
    }
   ],
   "source": [
    "print(run('scripts_for_stopwords/[English] But what is a neural network_ _ Chapter 1, Deep learning [DownSub.com].txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopwords 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clusters', 'process', 'clustering', 'algorithms', 'cluster', 'group', 'image', 'centers', 'characteristics', 'center', 'learning', 'clustering algorithm', 'algorithm', 'based', 'density', 'science', 'outliers', 'distance', 'cluster centers', 'diagram', 'objects', 'noise', 'approach', 'resent clusters', 'clusters process', 'iterations center', 'movements', 'movements blue', 'process iterations', 'blue threshold', 'blue', 'iterations', 'center movements', 'resent', 'threshold', 'shades readings', 'area clusters', 'readings dance', 'cluster shades', 'clusters core', 'core area', 'area', 'core samples', 'dance', 'off', 'readings', 'shades', 'core', 'dance off', 'samples']\n"
     ]
    }
   ],
   "source": [
    "make_wordfile('scripts_for_stopwords/075-Clustering algorithms.flac.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# 2) Preprocess the sentences\n",
    "def preprocess_sents(sentences, stop_words):\n",
    "    sents_after=[]   # stop_words 제거, lower()한 list of sentences\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        sents_after.append(' '.join([word.lower() for word in words if word.lower() not in stop_words and len(word)>1]))\n",
    "        sents_after = [s for s in sents_after if s!=''] \n",
    "    return sents_after\n",
    "\n",
    "# 3-1) Build the sentence graph\n",
    "def build_sent_graph(sents, tfidf):  # 문장 리스트 -> tf-idf matrix -> sentence graph\n",
    "    graph_sentence = []\n",
    "    tfidf_mat = tfidf.fit_transform(sents).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence\n",
    "\n",
    "# 4) Calculate the ranks of each sentence or word\n",
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0   # diagonal 부분 -> 0으로 바꿔줌(diagonal matrix)\n",
    "        link_sum = np.sum(A[:, id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    \n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "# 5-1) Get the list of keywords\n",
    "def get_keywords(text, kw_model=KeyBERT('all-MiniLM-L12-v2'), word_num=10, stopwords_list=None):\n",
    "    stopwords_list = stopwords_list\n",
    "    keywords = kw_model.extract_keywords(text, top_n=word_num, \n",
    "                                         keyphrase_ngram_range=(1,1), \n",
    "                                         stop_words=stopwords_list)\n",
    "    return keywords\n",
    "\n",
    "# 5-2) Get the list of keysentences\n",
    "def get_keysents(sorted_sent_idx, sentences, sent_num=2):\n",
    "    keysents=[]\n",
    "    index=[]\n",
    "    for idx in sorted_sent_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "    for idx in index:\n",
    "        keysents.append(sentences[idx])\n",
    "\n",
    "    return keysents\n",
    "\n",
    "# 6) Final: Get the sentence with blank, answer sentence, answer word\n",
    "def keysents_blank(keywords:list, keysents:list):\n",
    "    keysent=''   # blank 만들 keysent\n",
    "    keysent_blank=''   # blank 만든 keysent\n",
    "    keyword_keysent=''   # keysent의 blank에 들어갈 keyword\n",
    "    lowest_weight=23   # 가장 작은 weight(초기값: 최대 weight+1)\n",
    "    \n",
    "    for sent in keysents:\n",
    "        sent_weight = keysents.index(sent) + 1 \n",
    "        \n",
    "        keyword=''\n",
    "        for word in keywords:\n",
    "            if word in sent:\n",
    "                keyword = word\n",
    "                break   # keywords 리스트는 앞의 index일수록 순위가 높은 키워드 -> 문장에 존재하면 break    \n",
    "        if keyword!='':\n",
    "            word_weight = keywords.index(keyword) + 1\n",
    "        else:\n",
    "            word_weight = 23\n",
    "            \n",
    "        weight = sent_weight + word_weight\n",
    "        if weight<lowest_weight:\n",
    "            lowest_weight = weight\n",
    "            keysent = sent\n",
    "            keyword_keysent = keyword\n",
    "    \n",
    "    keysent_blank = keysent.replace(keyword_keysent, '__________')\n",
    "    \n",
    "    return {'keywords':keywords, 'sentence_blank':keysent_blank, 'sentence':keysent, 'answer':keyword_keysent}\n",
    "\n",
    "\n",
    "def key_question(script_path='scripts_for_stopwords/075-Clustering algorithms.flac.txt'):\n",
    "    sent_ngram = 2\n",
    "    stopwords_path = 'stop_words_english.txt'\n",
    "    script_path = script_path    \n",
    "\n",
    "    with open(script_path) as f:\n",
    "        text = f.read()\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentences = [sent.replace('\\n', ' ') for sent in sentences] \n",
    "    \n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [word.strip() for word in stop_words]\n",
    "        \n",
    "        \n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, sent_ngram))\n",
    "    stop_words = get_stopwords(stopwords_path)\n",
    "    sentences = get_script(script_path)\n",
    "    sents_after = preprocess_sents(sentences, stop_words)\n",
    "    sent_graph = build_sent_graph(sents_after, tfidf)\n",
    "    sent_rank_idx = get_ranks(sent_graph)  # 문장 가중치 그래프\n",
    "    sorted_sent_idx = sorted(sent_rank_idx,   # 문장 가중치 그래프-가중치 작은 차순 정렬\n",
    "                             key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "    keysents = get_keysents(sorted_sent_idx, sentences, sent_num=10)\n",
    "    \n",
    "    \n",
    "        \n",
    "    kw_model = KeyBERT('all-MiniLM-L12-v2')\n",
    "    keywords_weight = get_keywords(text, kw_model, 10, stop_words)\n",
    "    keywords = [word_tup[0] for word_tup in keywords_weight]\n",
    "    \n",
    "    \n",
    "    return keysents_blank(keywords, keysents)\n",
    "\n",
    "key_question(script_path='scripts_for_stopwords/sample_script.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynn1\\Anaconda3\\envs\\tc\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:394: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['daren', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'itse', 'll', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'keywords': ['gradient',\n",
       "  'derivative',\n",
       "  'derivatives',\n",
       "  'logistic',\n",
       "  'computation',\n",
       "  'descent',\n",
       "  'computational',\n",
       "  'computed',\n",
       "  'derive',\n",
       "  'propagation'],\n",
       " 'sentence_blank': 'I have to admit, using the computation graph is a little bit of an overkill for deriving __________ descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about full-fledged neural networks.',\n",
       " 'sentence': 'I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about full-fledged neural networks.',\n",
       " 'answer': 'gradient'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_question(script_path = 'scripts_for_stopwords/sample_script.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynn1\\Anaconda3\\envs\\tc\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('scripts_for_stopwords/075-Clustering algorithms.flac.txt') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_words_english.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords_list = f.readlines()\n",
    "    stopwords_list = [word.strip() for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 1.18k/1.18k [00:00<00:00, 1.18MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 190/190 [00:00<00:00, 190kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 10.2k/10.2k [00:00<00:00, 10.2MB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 573/573 [00:00<00:00, 575kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 116/116 [00:00<00:00, 116kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████| 39.3k/39.3k [00:00<00:00, 192kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 349/349 [00:00<00:00, 350kB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 134M/134M [00:13<00:00, 9.77MB/s]\n",
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 53.0/53.0 [00:00<00:00, 53.1kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████████| 112/112 [00:00<00:00, 28.3kB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 466k/466k [00:01<00:00, 461kB/s]\n",
      "Downloading: 100%|█████████████████████████████████████████████████████████████████████| 352/352 [00:00<00:00, 357kB/s]\n",
      "Downloading: 100%|████████████████████████████████████████████████████████████████| 13.2k/13.2k [00:00<00:00, 13.2MB/s]\n",
      "Downloading: 100%|███████████████████████████████████████████████████████████████████| 232k/232k [00:00<00:00, 385kB/s]\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT('all-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clustering', 0.55),\n",
       " ('clusters', 0.5067),\n",
       " ('cluster', 0.5036),\n",
       " ('algorithms', 0.4563),\n",
       " ('supervised', 0.3841),\n",
       " ('similarity', 0.3694),\n",
       " ('algorithm', 0.3669),\n",
       " ('learning', 0.348),\n",
       " ('unsupervised', 0.3421),\n",
       " ('similarities', 0.342)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(text, top_n=10)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lynn1\\Anaconda3\\envs\\tc\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:394: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ain', 'al', 'couldn', 'daren', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'itse', 'll', 'mayn', 'mightn', 'mon', 'mustn', 'myse', 'needn', 'oughtn', 'shan', 'shouldn', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('algorithms clustering', 0.5935),\n",
       " ('clustering algorithms', 0.5926),\n",
       " ('clustering drooping', 0.5872),\n",
       " ('clustering applications', 0.5549),\n",
       " ('clustering algorithm', 0.5521),\n",
       " ('clustering', 0.55),\n",
       " ('learning algorithms', 0.5161),\n",
       " ('similarities algorithms', 0.5076),\n",
       " ('clusters', 0.5067),\n",
       " ('cluster', 0.5036)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = kw_model.extract_keywords(text, top_n=10, keyphrase_ngram_range=(1,2), stop_words=stopwords_list)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['clusters', 'process', 'clustering', 'algorithms', 'cluster', 'group', 'characteristics', 'algorithm', 'science', 'outliers'], {'sentence_blank': \"I'm here shows the number of __________ and now they would group to reduce their numbers.\", 'sentence': \"I'm here shows the number of clusters and now they would group to reduce their numbers.\", 'answer': 'clusters'})\n"
     ]
    }
   ],
   "source": [
    "print(run('scripts_for_stopwords/075-Clustering algorithms.flac.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to buy ties at a science in the last four videos recovered algorithms that's required. Supervised learning ensembles and logistic regression. Let's take a break from it. And look at some unsupervised, learning algorithms, algorithms for clustering clustering is about drooping. Similar objects. For example, similar individuals may have similar tastes and needs similar products and possibly be interchangeable. Do they need of a customer? What do we mean by similar? For example, for individuals? We can look at age gender income. And so, it is pretty obvious that what appeals to a 20-year old is quite different from what it feels to 60 year old. We can group, people objects events, music movies location, and just about anything. We can think of.  If you think of buying a house, you can look at the number of bathrooms. And number of bedrooms. If it is a condo apartment duplex and so on, you can do it neighborhoods by characteristics and then find multiple houses in similar neighborhoods that can answer the needs of a buyer.  Similarity is a measure of distance between objects 12 plus touring is to figure out how to express the characteristics in terms of numbers that say we want the group hair. Dye by color. It is not obvious. How close is brown and Auburn are. But if you come for the colors to an RGB representation, we now have three numbers that can be used for similarities.  What are the algorithms available for clustering? There are many algorithms available? For example, here's a list of algorithms that are part of the S killer in library.  There are multiple approaches to clustering. We can use a top-down or bottom-up approach. For example, do we know how many clusters we want? We also have to consider the number of data points to process and I was specifically, Jordan performs. We will only cover a few algorithms here. Starting with. Tammy's K means is probably the best-known clustering algorithm and probably the easiest to understand K stands for the number of clusters. We want, here's an example of how it works, the animation on my right shows the entire process. The first step is to pick a starting points.  The easiest sister picked a distinct points as cluster centers in the upper-left image. We picked the Four Points. Farthest to the left. The next step is to associate all the point to discuss our centers as shown in the second image note, that this process left one center with no Associated points. Once this is done. We resent her the Clusters and repeat the process until a number of iterations is done or if the center movements are blue, a decided threshold.  The choice of starting points impacts the result. So it makes sense to repeat the process with other centres. When we looked at the starting image. We may have visualized different clusters than the ones we ended up with.  How do we decide on the number of clusters? We can try a different number clusters and see the impact of the distance from the cluster centers in the diagram. Here. We see a quick drop at the start increasing the number of clusters. This is called the elbow method. We pick a value where the curve is. Another approach is to build a cluster is from the bottom up. I'm here shows the number of clusters and now they would group to reduce their numbers. This was done using the, a deliberative clustering algorithm the bottom layer of the right of the diagram shows, 50 plus there is, and how they would group together to merge together.  The last thing I want to mention is dbscan, which stands for density, based spatial, clustering of applications with noise.  With this elderly, then we can eliminate outliers. So they don't impact are clustering the image on my right shows two clusters. Each cluster different shades between the readings that are in a dance-off core area and the ones that are still part of the Clusters, but I'm not part of the core samples. Finally. We see the outliers values. The great. That are not part of a cluster and are considered noise. The number of clusters is automatically decided based on density parameters.  In the next video will see a use case application of these algorithms. See you next time. I'm by Ties That a science and don't forget to subscribe. \n"
     ]
    }
   ],
   "source": [
    "with open('scripts_for_stopwords/075-Clustering algorithms.flac.txt') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences = [sent.replace('\\n', ' ') for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Welcome to buy ties at a science in the last four videos recovered algorithms that's required.\",\n",
       " 'Supervised learning ensembles and logistic regression.',\n",
       " \"Let's take a break from it.\",\n",
       " 'And look at some unsupervised, learning algorithms, algorithms for clustering clustering is about drooping.',\n",
       " 'Similar objects.',\n",
       " 'For example, similar individuals may have similar tastes and needs similar products and possibly be interchangeable.',\n",
       " 'Do they need of a customer?',\n",
       " 'What do we mean by similar?',\n",
       " 'For example, for individuals?',\n",
       " 'We can look at age gender income.',\n",
       " 'And so, it is pretty obvious that what appeals to a 20-year old is quite different from what it feels to 60 year old.',\n",
       " 'We can group, people objects events, music movies location, and just about anything.',\n",
       " 'We can think of.',\n",
       " 'If you think of buying a house, you can look at the number of bathrooms.',\n",
       " 'And number of bedrooms.',\n",
       " 'If it is a condo apartment duplex and so on, you can do it neighborhoods by characteristics and then find multiple houses in similar neighborhoods that can answer the needs of a buyer.',\n",
       " 'Similarity is a measure of distance between objects 12 plus touring is to figure out how to express the characteristics in terms of numbers that say we want the group hair.',\n",
       " 'Dye by color.',\n",
       " 'It is not obvious.',\n",
       " 'How close is brown and Auburn are.',\n",
       " 'But if you come for the colors to an RGB representation, we now have three numbers that can be used for similarities.',\n",
       " 'What are the algorithms available for clustering?',\n",
       " 'There are many algorithms available?',\n",
       " \"For example, here's a list of algorithms that are part of the S killer in library.\",\n",
       " 'There are multiple approaches to clustering.',\n",
       " 'We can use a top-down or bottom-up approach.',\n",
       " 'For example, do we know how many clusters we want?',\n",
       " 'We also have to consider the number of data points to process and I was specifically, Jordan performs.',\n",
       " 'We will only cover a few algorithms here.',\n",
       " 'Starting with.',\n",
       " \"Tammy's K means is probably the best-known clustering algorithm and probably the easiest to understand K stands for the number of clusters.\",\n",
       " \"We want, here's an example of how it works, the animation on my right shows the entire process.\",\n",
       " 'The first step is to pick a starting points.',\n",
       " 'The easiest sister picked a distinct points as cluster centers in the upper-left image.',\n",
       " 'We picked the Four Points.',\n",
       " 'Farthest to the left.',\n",
       " 'The next step is to associate all the point to discuss our centers as shown in the second image note, that this process left one center with no Associated points.',\n",
       " 'Once this is done.',\n",
       " 'We resent her the Clusters and repeat the process until a number of iterations is done or if the center movements are blue, a decided threshold.',\n",
       " 'The choice of starting points impacts the result.',\n",
       " 'So it makes sense to repeat the process with other centres.',\n",
       " 'When we looked at the starting image.',\n",
       " 'We may have visualized different clusters than the ones we ended up with.',\n",
       " 'How do we decide on the number of clusters?',\n",
       " 'We can try a different number clusters and see the impact of the distance from the cluster centers in the diagram.',\n",
       " 'Here.',\n",
       " 'We see a quick drop at the start increasing the number of clusters.',\n",
       " 'This is called the elbow method.',\n",
       " 'We pick a value where the curve is.',\n",
       " 'Another approach is to build a cluster is from the bottom up.',\n",
       " \"I'm here shows the number of clusters and now they would group to reduce their numbers.\",\n",
       " 'This was done using the, a deliberative clustering algorithm the bottom layer of the right of the diagram shows, 50 plus there is, and how they would group together to merge together.',\n",
       " 'The last thing I want to mention is dbscan, which stands for density, based spatial, clustering of applications with noise.',\n",
       " 'With this elderly, then we can eliminate outliers.',\n",
       " \"So they don't impact are clustering the image on my right shows two clusters.\",\n",
       " \"Each cluster different shades between the readings that are in a dance-off core area and the ones that are still part of the Clusters, but I'm not part of the core samples.\",\n",
       " 'Finally.',\n",
       " 'We see the outliers values.',\n",
       " 'The great.',\n",
       " 'That are not part of a cluster and are considered noise.',\n",
       " 'The number of clusters is automatically decided based on density parameters.',\n",
       " 'In the next video will see a use case application of these algorithms.',\n",
       " 'See you next time.',\n",
       " \"I'm by Ties That a science and don't forget to subscribe.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words_1 = stopwords.words('english')\n",
    "# len(stop_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'adj',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ahead',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a's\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'back',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'caption',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'co',\n",
       " 'co.',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'compute',\n",
       " 'computes',\n",
       " 'con',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " \"c's\",\n",
       " 'currently',\n",
       " 'dare',\n",
       " \"daren't\",\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'directly',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'evermore',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'fairly',\n",
       " 'far',\n",
       " 'farther',\n",
       " 'few',\n",
       " 'fewer',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'half',\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " \"i'd\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'inc.',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'inside',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'likewise',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " \"mayn't\",\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " \"mightn't\",\n",
       " 'mine',\n",
       " 'minus',\n",
       " 'miss',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'neverf',\n",
       " 'neverless',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'no-one',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'notwithstanding',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " \"one's\",\n",
       " 'only',\n",
       " 'onto',\n",
       " 'opposite',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " \"oughtn't\",\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respect',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'round',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someday',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " \"that's\",\n",
       " \"that've\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " \"there'd\",\n",
       " 'therefore',\n",
       " 'therein',\n",
       " \"there'll\",\n",
       " \"there're\",\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " \"there've\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'third',\n",
       " 'thirty',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'till',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t's\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'underneath',\n",
       " 'undoing',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'upwards',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'versus',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what'll\",\n",
       " \"what's\",\n",
       " \"what've\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'while',\n",
       " 'whilst',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " \"who'll\",\n",
       " 'whom',\n",
       " 'whomever',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'zero',\n",
       " 'a',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"when's\",\n",
       " \"why's\",\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'j',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'uucp',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'I',\n",
       " 'www',\n",
       " 'amount',\n",
       " 'bill',\n",
       " 'bottom',\n",
       " 'call',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'due',\n",
       " 'eleven',\n",
       " 'empty',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'forty',\n",
       " 'front',\n",
       " 'full',\n",
       " 'give',\n",
       " 'hasnt',\n",
       " 'herse',\n",
       " 'himse',\n",
       " 'interest',\n",
       " 'itse”',\n",
       " 'mill',\n",
       " 'move',\n",
       " 'myse”',\n",
       " 'part',\n",
       " 'put',\n",
       " 'show',\n",
       " 'side',\n",
       " 'sincere',\n",
       " 'sixty',\n",
       " 'system',\n",
       " 'ten',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'top',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'act',\n",
       " 'added',\n",
       " 'adopted',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'ah',\n",
       " 'announce',\n",
       " 'anymore',\n",
       " 'apparently',\n",
       " 'approximately',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'auth',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'biol',\n",
       " 'briefly',\n",
       " 'ca',\n",
       " 'date',\n",
       " 'ed',\n",
       " 'effect',\n",
       " 'et-al',\n",
       " 'ff',\n",
       " 'fix',\n",
       " 'gave',\n",
       " 'giving',\n",
       " 'heres',\n",
       " 'hes',\n",
       " 'hid',\n",
       " 'home',\n",
       " 'id',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'index',\n",
       " 'information',\n",
       " 'invention',\n",
       " 'itd',\n",
       " 'keys',\n",
       " 'kg',\n",
       " 'km',\n",
       " 'largely',\n",
       " 'lets',\n",
       " 'line',\n",
       " \"'ll\",\n",
       " 'means',\n",
       " 'mg',\n",
       " 'million',\n",
       " 'ml',\n",
       " 'mug',\n",
       " 'na',\n",
       " 'nay',\n",
       " 'necessarily',\n",
       " 'nos',\n",
       " 'noted',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'omitted',\n",
       " 'ord',\n",
       " 'owing',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'poorly',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'quickly',\n",
       " 'ran',\n",
       " 'readily',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'related',\n",
       " 'research',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'run',\n",
       " 'sec',\n",
       " 'section',\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'showed',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'slightly',\n",
       " 'somethan',\n",
       " 'specifically',\n",
       " 'state',\n",
       " 'states',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'thered',\n",
       " 'thereof',\n",
       " 'therere',\n",
       " 'thereto',\n",
       " 'theyd',\n",
       " 'theyre',\n",
       " 'thou',\n",
       " 'thoughh',\n",
       " 'thousand',\n",
       " 'throug',\n",
       " 'til',\n",
       " 'tip',\n",
       " 'ts',\n",
       " 'ups',\n",
       " 'usefully',\n",
       " 'usefulness',\n",
       " \"'ve\",\n",
       " 'vol',\n",
       " 'vols',\n",
       " 'wed',\n",
       " 'whats',\n",
       " 'wheres',\n",
       " 'whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stop_words_english.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "stop_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"buy ties science videos recovered algorithms 's required\",\n",
       " 'supervised learning ensembles logistic regression',\n",
       " \"'s break\",\n",
       " 'unsupervised learning algorithms algorithms clustering clustering drooping',\n",
       " 'objects',\n",
       " 'individuals tastes products interchangeable',\n",
       " 'customer',\n",
       " 'individuals',\n",
       " 'age gender income',\n",
       " 'pretty obvious appeals 20-year feels 60 year',\n",
       " 'group people objects events music movies location',\n",
       " 'buying house number bathrooms',\n",
       " 'number bedrooms',\n",
       " 'condo apartment duplex neighborhoods characteristics multiple houses neighborhoods answer buyer',\n",
       " 'similarity measure distance objects 12 touring figure express characteristics terms numbers group hair',\n",
       " 'dye color',\n",
       " 'obvious',\n",
       " 'close brown auburn',\n",
       " 'colors rgb representation numbers similarities',\n",
       " 'algorithms clustering',\n",
       " 'algorithms',\n",
       " \"'s list algorithms killer library\",\n",
       " 'multiple approaches clustering',\n",
       " 'top-down bottom-up approach',\n",
       " 'clusters',\n",
       " 'number data points process jordan performs',\n",
       " 'cover algorithms',\n",
       " 'starting',\n",
       " \"tammy 's best-known clustering algorithm easiest understand stands number clusters\",\n",
       " \"'s works animation entire process\",\n",
       " 'step pick starting points',\n",
       " 'easiest sister picked distinct points cluster centers upper-left image',\n",
       " 'picked points',\n",
       " 'farthest left',\n",
       " 'step associate point discuss centers image note process left center points',\n",
       " 'resent clusters repeat process number iterations center movements blue decided threshold',\n",
       " 'choice starting points impacts result',\n",
       " 'sense repeat process centres',\n",
       " 'looked starting image',\n",
       " 'visualized clusters ended',\n",
       " 'decide number clusters',\n",
       " 'number clusters impact distance cluster centers diagram',\n",
       " 'quick drop start increasing number clusters',\n",
       " 'called elbow method',\n",
       " 'pick curve',\n",
       " 'approach build cluster',\n",
       " \"'m number clusters group reduce numbers\",\n",
       " 'deliberative clustering algorithm layer diagram 50 group merge',\n",
       " 'mention dbscan stands density based spatial clustering applications noise',\n",
       " 'elderly eliminate outliers',\n",
       " \"n't impact clustering image clusters\",\n",
       " \"cluster shades readings dance-off core area clusters 'm core samples\",\n",
       " 'finally',\n",
       " 'outliers values',\n",
       " 'great',\n",
       " 'cluster considered noise',\n",
       " 'number clusters automatically decided based density parameters',\n",
       " 'video case application algorithms',\n",
       " 'time',\n",
       " \"'m ties science n't forget subscribe\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop_words 제거, lower()한 문장 리스트\n",
    "sents_after=[]\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    sents_after.append(' '.join([word.lower() for word in words if word.lower() not in stop_words and len(word)>1]))\n",
    "    sents_after = [s for s in sents_after if s!='']\n",
    "sents_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: 단어의 중요도 나타냄  \n",
    "TfidfVectorizer: 문장 단위로 TF-IDF 수치 벡터화한 matrix return  \n",
    "CountVectorizer: 단어 count를 기준으로 벡터화한 matrix return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3))\n",
    "graph_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트 -> tf-idf matrix -> sentence graph\n",
    "def build_sent_graph(sents):\n",
    "    tfidf_mat = tfidf.fit_transform(sents).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word graph 생성\n",
    "def build_word_graph(sent):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sent).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    graph_word = np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "    idx2word = {vocab[word] : word for word in vocab}\n",
    "    return graph_word, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , ..., 0.10216038, 0.        ,\n",
       "        0.3490473 ],\n",
       "       [0.        , 1.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 1.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.10216038, 0.        , 0.        , ..., 1.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.3490473 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_graph = build_sent_graph(sents_after)\n",
    "sent_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525, 525)\n",
      "{78: 'buy', 487: 'ties', 435: 'science', 513: 'videos', 413: 'recovered', 19: 'algorithms', 426: 'required', 79: 'buy ties', 488: 'ties science', 438: 'science videos', 514: 'videos recovered', 414: 'recovered algorithms', 26: 'algorithms required', 80: 'buy ties science', 490: 'ties science videos', 439: 'science videos recovered', 515: 'videos recovered algorithms', 415: 'recovered algorithms required', 474: 'supervised', 294: 'learning', 223: 'ensembles', 308: 'logistic', 418: 'regression', 475: 'supervised learning', 297: 'learning ensembles', 224: 'ensembles logistic', 309: 'logistic regression', 476: 'supervised learning ensembles', 298: 'learning ensembles logistic', 225: 'ensembles logistic regression', 73: 'break', 501: 'unsupervised', 121: 'clustering', 201: 'drooping', 502: 'unsupervised learning', 295: 'learning algorithms', 20: 'algorithms algorithms', 22: 'algorithms clustering', 127: 'clustering clustering', 129: 'clustering drooping', 503: 'unsupervised learning algorithms', 296: 'learning algorithms algorithms', 21: 'algorithms algorithms clustering', 23: 'algorithms clustering clustering', 128: 'clustering clustering drooping', 358: 'objects', 277: 'individuals', 480: 'tastes', 405: 'products', 280: 'interchangeable', 278: 'individuals tastes', 481: 'tastes products', 406: 'products interchangeable', 279: 'individuals tastes products', 482: 'tastes products interchangeable', 160: 'customer', 11: 'age', 245: 'gender', 273: 'income', 12: 'age gender', 246: 'gender income', 13: 'age gender income', 394: 'pretty', 363: 'obvious', 35: 'appeals', 3: '20', 522: 'year', 236: 'feels', 9: '60', 395: 'pretty obvious', 364: 'obvious appeals', 36: 'appeals 20', 4: '20 year', 523: 'year feels', 237: 'feels 60', 10: '60 year', 396: 'pretty obvious appeals', 365: 'obvious appeals 20', 37: 'appeals 20 year', 5: '20 year feels', 524: 'year feels 60', 238: 'feels 60 year', 248: 'group', 372: 'people', 228: 'events', 331: 'music', 324: 'movies', 307: 'location', 251: 'group people', 373: 'people objects', 361: 'objects events', 229: 'events music', 332: 'music movies', 325: 'movies location', 252: 'group people objects', 374: 'people objects events', 362: 'objects events music', 230: 'events music movies', 333: 'music movies location', 82: 'buying', 256: 'house', 343: 'number', 62: 'bathrooms', 83: 'buying house', 257: 'house number', 344: 'number bathrooms', 84: 'buying house number', 258: 'house number bathrooms', 63: 'bedrooms', 345: 'number bedrooms', 148: 'condo', 32: 'apartment', 205: 'duplex', 334: 'neighborhoods', 102: 'characteristics', 326: 'multiple', 259: 'houses', 30: 'answer', 81: 'buyer', 149: 'condo apartment', 33: 'apartment duplex', 206: 'duplex neighborhoods', 337: 'neighborhoods characteristics', 103: 'characteristics multiple', 329: 'multiple houses', 260: 'houses neighborhoods', 335: 'neighborhoods answer', 31: 'answer buyer', 150: 'condo apartment duplex', 34: 'apartment duplex neighborhoods', 207: 'duplex neighborhoods characteristics', 338: 'neighborhoods characteristics multiple', 104: 'characteristics multiple houses', 330: 'multiple houses neighborhoods', 261: 'houses neighborhoods answer', 336: 'neighborhoods answer buyer', 447: 'similarity', 313: 'measure', 190: 'distance', 0: '12', 495: 'touring', 239: 'figure', 231: 'express', 483: 'terms', 354: 'numbers', 255: 'hair', 448: 'similarity measure', 314: 'measure distance', 193: 'distance objects', 359: 'objects 12', 1: '12 touring', 496: 'touring figure', 240: 'figure express', 232: 'express characteristics', 105: 'characteristics terms', 484: 'terms numbers', 355: 'numbers group', 249: 'group hair', 449: 'similarity measure distance', 315: 'measure distance objects', 194: 'distance objects 12', 360: 'objects 12 touring', 2: '12 touring figure', 497: 'touring figure express', 241: 'figure express characteristics', 233: 'express characteristics terms', 106: 'characteristics terms numbers', 485: 'terms numbers group', 356: 'numbers group hair', 208: 'dye', 144: 'color', 209: 'dye color', 110: 'close', 74: 'brown', 53: 'auburn', 111: 'close brown', 75: 'brown auburn', 112: 'close brown auburn', 145: 'colors', 431: 'rgb', 423: 'representation', 446: 'similarities', 146: 'colors rgb', 432: 'rgb representation', 424: 'representation numbers', 357: 'numbers similarities', 147: 'colors rgb representation', 433: 'rgb representation numbers', 425: 'representation numbers similarities', 304: 'list', 286: 'killer', 303: 'library', 305: 'list algorithms', 24: 'algorithms killer', 287: 'killer library', 306: 'list algorithms killer', 25: 'algorithms killer library', 45: 'approaches', 327: 'multiple approaches', 46: 'approaches clustering', 328: 'multiple approaches clustering', 492: 'top', 198: 'down', 70: 'bottom', 504: 'up', 42: 'approach', 493: 'top down', 199: 'down bottom', 71: 'bottom up', 505: 'up approach', 494: 'top down bottom', 200: 'down bottom up', 72: 'bottom up approach', 132: 'clusters', 164: 'data', 387: 'points', 397: 'process', 284: 'jordan', 375: 'performs', 350: 'number data', 165: 'data points', 392: 'points process', 399: 'process jordan', 285: 'jordan performs', 351: 'number data points', 166: 'data points process', 393: 'points process jordan', 400: 'process jordan performs', 157: 'cover', 158: 'cover algorithms', 464: 'starting', 477: 'tammy', 64: 'best', 288: 'known', 14: 'algorithm', 210: 'easiest', 498: 'understand', 456: 'stands', 478: 'tammy best', 65: 'best known', 289: 'known clustering', 122: 'clustering algorithm', 15: 'algorithm easiest', 213: 'easiest understand', 499: 'understand stands', 459: 'stands number', 346: 'number clusters', 479: 'tammy best known', 66: 'best known clustering', 290: 'known clustering algorithm', 123: 'clustering algorithm easiest', 16: 'algorithm easiest understand', 214: 'easiest understand stands', 500: 'understand stands number', 460: 'stands number clusters', 519: 'works', 27: 'animation', 226: 'entire', 520: 'works animation', 28: 'animation entire', 227: 'entire process', 521: 'works animation entire', 29: 'animation entire process', 468: 'step', 376: 'pick', 471: 'step pick', 378: 'pick starting', 466: 'starting points', 472: 'step pick starting', 379: 'pick starting points', 450: 'sister', 380: 'picked', 195: 'distinct', 113: 'cluster', 95: 'centers', 506: 'upper', 299: 'left', 262: 'image', 211: 'easiest sister', 451: 'sister picked', 381: 'picked distinct', 196: 'distinct points', 388: 'points cluster', 114: 'cluster centers', 99: 'centers upper', 507: 'upper left', 302: 'left image', 212: 'easiest sister picked', 452: 'sister picked distinct', 382: 'picked distinct points', 197: 'distinct points cluster', 389: 'points cluster centers', 116: 'cluster centers upper', 100: 'centers upper left', 508: 'upper left image', 383: 'picked points', 234: 'farthest', 235: 'farthest left', 50: 'associate', 384: 'point', 187: 'discuss', 340: 'note', 91: 'center', 469: 'step associate', 51: 'associate point', 385: 'point discuss', 188: 'discuss centers', 97: 'centers image', 264: 'image note', 341: 'note process', 401: 'process left', 300: 'left center', 94: 'center points', 470: 'step associate point', 52: 'associate point discuss', 386: 'point discuss centers', 189: 'discuss centers image', 98: 'centers image note', 265: 'image note process', 342: 'note process left', 402: 'process left center', 301: 'left center points', 427: 'resent', 419: 'repeat', 281: 'iterations', 321: 'movements', 67: 'blue', 173: 'decided', 486: 'threshold', 428: 'resent clusters', 142: 'clusters repeat', 420: 'repeat process', 403: 'process number', 352: 'number iterations', 282: 'iterations center', 92: 'center movements', 322: 'movements blue', 68: 'blue decided', 176: 'decided threshold', 429: 'resent clusters repeat', 143: 'clusters repeat process', 422: 'repeat process number', 404: 'process number iterations', 353: 'number iterations center', 283: 'iterations center movements', 93: 'center movements blue', 323: 'movements blue decided', 69: 'blue decided threshold', 107: 'choice', 271: 'impacts', 430: 'result', 108: 'choice starting', 390: 'points impacts', 272: 'impacts result', 109: 'choice starting points', 467: 'starting points impacts', 391: 'points impacts result', 440: 'sense', 101: 'centres', 441: 'sense repeat', 398: 'process centres', 442: 'sense repeat process', 421: 'repeat process centres', 310: 'looked', 311: 'looked starting', 465: 'starting image', 312: 'looked starting image', 516: 'visualized', 222: 'ended', 517: 'visualized clusters', 137: 'clusters ended', 518: 'visualized clusters ended', 170: 'decide', 171: 'decide number', 172: 'decide number clusters', 266: 'impact', 184: 'diagram', 140: 'clusters impact', 269: 'impact distance', 191: 'distance cluster', 96: 'centers diagram', 349: 'number clusters impact', 141: 'clusters impact distance', 270: 'impact distance cluster', 192: 'distance cluster centers', 115: 'cluster centers diagram', 407: 'quick', 202: 'drop', 461: 'start', 274: 'increasing', 408: 'quick drop', 203: 'drop start', 462: 'start increasing', 275: 'increasing number', 409: 'quick drop start', 204: 'drop start increasing', 463: 'start increasing number', 276: 'increasing number clusters', 85: 'called', 215: 'elbow', 320: 'method', 86: 'called elbow', 216: 'elbow method', 87: 'called elbow method', 159: 'curve', 377: 'pick curve', 76: 'build', 43: 'approach build', 77: 'build cluster', 44: 'approach build cluster', 416: 'reduce', 138: 'clusters group', 253: 'group reduce', 417: 'reduce numbers', 348: 'number clusters group', 139: 'clusters group reduce', 254: 'group reduce numbers', 177: 'deliberative', 291: 'layer', 6: '50', 319: 'merge', 178: 'deliberative clustering', 17: 'algorithm layer', 292: 'layer diagram', 185: 'diagram 50', 7: '50 group', 250: 'group merge', 179: 'deliberative clustering algorithm', 124: 'clustering algorithm layer', 18: 'algorithm layer diagram', 293: 'layer diagram 50', 186: 'diagram 50 group', 8: '50 group merge', 316: 'mention', 167: 'dbscan', 180: 'density', 57: 'based', 453: 'spatial', 40: 'applications', 339: 'noise', 317: 'mention dbscan', 168: 'dbscan stands', 457: 'stands density', 181: 'density based', 60: 'based spatial', 454: 'spatial clustering', 125: 'clustering applications', 41: 'applications noise', 318: 'mention dbscan stands', 169: 'dbscan stands density', 458: 'stands density based', 182: 'density based spatial', 61: 'based spatial clustering', 455: 'spatial clustering applications', 126: 'clustering applications noise', 217: 'elderly', 220: 'eliminate', 369: 'outliers', 218: 'elderly eliminate', 221: 'eliminate outliers', 219: 'elderly eliminate outliers', 267: 'impact clustering', 130: 'clustering image', 263: 'image clusters', 268: 'impact clustering image', 131: 'clustering image clusters', 443: 'shades', 410: 'readings', 161: 'dance', 366: 'off', 153: 'core', 47: 'area', 434: 'samples', 119: 'cluster shades', 444: 'shades readings', 411: 'readings dance', 162: 'dance off', 367: 'off core', 154: 'core area', 48: 'area clusters', 135: 'clusters core', 156: 'core samples', 120: 'cluster shades readings', 445: 'shades readings dance', 412: 'readings dance off', 163: 'dance off core', 368: 'off core area', 155: 'core area clusters', 49: 'area clusters core', 136: 'clusters core samples', 242: 'finally', 509: 'values', 370: 'outliers values', 247: 'great', 151: 'considered', 117: 'cluster considered', 152: 'considered noise', 118: 'cluster considered noise', 54: 'automatically', 371: 'parameters', 133: 'clusters automatically', 55: 'automatically decided', 174: 'decided based', 58: 'based density', 183: 'density parameters', 347: 'number clusters automatically', 134: 'clusters automatically decided', 56: 'automatically decided based', 175: 'decided based density', 59: 'based density parameters', 510: 'video', 88: 'case', 38: 'application', 511: 'video case', 89: 'case application', 39: 'application algorithms', 512: 'video case application', 90: 'case application algorithms', 491: 'time', 243: 'forget', 473: 'subscribe', 436: 'science forget', 244: 'forget subscribe', 489: 'ties science forget', 437: 'science forget subscribe'}\n"
     ]
    }
   ],
   "source": [
    "word_graph, idx2word = build_word_graph(sents_after)\n",
    "print(word_graph.shape)\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0   # diagonal 부분 -> 0으로 바꿔줌(diagonal matrix)\n",
    "        link_sum = np.sum(A[:, id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    \n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.9393160724618903,\n",
       " 1: 0.22345147584497832,\n",
       " 2: 0.15000000000000002,\n",
       " 3: 1.8737618357205856,\n",
       " 4: 0.6130794269312003,\n",
       " 5: 0.9999999999999998,\n",
       " 6: 0.15000000000000002,\n",
       " 7: 0.9999999999999998,\n",
       " 8: 0.15000000000000002,\n",
       " 9: 0.9999999999999998,\n",
       " 10: 0.6996554015044547,\n",
       " 11: 0.6459902030898118,\n",
       " 12: 0.8812406870758399,\n",
       " 13: 0.2984911013572488,\n",
       " 14: 0.8847084664283494,\n",
       " 15: 0.15000000000000002,\n",
       " 16: 0.9999999999999998,\n",
       " 17: 0.15000000000000002,\n",
       " 18: 0.3227035723169527,\n",
       " 19: 2.0866566757758376,\n",
       " 20: 1.583984144099648,\n",
       " 21: 0.7829481002984047,\n",
       " 22: 0.8819855829194424,\n",
       " 23: 0.3411087519189534,\n",
       " 24: 1.657752557379373,\n",
       " 25: 1.271563724622544,\n",
       " 26: 1.0728599767780531,\n",
       " 27: 0.94406293975779,\n",
       " 28: 1.4242170057875143,\n",
       " 29: 0.4518320816202148,\n",
       " 30: 1.5174930071520718,\n",
       " 31: 1.4395108099139957,\n",
       " 32: 0.8669801525239387,\n",
       " 33: 0.3659041998645545,\n",
       " 34: 1.300939549652392,\n",
       " 35: 1.305990914010611,\n",
       " 36: 0.9778164116982234,\n",
       " 37: 0.5461748576328475,\n",
       " 38: 1.0168933328119203,\n",
       " 39: 0.8403099388282287,\n",
       " 40: 1.624237093453213,\n",
       " 41: 1.8926565312998012,\n",
       " 42: 1.0865140600438477,\n",
       " 43: 0.15000000000000002,\n",
       " 44: 0.3625867199812462,\n",
       " 45: 0.8266110637563713,\n",
       " 46: 1.8302081099295018,\n",
       " 47: 0.8437500007694396,\n",
       " 48: 0.7949529923388611,\n",
       " 49: 0.9999999999999998,\n",
       " 50: 1.8874909065195078,\n",
       " 51: 0.7246138019729993,\n",
       " 52: 0.15000000000000002,\n",
       " 53: 0.9999999999999998,\n",
       " 54: 0.15000000000000002,\n",
       " 55: 0.6694566223096592,\n",
       " 56: 1.2553761502615022,\n",
       " 57: 0.7829481002984047,\n",
       " 58: 0.15000000000000002,\n",
       " 59: 0.35921488928777334}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_rank_idx = get_ranks(sent_graph)  # 문장 가중치 그래프\n",
    "sent_rank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19,\n",
       " 41,\n",
       " 50,\n",
       " 3,\n",
       " 46,\n",
       " 24,\n",
       " 40,\n",
       " 20,\n",
       " 30,\n",
       " 31,\n",
       " 28,\n",
       " 35,\n",
       " 34,\n",
       " 25,\n",
       " 56,\n",
       " 42,\n",
       " 26,\n",
       " 38,\n",
       " 5,\n",
       " 7,\n",
       " 9,\n",
       " 16,\n",
       " 49,\n",
       " 53,\n",
       " 36,\n",
       " 27,\n",
       " 0,\n",
       " 14,\n",
       " 22,\n",
       " 12,\n",
       " 32,\n",
       " 47,\n",
       " 39,\n",
       " 45,\n",
       " 48,\n",
       " 21,\n",
       " 57,\n",
       " 51,\n",
       " 10,\n",
       " 55,\n",
       " 11,\n",
       " 4,\n",
       " 37,\n",
       " 29,\n",
       " 33,\n",
       " 44,\n",
       " 59,\n",
       " 23,\n",
       " 18,\n",
       " 13,\n",
       " 1,\n",
       " 2,\n",
       " 6,\n",
       " 8,\n",
       " 15,\n",
       " 17,\n",
       " 43,\n",
       " 52,\n",
       " 54,\n",
       " 58]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sent_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "sorted_sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[343,\n",
       " 132,\n",
       " 397,\n",
       " 387,\n",
       " 121,\n",
       " 346,\n",
       " 19,\n",
       " 113,\n",
       " 248,\n",
       " 95,\n",
       " 262,\n",
       " 91,\n",
       " 299,\n",
       " 354,\n",
       " 210,\n",
       " 102,\n",
       " 456,\n",
       " 14,\n",
       " 122,\n",
       " 173,\n",
       " 294,\n",
       " 57,\n",
       " 180,\n",
       " 190,\n",
       " 114,\n",
       " 468,\n",
       " 369,\n",
       " 435,\n",
       " 487,\n",
       " 488,\n",
       " 420,\n",
       " 419,\n",
       " 184,\n",
       " 358,\n",
       " 402,\n",
       " 187,\n",
       " 189,\n",
       " 342,\n",
       " 50,\n",
       " 264,\n",
       " 300,\n",
       " 341,\n",
       " 386,\n",
       " 470,\n",
       " 51,\n",
       " 52,\n",
       " 98,\n",
       " 265,\n",
       " 385,\n",
       " 97,\n",
       " 188,\n",
       " 340,\n",
       " 384,\n",
       " 469,\n",
       " 94,\n",
       " 401,\n",
       " 301,\n",
       " 42,\n",
       " 281,\n",
       " 321,\n",
       " 283,\n",
       " 176,\n",
       " 352,\n",
       " 422,\n",
       " 429,\n",
       " 322,\n",
       " 404,\n",
       " 428,\n",
       " 486,\n",
       " 67,\n",
       " 69,\n",
       " 282,\n",
       " 323,\n",
       " 427,\n",
       " 68,\n",
       " 92,\n",
       " 93,\n",
       " 142,\n",
       " 403,\n",
       " 353,\n",
       " 143,\n",
       " 466,\n",
       " 249,\n",
       " 241,\n",
       " 255,\n",
       " 1,\n",
       " 193,\n",
       " 231,\n",
       " 233,\n",
       " 356,\n",
       " 360,\n",
       " 449,\n",
       " 497,\n",
       " 0,\n",
       " 2,\n",
       " 105,\n",
       " 106,\n",
       " 239,\n",
       " 315,\n",
       " 355,\n",
       " 447,\n",
       " 448,\n",
       " 483,\n",
       " 485,\n",
       " 496,\n",
       " 232,\n",
       " 194,\n",
       " 240,\n",
       " 484,\n",
       " 313,\n",
       " 495,\n",
       " 359,\n",
       " 314,\n",
       " 464,\n",
       " 339,\n",
       " 326,\n",
       " 80,\n",
       " 438,\n",
       " 513,\n",
       " 79,\n",
       " 413,\n",
       " 414,\n",
       " 490,\n",
       " 515,\n",
       " 26,\n",
       " 415,\n",
       " 439,\n",
       " 514,\n",
       " 78,\n",
       " 426,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 389,\n",
       " 452,\n",
       " 99,\n",
       " 100,\n",
       " 211,\n",
       " 302,\n",
       " 382,\n",
       " 388,\n",
       " 450,\n",
       " 197,\n",
       " 381,\n",
       " 508,\n",
       " 116,\n",
       " 195,\n",
       " 506,\n",
       " 507,\n",
       " 451,\n",
       " 196,\n",
       " 212,\n",
       " 214,\n",
       " 290,\n",
       " 499,\n",
       " 500,\n",
       " 15,\n",
       " 477,\n",
       " 498,\n",
       " 64,\n",
       " 288,\n",
       " 460,\n",
       " 478,\n",
       " 16,\n",
       " 65,\n",
       " 213,\n",
       " 289,\n",
       " 459,\n",
       " 66,\n",
       " 479,\n",
       " 123,\n",
       " 318,\n",
       " 167,\n",
       " 169,\n",
       " 60,\n",
       " 182,\n",
       " 317,\n",
       " 458,\n",
       " 40,\n",
       " 61,\n",
       " 125,\n",
       " 168,\n",
       " 181,\n",
       " 316,\n",
       " 454,\n",
       " 455,\n",
       " 41,\n",
       " 453,\n",
       " 126,\n",
       " 457,\n",
       " 368,\n",
       " 163,\n",
       " 155,\n",
       " 48,\n",
       " 153,\n",
       " 162,\n",
       " 366,\n",
       " 49,\n",
       " 120,\n",
       " 156,\n",
       " 410,\n",
       " 411,\n",
       " 443,\n",
       " 445,\n",
       " 47,\n",
       " 119,\n",
       " 154,\n",
       " 367,\n",
       " 412,\n",
       " 434,\n",
       " 135,\n",
       " 136,\n",
       " 444,\n",
       " 161,\n",
       " 493,\n",
       " 494,\n",
       " 504,\n",
       " 70,\n",
       " 72,\n",
       " 71,\n",
       " 199,\n",
       " 200,\n",
       " 492,\n",
       " 505,\n",
       " 198,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 405,\n",
       " 406,\n",
       " 480,\n",
       " 481,\n",
       " 482,\n",
       " 335,\n",
       " 334,\n",
       " 30,\n",
       " 104,\n",
       " 337,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 205,\n",
       " 259,\n",
       " 329,\n",
       " 81,\n",
       " 103,\n",
       " 206,\n",
       " 207,\n",
       " 260,\n",
       " 261,\n",
       " 330,\n",
       " 338,\n",
       " 336,\n",
       " 20,\n",
       " 21,\n",
       " 129,\n",
       " 127,\n",
       " 201,\n",
       " 501,\n",
       " 23,\n",
       " 128,\n",
       " 295,\n",
       " 296,\n",
       " 502,\n",
       " 503,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 37,\n",
       " 9,\n",
       " 10,\n",
       " 35,\n",
       " 36,\n",
       " 236,\n",
       " 237,\n",
       " 364,\n",
       " 395,\n",
       " 396,\n",
       " 522,\n",
       " 523,\n",
       " 524,\n",
       " 238,\n",
       " 365,\n",
       " 394,\n",
       " 53,\n",
       " 74,\n",
       " 75,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 215,\n",
       " 216,\n",
       " 245,\n",
       " 246,\n",
       " 273,\n",
       " 320,\n",
       " 144,\n",
       " 208,\n",
       " 209,\n",
       " 266,\n",
       " 308,\n",
       " 475,\n",
       " 297,\n",
       " 418,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 309,\n",
       " 474,\n",
       " 298,\n",
       " 476,\n",
       " 177,\n",
       " 186,\n",
       " 6,\n",
       " 17,\n",
       " 18,\n",
       " 179,\n",
       " 7,\n",
       " 8,\n",
       " 124,\n",
       " 319,\n",
       " 178,\n",
       " 185,\n",
       " 250,\n",
       " 292,\n",
       " 293,\n",
       " 291,\n",
       " 380,\n",
       " 272,\n",
       " 107,\n",
       " 108,\n",
       " 271,\n",
       " 430,\n",
       " 109,\n",
       " 391,\n",
       " 390,\n",
       " 467,\n",
       " 510,\n",
       " 512,\n",
       " 511,\n",
       " 39,\n",
       " 38,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 304,\n",
       " 25,\n",
       " 303,\n",
       " 24,\n",
       " 286,\n",
       " 287,\n",
       " 305,\n",
       " 306,\n",
       " 252,\n",
       " 332,\n",
       " 362,\n",
       " 251,\n",
       " 324,\n",
       " 325,\n",
       " 228,\n",
       " 230,\n",
       " 331,\n",
       " 374,\n",
       " 373,\n",
       " 229,\n",
       " 333,\n",
       " 307,\n",
       " 361,\n",
       " 372,\n",
       " 407,\n",
       " 202,\n",
       " 276,\n",
       " 409,\n",
       " 463,\n",
       " 408,\n",
       " 461,\n",
       " 462,\n",
       " 204,\n",
       " 275,\n",
       " 203,\n",
       " 274,\n",
       " 399,\n",
       " 351,\n",
       " 166,\n",
       " 165,\n",
       " 350,\n",
       " 285,\n",
       " 164,\n",
       " 375,\n",
       " 400,\n",
       " 284,\n",
       " 392,\n",
       " 393,\n",
       " 84,\n",
       " 344,\n",
       " 82,\n",
       " 83,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 62,\n",
       " 425,\n",
       " 433,\n",
       " 423,\n",
       " 446,\n",
       " 146,\n",
       " 424,\n",
       " 145,\n",
       " 147,\n",
       " 431,\n",
       " 432,\n",
       " 357,\n",
       " 376,\n",
       " 371,\n",
       " 175,\n",
       " 133,\n",
       " 54,\n",
       " 55,\n",
       " 58,\n",
       " 134,\n",
       " 174,\n",
       " 59,\n",
       " 183,\n",
       " 56,\n",
       " 347,\n",
       " 227,\n",
       " 27,\n",
       " 28,\n",
       " 519,\n",
       " 521,\n",
       " 29,\n",
       " 226,\n",
       " 520,\n",
       " 22,\n",
       " 141,\n",
       " 96,\n",
       " 115,\n",
       " 140,\n",
       " 191,\n",
       " 192,\n",
       " 270,\n",
       " 349,\n",
       " 269,\n",
       " 277,\n",
       " 137,\n",
       " 222,\n",
       " 516,\n",
       " 517,\n",
       " 518,\n",
       " 363,\n",
       " 436,\n",
       " 243,\n",
       " 244,\n",
       " 473,\n",
       " 437,\n",
       " 489,\n",
       " 378,\n",
       " 379,\n",
       " 471,\n",
       " 472,\n",
       " 417,\n",
       " 139,\n",
       " 253,\n",
       " 254,\n",
       " 416,\n",
       " 138,\n",
       " 348,\n",
       " 398,\n",
       " 101,\n",
       " 421,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 370,\n",
       " 509,\n",
       " 44,\n",
       " 76,\n",
       " 43,\n",
       " 77,\n",
       " 311,\n",
       " 310,\n",
       " 465,\n",
       " 312,\n",
       " 131,\n",
       " 267,\n",
       " 268,\n",
       " 130,\n",
       " 263,\n",
       " 327,\n",
       " 45,\n",
       " 46,\n",
       " 328,\n",
       " 152,\n",
       " 117,\n",
       " 151,\n",
       " 118,\n",
       " 158,\n",
       " 157,\n",
       " 159,\n",
       " 377,\n",
       " 345,\n",
       " 63,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 235,\n",
       " 234,\n",
       " 383,\n",
       " 73,\n",
       " 160,\n",
       " 242,\n",
       " 247,\n",
       " 491]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rank_idx = get_ranks(word_graph)\n",
    "sorted_word_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "sorted_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(sorted_word_idx, idx2word, word_num=5):\n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_idx[:word_num]:\n",
    "        index.append(idx)      \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사만 추출할 수는 없나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['number',\n",
       " 'clusters',\n",
       " 'process',\n",
       " 'points',\n",
       " 'clustering',\n",
       " 'number clusters',\n",
       " 'algorithms',\n",
       " 'cluster',\n",
       " 'group',\n",
       " 'centers',\n",
       " 'image',\n",
       " 'center',\n",
       " 'left',\n",
       " 'numbers',\n",
       " 'easiest',\n",
       " 'characteristics',\n",
       " 'stands',\n",
       " 'algorithm',\n",
       " 'clustering algorithm',\n",
       " 'decided',\n",
       " 'learning',\n",
       " 'based',\n",
       " 'density',\n",
       " 'distance',\n",
       " 'cluster centers',\n",
       " 'step',\n",
       " 'outliers',\n",
       " 'science',\n",
       " 'ties',\n",
       " 'ties science',\n",
       " 'repeat process',\n",
       " 'repeat',\n",
       " 'diagram',\n",
       " 'objects',\n",
       " 'process left center',\n",
       " 'discuss',\n",
       " 'discuss centers image',\n",
       " 'note process left',\n",
       " 'associate',\n",
       " 'image note',\n",
       " 'left center',\n",
       " 'note process',\n",
       " 'point discuss centers',\n",
       " 'step associate point',\n",
       " 'associate point',\n",
       " 'associate point discuss',\n",
       " 'centers image note',\n",
       " 'image note process',\n",
       " 'point discuss',\n",
       " 'centers image']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = keywords(sorted_word_idx, idx2word, 50)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keysents(sorted_sent_idx, sent_num=2):\n",
    "    keysents=[]\n",
    "    index=[]\n",
    "    for idx in sorted_sent_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "    for idx in index:\n",
    "        keysents.append(sentences[idx])\n",
    "\n",
    "    return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And so, this will be one step of grade with respect to a single example.',\n",
       " 'It turns out that if you are familiar with calculus, you could show that this ends up being -Y_over_A+1-Y_over_1-A.',\n",
       " 'Welcome back.',\n",
       " \"Here's a cleaned-up version of the diagram.\",\n",
       " \"We'll provide the derivative formulas, what else you need, throughout this course.\",\n",
       " 'The key takeaways will be what you need to implement.',\n",
       " 'Now, having computed this quantity of DA and the derivative or your final alpha variable with respect to A, you can then go backwards.',\n",
       " 'Then, similarly, DW2, which is how much you want to change W2, is X2_times_DZ and B, excuse me, DB is equal to DZ.',\n",
       " 'Then, the final step in that computation is to go back to compute how much you need to change W and B.',\n",
       " 'I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about full-fledged neural networks.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysents = keysents(sorted_sent_idx, sent_num=10)\n",
    "keysents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keysents는 여러 후보 중에서 keywords가 많은 문장을 선택하는 방향으로?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keyword \n",
    "    - return keywords, keyword 순위\n",
    "2. keysents\n",
    "    - return keysents, keysents with blank(keyword), keyword in blank\n",
    "    - keysents 1위부터 돌면서 weight=(각 keysents의 순위 + 갖고 있는 keyword 순위(다수라면 더 낮은 keyword 순위))\n",
    "    - weight가 같은 keysents라면 자체 keysents 순위가 더 높은 keysents 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keysents_blank(keywords:list, keysents:list):\n",
    "    keysent=''   # blank 만들 keysent\n",
    "    keysent_blank=''   # blank 만든 keysent\n",
    "    keyword_keysent=''   # keysent의 blank에 들어갈 keyword\n",
    "    lowest_weight=23   # 가장 작은 weight(초기값: 최대 weight+1)\n",
    "    \n",
    "    for sent in keysents:\n",
    "        sent_weight = keysents.index(sent) + 1 \n",
    "        \n",
    "        keyword=''\n",
    "        for word in keywords:\n",
    "            if word in sent:\n",
    "                keyword = word\n",
    "                break   # keywords 리스트는 앞의 index일수록 순위가 높은 키워드 -> 문장에 존재하면 break    \n",
    "        if keyword!='':\n",
    "            word_weight = keywords.index(keyword) + 1\n",
    "        else:\n",
    "            word_weight = 23\n",
    "            \n",
    "        weight = sent_weight + word_weight\n",
    "        if weight<lowest_weight:\n",
    "            lowest_weight = weight\n",
    "            keysent = sent\n",
    "            keyword_keysent = keyword\n",
    "    \n",
    "    keysent_blank = keysent.replace(keyword_keysent, '__________')\n",
    "    \n",
    "    return {'sentence_blank':keysent_blank, 'sentence':keysent, 'answer':keyword_keysent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_blank': 'And so, this will be one step of grade with __________ to a single example.',\n",
       " 'sentence': 'And so, this will be one step of grade with respect to a single example.',\n",
       " 'answer': 'respect'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysents_blank(keywords, keysents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tc",
   "language": "python",
   "name": "tc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
