{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "    \n",
    "# 1-1) Load the stopwords\n",
    "def get_stopwords(stopwords_path):\n",
    "    stop_words_1 = stopwords.words('english')   # stopwords 179개\n",
    "    with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
    "        stop_words = f.readlines()\n",
    "        stop_words = [word.strip() for word in stop_words]   # stopwords 854개\n",
    "    return stop_words\n",
    "        \n",
    "# 1-2) Load the script  \n",
    "def get_script(script_path):\n",
    "    with open(script_path) as f:\n",
    "        text = f.read()\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [sent.replace('\\n', ' ') for sent in sentences] \n",
    "    return sentences\n",
    "\n",
    "# 3-1) Build the sentence graph\n",
    "def build_sent_graph(sents, tfidf):  # 문장 리스트 -> tf-idf matrix -> sentence graph\n",
    "    graph_sentence = []\n",
    "    tfidf_mat = tfidf.fit_transform(sents).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence\n",
    "\n",
    "# 3-2) Build the word graph\n",
    "def build_word_graph(sent, cnt_vec):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sent).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    graph_word = np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "    idx2word = {vocab[word] : word for word in vocab}\n",
    "    return graph_word, idx2word\n",
    "\n",
    "# 4) Calculate the ranks of each sentence or word\n",
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0   # diagonal 부분 -> 0으로 바꿔줌(diagonal matrix)\n",
    "        link_sum = np.sum(A[:, id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    \n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}\n",
    "\n",
    "# 5-1) Get the list of keywords\n",
    "def get_keywords(sorted_word_idx, idx2word, word_num=5):\n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_idx[:word_num]:\n",
    "        index.append(idx)      \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "# 5-2) Get the list of keysentences\n",
    "def get_keysents(sorted_sent_idx, sentences, sent_num=2):\n",
    "    keysents=[]\n",
    "    index=[]\n",
    "    for idx in sorted_sent_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "    for idx in index:\n",
    "        keysents.append(sentences[idx])\n",
    "\n",
    "    return keysents\n",
    "\n",
    "# 6) Final: Get the sentence with blank, answer sentence, answer word\n",
    "def keysents_blank(keywords:list, keysents:list):\n",
    "    keysent=''   # blank 만들 keysent\n",
    "    keysent_blank=''   # blank 만든 keysent\n",
    "    keyword_keysent=''   # keysent의 blank에 들어갈 keyword\n",
    "    lowest_weight=23   # 가장 작은 weight(초기값: 최대 weight+1)\n",
    "    \n",
    "    for sent in keysents:\n",
    "        sent_weight = keysents.index(sent) + 1 \n",
    "        \n",
    "        keyword=''\n",
    "        for word in keywords:\n",
    "            if word in sent:\n",
    "                keyword = word\n",
    "                break   # keywords 리스트는 앞의 index일수록 순위가 높은 키워드 -> 문장에 존재하면 break    \n",
    "        if keyword!='':\n",
    "            word_weight = keywords.index(keyword) + 1\n",
    "        else:\n",
    "            word_weight = 23\n",
    "            \n",
    "        weight = sent_weight + word_weight\n",
    "        if weight<lowest_weight:\n",
    "            lowest_weight = weight\n",
    "            keysent = sent\n",
    "            keyword_keysent = keyword\n",
    "    \n",
    "    keysent_blank = keysent.replace(keyword_keysent, '__________')\n",
    "    \n",
    "    return {'sentence_blank':keysent_blank, 'sentence':keysent, 'answer':keyword_keysent}\n",
    "\n",
    "def preprocess_sents(sentences, stop_words):\n",
    "    # 2) Preprocess the sentences\n",
    "    sents_after=[]   # stop_words 제거, lower()한 list of sentences\n",
    "    for sent in sentences:\n",
    "        words = word_tokenize(sent)\n",
    "        sents_after.append(' '.join([word.lower() for word in words if word.lower() not in stop_words and len(word)>1]))\n",
    "        sents_after = [s for s in sents_after if s!=''] \n",
    "    return sents_after\n",
    "\n",
    "def run():\n",
    "    # 1-3) Set the algorithm\n",
    "    sent_ngram = 1\n",
    "    word_ngram = 3    \n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, sent_ngram))\n",
    "    cnt_vec = CountVectorizer(ngram_range=(1, word_ngram))\n",
    "    \n",
    "    stopwords_path = 'stop_words_english.txt'\n",
    "    script_path = 'sample_script.txt'\n",
    "    stop_words = get_stopwords(stopwords_path)\n",
    "    sentences = get_script(script_path)\n",
    "    sents_after = preprocess_sents(sentences, stop_words)\n",
    "    \n",
    "    sent_graph = build_sent_graph(sents_after, tfidf)\n",
    "    word_graph, idx2word = build_word_graph(sents_after, cnt_vec)\n",
    "    \n",
    "    sent_rank_idx = get_ranks(sent_graph)  # 문장 가중치 그래프\n",
    "    sorted_sent_idx = sorted(sent_rank_idx,   # 문장 가중치 그래프-가중치 작은 차순 정렬\n",
    "                             key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "    word_rank_idx = get_ranks(word_graph)  # 단어 가중치 그래프\n",
    "    sorted_word_idx = sorted(word_rank_idx, \n",
    "                             key=lambda k: word_rank_idx[k], reverse=True)\n",
    "\n",
    "    keywords = get_keywords(sorted_word_idx, idx2word, word_num=10)\n",
    "    keysents = get_keysents(sorted_sent_idx, sentences, sent_num=10)\n",
    "\n",
    "    return keysents_blank(keywords, keysents)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer, word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome back. In this video,\n",
      "we'll talk about how to compute derivatives for you\n",
      "to implement gradient descent for logistic regression.\n",
      "The key takeaways will be what you need to implement.\n",
      "That is, the key equations you need in order to\n",
      "implement gradient descent for logistic regression.\n",
      "In this video, I want to do this computation using the computation graph.\n",
      "I have to admit, using the computation graph is a little bit of\n",
      "an overkill for deriving gradient descent for logistic regression,\n",
      "but I want to start explaining things this\n",
      "way to get you familiar with these ideas so that,\n",
      "hopefully, it will make a bit more sense when we talk about full-fledged neural networks.\n",
      "To that, let's dive into gradient descent for logistic regression.\n",
      "To recap, we had set up logistic regression as follows,\n",
      "your predictions, Y_hat, is defined as follows,\n",
      "where z is that.\n",
      "If we focus on just one example for now, then the loss,\n",
      "or respect to that one example,\n",
      "is defined as follows,\n",
      "where A is the output of logistic regression,\n",
      "and Y is the ground truth label.\n",
      "Let's write this out as a computation graph and for this example,\n",
      "let's say we have only two features, X1 and X2.\n",
      "In order to compute Z,\n",
      "we'll need to input W1,\n",
      "W2, and B, in addition to the feature values X1, X2.\n",
      "These things, in a computational graph,\n",
      "get used to compute Z, which is W1,\n",
      "X1 + W2 X2 + B,\n",
      "rectangular box around that.\n",
      "Then, we compute Y_hat,\n",
      "or A = Sigma_of_Z,\n",
      "that's the next step in the computation graph, and then, finally,\n",
      "we compute L, AY,\n",
      "and I won't copy the formula again.\n",
      "In logistic regression, what we want to do is to modify the parameters,\n",
      "W and B, in order to reduce this loss.\n",
      "We've described the forward propagation steps of how you actually\n",
      "compute the loss on a single training example,\n",
      "now let's talk about how you can go backwards to compute the derivatives.\n",
      "Here's a cleaned-up version of the diagram.\n",
      "Because what we want to do is compute derivatives with respect to this loss,\n",
      "the first thing we want to do when going backwards is to\n",
      "compute the derivative of this loss with respect to,\n",
      "the script over there, with respect to this variable A.\n",
      "So, in the code,\n",
      "you just use DA to denote this variable.\n",
      "It turns out that if you are familiar with calculus,\n",
      "you could show that this ends up being -Y_over_A+1-Y_over_1-A.\n",
      "And the way you do that is you take the formula for the loss and,\n",
      "if you're familiar with calculus,\n",
      "you can compute the derivative with respect to the variable,\n",
      "lowercase A, and you get this formula.\n",
      "But if you're not familiar with calculus, don't worry about it.\n",
      "We'll provide the derivative formulas,\n",
      "what else you need, throughout this course.\n",
      "If you are an expert in calculus,\n",
      "I encourage you to look up the formula for the loss from\n",
      "their previous slide and try taking derivative with respect to A using calculus,\n",
      "but if you don't know enough calculus to do that, don't worry about it.\n",
      "Now, having computed this quantity of DA and\n",
      "the derivative or your final alpha variable with respect to A,\n",
      "you can then go backwards.\n",
      "It turns out that you can show DZ which,\n",
      "this is the part called variable name,\n",
      "this is going to be the derivative of the loss,\n",
      "with respect to Z, or for L,\n",
      "you could really write the loss including A and Y explicitly as parameters or not, right?\n",
      "Either type of notation is equally acceptable.\n",
      "We can show that this is equal to A-Y.\n",
      "Just a couple of comments only for those of you experts in calculus,\n",
      "if you're not expert in calculus, don't worry about it.\n",
      "But it turns out that this, DL DZ,\n",
      "this can be expressed as DL_DA_times_DA_DZ,\n",
      "and it turns out that DA DZ,\n",
      "this turns out to be A_times_1-A,\n",
      "and DL DA we have previously worked out over here,\n",
      "if you take these two quantities, DL DA,\n",
      "which is this term, together with DA DZ,\n",
      "which is this term, and just take these two things and multiply them.\n",
      "You can show that the equation simplifies to A-Y.\n",
      "That's how you derive it,\n",
      "and that this is really the chain rule that have briefly eluded to the form.\n",
      "Feel free to go through that calculation yourself if you are knowledgeable in calculus,\n",
      "but if you aren't, all you need to know is that you can compute\n",
      "DZ as A-Y and we've already done that calculus for you.\n",
      "Then, the final step in that computation is to go back to\n",
      "compute how much you need to change W and B.\n",
      "In particular, you can show that the derivative with respect to W1 and in quotes,\n",
      "call this DW1, that this is equal to X1_times_DZ.\n",
      "Then, similarly, DW2, which is how much you want to change W2,\n",
      "is X2_times_DZ and B,\n",
      "excuse me, DB is equal to DZ.\n",
      "If you want to do gradient descent with respect to just this one example,\n",
      "what you would do is the following;\n",
      "you would use this formula to compute DZ,\n",
      "and then use these formulas to compute DW1, DW2,\n",
      "and DB, and then you perform these updates.\n",
      "W1 gets updated as W1 minus,\n",
      "learning rate alpha, times DW1.\n",
      "W2 gets updated similarly,\n",
      "and B gets set as B minus the learning rate times DB.\n",
      "And so, this will be one step of grade with respect to a single example.\n",
      "You see in how to compute derivatives and implement\n",
      "gradient descent for logistic regression with respect to a single training example.\n",
      "But training logistic regression model,\n",
      "you have not just one training example given training sets of M training examples.\n",
      "In the next video,\n",
      "let's see how you can take these ideas and apply them to learning,\n",
      "not just from one example,\n",
      "but from an entire training set.\n"
     ]
    }
   ],
   "source": [
    "with open('sample_script.txt') as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "sentences = [sent.replace('\\n', ' ') for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome back.',\n",
       " \"In this video, we'll talk about how to compute derivatives for you to implement gradient descent for logistic regression.\",\n",
       " 'The key takeaways will be what you need to implement.',\n",
       " 'That is, the key equations you need in order to implement gradient descent for logistic regression.',\n",
       " 'In this video, I want to do this computation using the computation graph.',\n",
       " 'I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about full-fledged neural networks.',\n",
       " \"To that, let's dive into gradient descent for logistic regression.\",\n",
       " 'To recap, we had set up logistic regression as follows, your predictions, Y_hat, is defined as follows, where z is that.',\n",
       " 'If we focus on just one example for now, then the loss, or respect to that one example, is defined as follows, where A is the output of logistic regression, and Y is the ground truth label.',\n",
       " \"Let's write this out as a computation graph and for this example, let's say we have only two features, X1 and X2.\",\n",
       " \"In order to compute Z, we'll need to input W1, W2, and B, in addition to the feature values X1, X2.\",\n",
       " 'These things, in a computational graph, get used to compute Z, which is W1, X1 + W2 X2 + B, rectangular box around that.',\n",
       " \"Then, we compute Y_hat, or A = Sigma_of_Z, that's the next step in the computation graph, and then, finally, we compute L, AY, and I won't copy the formula again.\",\n",
       " 'In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss.',\n",
       " \"We've described the forward propagation steps of how you actually compute the loss on a single training example, now let's talk about how you can go backwards to compute the derivatives.\",\n",
       " \"Here's a cleaned-up version of the diagram.\",\n",
       " 'Because what we want to do is compute derivatives with respect to this loss, the first thing we want to do when going backwards is to compute the derivative of this loss with respect to, the script over there, with respect to this variable A.',\n",
       " 'So, in the code, you just use DA to denote this variable.',\n",
       " 'It turns out that if you are familiar with calculus, you could show that this ends up being -Y_over_A+1-Y_over_1-A.',\n",
       " \"And the way you do that is you take the formula for the loss and, if you're familiar with calculus, you can compute the derivative with respect to the variable, lowercase A, and you get this formula.\",\n",
       " \"But if you're not familiar with calculus, don't worry about it.\",\n",
       " \"We'll provide the derivative formulas, what else you need, throughout this course.\",\n",
       " \"If you are an expert in calculus, I encourage you to look up the formula for the loss from their previous slide and try taking derivative with respect to A using calculus, but if you don't know enough calculus to do that, don't worry about it.\",\n",
       " 'Now, having computed this quantity of DA and the derivative or your final alpha variable with respect to A, you can then go backwards.',\n",
       " 'It turns out that you can show DZ which, this is the part called variable name, this is going to be the derivative of the loss, with respect to Z, or for L, you could really write the loss including A and Y explicitly as parameters or not, right?',\n",
       " 'Either type of notation is equally acceptable.',\n",
       " 'We can show that this is equal to A-Y.',\n",
       " \"Just a couple of comments only for those of you experts in calculus, if you're not expert in calculus, don't worry about it.\",\n",
       " 'But it turns out that this, DL DZ, this can be expressed as DL_DA_times_DA_DZ, and it turns out that DA DZ, this turns out to be A_times_1-A, and DL DA we have previously worked out over here, if you take these two quantities, DL DA, which is this term, together with DA DZ, which is this term, and just take these two things and multiply them.',\n",
       " 'You can show that the equation simplifies to A-Y.',\n",
       " \"That's how you derive it, and that this is really the chain rule that have briefly eluded to the form.\",\n",
       " \"Feel free to go through that calculation yourself if you are knowledgeable in calculus, but if you aren't, all you need to know is that you can compute DZ as A-Y and we've already done that calculus for you.\",\n",
       " 'Then, the final step in that computation is to go back to compute how much you need to change W and B.',\n",
       " 'In particular, you can show that the derivative with respect to W1 and in quotes, call this DW1, that this is equal to X1_times_DZ.',\n",
       " 'Then, similarly, DW2, which is how much you want to change W2, is X2_times_DZ and B, excuse me, DB is equal to DZ.',\n",
       " 'If you want to do gradient descent with respect to just this one example, what you would do is the following; you would use this formula to compute DZ, and then use these formulas to compute DW1, DW2, and DB, and then you perform these updates.',\n",
       " 'W1 gets updated as W1 minus, learning rate alpha, times DW1.',\n",
       " 'W2 gets updated similarly, and B gets set as B minus the learning rate times DB.',\n",
       " 'And so, this will be one step of grade with respect to a single example.',\n",
       " 'You see in how to compute derivatives and implement gradient descent for logistic regression with respect to a single training example.',\n",
       " 'But training logistic regression model, you have not just one training example given training sets of M training examples.',\n",
       " \"In the next video, let's see how you can take these ideas and apply them to learning, not just from one example, but from an entire training set.\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_1 = stopwords.words('english')\n",
    "len(stop_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abroad',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'across',\n",
       " 'actually',\n",
       " 'adj',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ago',\n",
       " 'ahead',\n",
       " \"ain't\",\n",
       " 'all',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'alongside',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amid',\n",
       " 'amidst',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anybody',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anyways',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appear',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'around',\n",
       " 'as',\n",
       " \"a's\",\n",
       " 'aside',\n",
       " 'ask',\n",
       " 'asking',\n",
       " 'associated',\n",
       " 'at',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awfully',\n",
       " 'back',\n",
       " 'backward',\n",
       " 'backwards',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believe',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'best',\n",
       " 'better',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'brief',\n",
       " 'but',\n",
       " 'by',\n",
       " 'came',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " \"can't\",\n",
       " 'caption',\n",
       " 'cause',\n",
       " 'causes',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'changes',\n",
       " 'clearly',\n",
       " \"c'mon\",\n",
       " 'co',\n",
       " 'co.',\n",
       " 'com',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'compute',\n",
       " 'computes',\n",
       " 'con',\n",
       " 'concerning',\n",
       " 'consequently',\n",
       " 'consider',\n",
       " 'considering',\n",
       " 'contain',\n",
       " 'containing',\n",
       " 'contains',\n",
       " 'corresponding',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'course',\n",
       " \"c's\",\n",
       " 'currently',\n",
       " 'dare',\n",
       " \"daren't\",\n",
       " 'definitely',\n",
       " 'described',\n",
       " 'despite',\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'different',\n",
       " 'directly',\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'done',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'downwards',\n",
       " 'during',\n",
       " 'each',\n",
       " 'edu',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'eighty',\n",
       " 'either',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'end',\n",
       " 'ending',\n",
       " 'enough',\n",
       " 'entirely',\n",
       " 'especially',\n",
       " 'et',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'evermore',\n",
       " 'every',\n",
       " 'everybody',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'ex',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'except',\n",
       " 'fairly',\n",
       " 'far',\n",
       " 'farther',\n",
       " 'few',\n",
       " 'fewer',\n",
       " 'fifth',\n",
       " 'first',\n",
       " 'five',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'follows',\n",
       " 'for',\n",
       " 'forever',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forth',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'further',\n",
       " 'furthermore',\n",
       " 'get',\n",
       " 'gets',\n",
       " 'getting',\n",
       " 'given',\n",
       " 'gives',\n",
       " 'go',\n",
       " 'goes',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'greetings',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'half',\n",
       " 'happens',\n",
       " 'hardly',\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " \"here's\",\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hither',\n",
       " 'hopefully',\n",
       " 'how',\n",
       " 'howbeit',\n",
       " 'however',\n",
       " 'hundred',\n",
       " \"i'd\",\n",
       " 'ie',\n",
       " 'if',\n",
       " 'ignored',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'immediate',\n",
       " 'in',\n",
       " 'inasmuch',\n",
       " 'inc',\n",
       " 'inc.',\n",
       " 'indeed',\n",
       " 'indicate',\n",
       " 'indicated',\n",
       " 'indicates',\n",
       " 'inner',\n",
       " 'inside',\n",
       " 'insofar',\n",
       " 'instead',\n",
       " 'into',\n",
       " 'inward',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " 'its',\n",
       " \"it's\",\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'k',\n",
       " 'keep',\n",
       " 'keeps',\n",
       " 'kept',\n",
       " 'know',\n",
       " 'known',\n",
       " 'knows',\n",
       " 'last',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'lest',\n",
       " 'let',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'likewise',\n",
       " 'little',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looks',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'mainly',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'many',\n",
       " 'may',\n",
       " 'maybe',\n",
       " \"mayn't\",\n",
       " 'me',\n",
       " 'mean',\n",
       " 'meantime',\n",
       " 'meanwhile',\n",
       " 'merely',\n",
       " 'might',\n",
       " \"mightn't\",\n",
       " 'mine',\n",
       " 'minus',\n",
       " 'miss',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'mr',\n",
       " 'mrs',\n",
       " 'much',\n",
       " 'must',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'nd',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'necessary',\n",
       " 'need',\n",
       " \"needn't\",\n",
       " 'needs',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'neverf',\n",
       " 'neverless',\n",
       " 'nevertheless',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'ninety',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'non',\n",
       " 'none',\n",
       " 'nonetheless',\n",
       " 'noone',\n",
       " 'no-one',\n",
       " 'nor',\n",
       " 'normally',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'notwithstanding',\n",
       " 'novel',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'obviously',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " \"one's\",\n",
       " 'only',\n",
       " 'onto',\n",
       " 'opposite',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " \"oughtn't\",\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outside',\n",
       " 'over',\n",
       " 'overall',\n",
       " 'own',\n",
       " 'particular',\n",
       " 'particularly',\n",
       " 'past',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'placed',\n",
       " 'please',\n",
       " 'plus',\n",
       " 'possible',\n",
       " 'presumably',\n",
       " 'probably',\n",
       " 'provided',\n",
       " 'provides',\n",
       " 'que',\n",
       " 'quite',\n",
       " 'qv',\n",
       " 'rather',\n",
       " 'rd',\n",
       " 're',\n",
       " 'really',\n",
       " 'reasonably',\n",
       " 'recent',\n",
       " 'recently',\n",
       " 'regarding',\n",
       " 'regardless',\n",
       " 'regards',\n",
       " 'relatively',\n",
       " 'respectively',\n",
       " 'right',\n",
       " 'round',\n",
       " 'said',\n",
       " 'same',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'second',\n",
       " 'secondly',\n",
       " 'see',\n",
       " 'seeing',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'seen',\n",
       " 'self',\n",
       " 'selves',\n",
       " 'sensible',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'seriously',\n",
       " 'seven',\n",
       " 'several',\n",
       " 'shall',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'six',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somebody',\n",
       " 'someday',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'sorry',\n",
       " 'specified',\n",
       " 'specify',\n",
       " 'specifying',\n",
       " 'still',\n",
       " 'sub',\n",
       " 'such',\n",
       " 'sup',\n",
       " 'sure',\n",
       " 'take',\n",
       " 'taken',\n",
       " 'taking',\n",
       " 'tell',\n",
       " 'tends',\n",
       " 'th',\n",
       " 'than',\n",
       " 'thank',\n",
       " 'thanks',\n",
       " 'thanx',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'thats',\n",
       " \"that's\",\n",
       " \"that've\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " \"there'd\",\n",
       " 'therefore',\n",
       " 'therein',\n",
       " \"there'll\",\n",
       " \"there're\",\n",
       " 'theres',\n",
       " \"there's\",\n",
       " 'thereupon',\n",
       " \"there've\",\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'thing',\n",
       " 'things',\n",
       " 'think',\n",
       " 'third',\n",
       " 'thirty',\n",
       " 'this',\n",
       " 'thorough',\n",
       " 'thoroughly',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'till',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'took',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'tried',\n",
       " 'tries',\n",
       " 'truly',\n",
       " 'try',\n",
       " 'trying',\n",
       " \"t's\",\n",
       " 'twice',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'underneath',\n",
       " 'undoing',\n",
       " 'unfortunately',\n",
       " 'unless',\n",
       " 'unlike',\n",
       " 'unlikely',\n",
       " 'until',\n",
       " 'unto',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'upwards',\n",
       " 'us',\n",
       " 'use',\n",
       " 'used',\n",
       " 'useful',\n",
       " 'uses',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'v',\n",
       " 'value',\n",
       " 'various',\n",
       " 'versus',\n",
       " 'very',\n",
       " 'via',\n",
       " 'viz',\n",
       " 'vs',\n",
       " 'want',\n",
       " 'wants',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'way',\n",
       " 'we',\n",
       " \"we'd\",\n",
       " 'welcome',\n",
       " 'well',\n",
       " \"we'll\",\n",
       " 'went',\n",
       " 'were',\n",
       " \"we're\",\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'whatever',\n",
       " \"what'll\",\n",
       " \"what's\",\n",
       " \"what've\",\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " \"where's\",\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'whichever',\n",
       " 'while',\n",
       " 'whilst',\n",
       " 'whither',\n",
       " 'who',\n",
       " \"who'd\",\n",
       " 'whoever',\n",
       " 'whole',\n",
       " \"who'll\",\n",
       " 'whom',\n",
       " 'whomever',\n",
       " \"who's\",\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'willing',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'wonder',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'yes',\n",
       " 'yet',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\",\n",
       " 'zero',\n",
       " 'a',\n",
       " \"how's\",\n",
       " 'i',\n",
       " \"when's\",\n",
       " \"why's\",\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'j',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'uucp',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " 'I',\n",
       " 'www',\n",
       " 'amount',\n",
       " 'bill',\n",
       " 'bottom',\n",
       " 'call',\n",
       " 'computer',\n",
       " 'con',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'due',\n",
       " 'eleven',\n",
       " 'empty',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'forty',\n",
       " 'front',\n",
       " 'full',\n",
       " 'give',\n",
       " 'hasnt',\n",
       " 'herse',\n",
       " 'himse',\n",
       " 'interest',\n",
       " 'itse”',\n",
       " 'mill',\n",
       " 'move',\n",
       " 'myse”',\n",
       " 'part',\n",
       " 'put',\n",
       " 'show',\n",
       " 'side',\n",
       " 'sincere',\n",
       " 'sixty',\n",
       " 'system',\n",
       " 'ten',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'top',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'abst',\n",
       " 'accordance',\n",
       " 'act',\n",
       " 'added',\n",
       " 'adopted',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affects',\n",
       " 'ah',\n",
       " 'announce',\n",
       " 'anymore',\n",
       " 'apparently',\n",
       " 'approximately',\n",
       " 'aren',\n",
       " 'arent',\n",
       " 'arise',\n",
       " 'auth',\n",
       " 'beginning',\n",
       " 'beginnings',\n",
       " 'begins',\n",
       " 'biol',\n",
       " 'briefly',\n",
       " 'ca',\n",
       " 'date',\n",
       " 'ed',\n",
       " 'effect',\n",
       " 'et-al',\n",
       " 'ff',\n",
       " 'fix',\n",
       " 'gave',\n",
       " 'giving',\n",
       " 'heres',\n",
       " 'hes',\n",
       " 'hid',\n",
       " 'home',\n",
       " 'id',\n",
       " 'im',\n",
       " 'immediately',\n",
       " 'importance',\n",
       " 'important',\n",
       " 'index',\n",
       " 'information',\n",
       " 'invention',\n",
       " 'itd',\n",
       " 'keys',\n",
       " 'kg',\n",
       " 'km',\n",
       " 'largely',\n",
       " 'lets',\n",
       " 'line',\n",
       " \"'ll\",\n",
       " 'means',\n",
       " 'mg',\n",
       " 'million',\n",
       " 'ml',\n",
       " 'mug',\n",
       " 'na',\n",
       " 'nay',\n",
       " 'necessarily',\n",
       " 'nos',\n",
       " 'noted',\n",
       " 'obtain',\n",
       " 'obtained',\n",
       " 'omitted',\n",
       " 'ord',\n",
       " 'owing',\n",
       " 'page',\n",
       " 'pages',\n",
       " 'poorly',\n",
       " 'possibly',\n",
       " 'potentially',\n",
       " 'pp',\n",
       " 'predominantly',\n",
       " 'present',\n",
       " 'previously',\n",
       " 'primarily',\n",
       " 'promptly',\n",
       " 'proud',\n",
       " 'quickly',\n",
       " 'ran',\n",
       " 'readily',\n",
       " 'ref',\n",
       " 'refs',\n",
       " 'related',\n",
       " 'research',\n",
       " 'resulted',\n",
       " 'resulting',\n",
       " 'results',\n",
       " 'run',\n",
       " 'sec',\n",
       " 'section',\n",
       " 'shed',\n",
       " 'shes',\n",
       " 'showed',\n",
       " 'shown',\n",
       " 'showns',\n",
       " 'shows',\n",
       " 'significant',\n",
       " 'significantly',\n",
       " 'similar',\n",
       " 'similarly',\n",
       " 'slightly',\n",
       " 'somethan',\n",
       " 'specifically',\n",
       " 'state',\n",
       " 'states',\n",
       " 'stop',\n",
       " 'strongly',\n",
       " 'substantially',\n",
       " 'successfully',\n",
       " 'sufficiently',\n",
       " 'suggest',\n",
       " 'thered',\n",
       " 'thereof',\n",
       " 'therere',\n",
       " 'thereto',\n",
       " 'theyd',\n",
       " 'theyre',\n",
       " 'thou',\n",
       " 'thoughh',\n",
       " 'thousand',\n",
       " 'throug',\n",
       " 'til',\n",
       " 'tip',\n",
       " 'ts',\n",
       " 'ups',\n",
       " 'usefully',\n",
       " 'usefulness',\n",
       " \"'ve\",\n",
       " 'vol',\n",
       " 'vols',\n",
       " 'wed',\n",
       " 'whats',\n",
       " 'wheres',\n",
       " 'whim',\n",
       " 'whod',\n",
       " 'whos',\n",
       " 'widely',\n",
       " 'words',\n",
       " 'world',\n",
       " 'youd',\n",
       " 'youre']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stop_words_english.txt', 'r', encoding='utf-8') as f:\n",
    "    stop_words = f.readlines()\n",
    "    stop_words = [word.strip() for word in stop_words]\n",
    "stop_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "854"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video talk derivatives implement gradient descent logistic regression',\n",
       " 'key takeaways implement',\n",
       " 'key equations order implement gradient descent logistic regression',\n",
       " 'video computation computation graph',\n",
       " 'admit computation graph bit overkill deriving gradient descent logistic regression start explaining familiar ideas bit sense talk full-fledged neural networks',\n",
       " \"'s dive gradient descent logistic regression\",\n",
       " 'recap set logistic regression predictions y_hat defined',\n",
       " 'focus loss respect defined output logistic regression ground truth label',\n",
       " \"'s write computation graph 's features x1 x2\",\n",
       " 'order input w1 w2 addition feature values x1 x2',\n",
       " 'computational graph w1 x1 w2 x2 rectangular box',\n",
       " \"y_hat sigma_of_z 's step computation graph finally ay wo n't copy formula\",\n",
       " 'logistic regression modify parameters order reduce loss',\n",
       " \"propagation steps loss single training 's talk derivatives\",\n",
       " \"'s cleaned-up version diagram\",\n",
       " 'derivatives respect loss derivative loss respect script respect variable',\n",
       " 'code da denote variable',\n",
       " 'turns familiar calculus ends -y_over_a+1-y_over_1-a',\n",
       " \"formula loss 're familiar calculus derivative respect variable lowercase formula\",\n",
       " \"'re familiar calculus n't worry\",\n",
       " 'provide derivative formulas',\n",
       " \"expert calculus encourage formula loss previous slide derivative respect calculus n't calculus n't worry\",\n",
       " 'computed quantity da derivative final alpha variable respect',\n",
       " 'turns dz called variable derivative loss respect write loss including explicitly parameters',\n",
       " 'type notation equally acceptable',\n",
       " 'equal a-y',\n",
       " \"couple comments experts calculus 're expert calculus n't worry\",\n",
       " 'turns dl dz expressed dl_da_times_da_dz turns da dz turns a_times_1-a dl da worked quantities dl da term da dz term multiply',\n",
       " 'equation simplifies a-y',\n",
       " \"'s derive chain rule eluded form\",\n",
       " \"feel free calculation knowledgeable calculus n't dz a-y calculus\",\n",
       " 'final step computation change',\n",
       " 'derivative respect w1 quotes dw1 equal x1_times_dz',\n",
       " 'dw2 change w2 x2_times_dz excuse db equal dz',\n",
       " 'gradient descent respect formula dz formulas dw1 dw2 db perform updates',\n",
       " 'w1 updated w1 learning rate alpha times dw1',\n",
       " 'w2 updated set learning rate times db',\n",
       " 'step grade respect single',\n",
       " 'derivatives implement gradient descent logistic regression respect single training',\n",
       " 'training logistic regression model training training sets training examples',\n",
       " \"video 's ideas apply learning entire training set\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop_words 제거, lower()한 문장 리스트\n",
    "sents_after=[]\n",
    "for sent in sentences:\n",
    "    words = word_tokenize(sent)\n",
    "    sents_after.append(' '.join([word.lower() for word in words if word.lower() not in stop_words and len(word)>1]))\n",
    "    sents_after = [s for s in sents_after if s!='']\n",
    "sents_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF: 단어의 중요도 나타냄  \n",
    "TfidfVectorizer: 문장 단위로 TF-IDF 수치 벡터화한 matrix return  \n",
    "CountVectorizer: 단어 count를 기준으로 벡터화한 matrix return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
    "cnt_vec = CountVectorizer(ngram_range=(1, 3))\n",
    "graph_sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 리스트 -> tf-idf matrix -> sentence graph\n",
    "def build_sent_graph(sents):\n",
    "    tfidf_mat = tfidf.fit_transform(sents).toarray()\n",
    "    graph_sentence = np.dot(tfidf_mat, tfidf_mat.T)\n",
    "    return graph_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word graph 생성\n",
    "def build_word_graph(sent):\n",
    "    cnt_vec_mat = normalize(cnt_vec.fit_transform(sent).toarray().astype(float), axis=0)\n",
    "    vocab = cnt_vec.vocabulary_\n",
    "    graph_word = np.dot(cnt_vec_mat.T, cnt_vec_mat)\n",
    "    idx2word = {vocab[word] : word for word in vocab}\n",
    "    return graph_word, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.18706219, 0.5055462 , ..., 0.65789828, 0.09646828,\n",
       "        0.14247652],\n",
       "       [0.18706219, 1.        , 0.41530032, ..., 0.18186413, 0.        ,\n",
       "        0.        ],\n",
       "       [0.5055462 , 0.41530032, 1.        , ..., 0.49149814, 0.09101052,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.65789828, 0.18186413, 0.49149814, ..., 1.        , 0.40334796,\n",
       "        0.12068576],\n",
       "       [0.09646828, 0.        , 0.09101052, ..., 0.40334796, 1.        ,\n",
       "        0.28064357],\n",
       "       [0.14247652, 0.        , 0.        , ..., 0.12068576, 0.28064357,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_graph = build_sent_graph(sents_after)\n",
    "sent_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581, 581)\n",
      "{524: 'video', 465: 'talk', 113: 'derivatives', 274: 'implement', 253: 'gradient', 124: 'descent', 298: 'logistic', 392: 'regression', 529: 'video talk', 466: 'talk derivatives', 114: 'derivatives implement', 275: 'implement gradient', 254: 'gradient descent', 125: 'descent logistic', 299: 'logistic regression', 530: 'video talk derivatives', 467: 'talk derivatives implement', 115: 'derivatives implement gradient', 276: 'implement gradient descent', 255: 'gradient descent logistic', 126: 'descent logistic regression', 283: 'key', 463: 'takeaways', 286: 'key takeaways', 464: 'takeaways implement', 287: 'key takeaways implement', 182: 'equations', 336: 'order', 284: 'key equations', 183: 'equations order', 337: 'order implement', 285: 'key equations order', 184: 'equations order implement', 338: 'order implement gradient', 62: 'computation', 257: 'graph', 525: 'video computation', 64: 'computation computation', 66: 'computation graph', 526: 'video computation computation', 65: 'computation computation graph', 7: 'admit', 21: 'bit', 346: 'overkill', 121: 'deriving', 451: 'start', 196: 'explaining', 204: 'familiar', 269: 'ideas', 430: 'sense', 247: 'full', 228: 'fledged', 331: 'neural', 330: 'networks', 8: 'admit computation', 258: 'graph bit', 22: 'bit overkill', 347: 'overkill deriving', 122: 'deriving gradient', 403: 'regression start', 452: 'start explaining', 197: 'explaining familiar', 209: 'familiar ideas', 272: 'ideas bit', 24: 'bit sense', 431: 'sense talk', 468: 'talk full', 248: 'full fledged', 229: 'fledged neural', 332: 'neural networks', 9: 'admit computation graph', 67: 'computation graph bit', 259: 'graph bit overkill', 23: 'bit overkill deriving', 348: 'overkill deriving gradient', 123: 'deriving gradient descent', 305: 'logistic regression start', 404: 'regression start explaining', 453: 'start explaining familiar', 198: 'explaining familiar ideas', 210: 'familiar ideas bit', 273: 'ideas bit sense', 25: 'bit sense talk', 432: 'sense talk full', 469: 'talk full fledged', 249: 'full fledged neural', 230: 'fledged neural networks', 130: 'dive', 131: 'dive gradient', 132: 'dive gradient descent', 385: 'recap', 433: 'set', 354: 'predictions', 574: 'y_hat', 98: 'defined', 386: 'recap set', 436: 'set logistic', 399: 'regression predictions', 355: 'predictions y_hat', 575: 'y_hat defined', 387: 'recap set logistic', 437: 'set logistic regression', 303: 'logistic regression predictions', 400: 'regression predictions y_hat', 356: 'predictions y_hat defined', 231: 'focus', 306: 'loss', 405: 'respect', 343: 'output', 266: 'ground', 488: 'truth', 291: 'label', 232: 'focus loss', 315: 'loss respect', 408: 'respect defined', 99: 'defined output', 344: 'output logistic', 393: 'regression ground', 267: 'ground truth', 489: 'truth label', 233: 'focus loss respect', 316: 'loss respect defined', 409: 'respect defined output', 100: 'defined output logistic', 345: 'output logistic regression', 300: 'logistic regression ground', 394: 'regression ground truth', 268: 'ground truth label', 558: 'write', 214: 'features', 563: 'x1', 568: 'x2', 559: 'write computation', 260: 'graph features', 215: 'features x1', 566: 'x1 x2', 560: 'write computation graph', 68: 'computation graph features', 261: 'graph features x1', 216: 'features x1 x2', 280: 'input', 531: 'w1', 542: 'w2', 4: 'addition', 211: 'feature', 513: 'values', 339: 'order input', 281: 'input w1', 538: 'w1 w2', 543: 'w2 addition', 5: 'addition feature', 212: 'feature values', 514: 'values x1', 340: 'order input w1', 282: 'input w1 w2', 539: 'w1 w2 addition', 544: 'w2 addition feature', 6: 'addition feature values', 213: 'feature values x1', 515: 'values x1 x2', 70: 'computational', 388: 'rectangular', 26: 'box', 71: 'computational graph', 264: 'graph w1', 540: 'w1 x1', 564: 'x1 w2', 547: 'w2 x2', 569: 'x2 rectangular', 389: 'rectangular box', 72: 'computational graph w1', 265: 'graph w1 x1', 541: 'w1 x1 w2', 565: 'x1 w2 x2', 548: 'w2 x2 rectangular', 570: 'x2 rectangular box', 441: 'sigma_of_z', 454: 'step', 225: 'finally', 18: 'ay', 551: 'wo', 76: 'copy', 235: 'formula', 576: 'y_hat sigma_of_z', 442: 'sigma_of_z step', 455: 'step computation', 262: 'graph finally', 226: 'finally ay', 19: 'ay wo', 552: 'wo copy', 77: 'copy formula', 577: 'y_hat sigma_of_z step', 443: 'sigma_of_z step computation', 457: 'step computation graph', 69: 'computation graph finally', 263: 'graph finally ay', 227: 'finally ay wo', 20: 'ay wo copy', 553: 'wo copy formula', 326: 'modify', 349: 'parameters', 390: 'reduce', 397: 'regression modify', 327: 'modify parameters', 350: 'parameters order', 341: 'order reduce', 391: 'reduce loss', 302: 'logistic regression modify', 398: 'regression modify parameters', 328: 'modify parameters order', 351: 'parameters order reduce', 342: 'order reduce loss', 360: 'propagation', 460: 'steps', 445: 'single', 477: 'training', 361: 'propagation steps', 461: 'steps loss', 319: 'loss single', 446: 'single training', 484: 'training talk', 362: 'propagation steps loss', 462: 'steps loss single', 320: 'loss single training', 447: 'single training talk', 485: 'training talk derivatives', 53: 'cleaned', 504: 'up', 522: 'version', 129: 'diagram', 54: 'cleaned up', 505: 'up version', 523: 'version diagram', 55: 'cleaned up version', 506: 'up version diagram', 103: 'derivative', 427: 'script', 516: 'variable', 116: 'derivatives respect', 412: 'respect loss', 307: 'loss derivative', 107: 'derivative loss', 414: 'respect script', 428: 'script respect', 418: 'respect variable', 117: 'derivatives respect loss', 413: 'respect loss derivative', 308: 'loss derivative loss', 108: 'derivative loss respect', 317: 'loss respect script', 415: 'respect script respect', 429: 'script respect variable', 56: 'code', 81: 'da', 101: 'denote', 57: 'code da', 82: 'da denote', 102: 'denote variable', 58: 'code da denote', 83: 'da denote variable', 490: 'turns', 30: 'calculus', 169: 'ends', 579: 'y_over_a', 578: 'y_over_1', 499: 'turns familiar', 205: 'familiar calculus', 39: 'calculus ends', 170: 'ends y_over_a', 580: 'y_over_a y_over_1', 500: 'turns familiar calculus', 207: 'familiar calculus ends', 40: 'calculus ends y_over_a', 171: 'ends y_over_a y_over_1', 380: 're', 321: 'lowercase', 238: 'formula loss', 313: 'loss re', 383: 're familiar', 33: 'calculus derivative', 109: 'derivative respect', 519: 'variable lowercase', 322: 'lowercase formula', 240: 'formula loss re', 314: 'loss re familiar', 384: 're familiar calculus', 206: 'familiar calculus derivative', 34: 'calculus derivative respect', 111: 'derivative respect variable', 419: 'respect variable lowercase', 520: 'variable lowercase formula', 557: 'worry', 43: 'calculus worry', 208: 'familiar calculus worry', 363: 'provide', 241: 'formulas', 364: 'provide derivative', 106: 'derivative formulas', 365: 'provide derivative formulas', 189: 'expert', 166: 'encourage', 357: 'previous', 448: 'slide', 190: 'expert calculus', 37: 'calculus encourage', 167: 'encourage formula', 311: 'loss previous', 358: 'previous slide', 449: 'slide derivative', 406: 'respect calculus', 31: 'calculus calculus', 191: 'expert calculus encourage', 38: 'calculus encourage formula', 168: 'encourage formula loss', 239: 'formula loss previous', 312: 'loss previous slide', 359: 'previous slide derivative', 450: 'slide derivative respect', 110: 'derivative respect calculus', 407: 'respect calculus calculus', 32: 'calculus calculus worry', 73: 'computed', 369: 'quantity', 220: 'final', 10: 'alpha', 74: 'computed quantity', 370: 'quantity da', 84: 'da derivative', 104: 'derivative final', 221: 'final alpha', 13: 'alpha variable', 521: 'variable respect', 75: 'computed quantity da', 371: 'quantity da derivative', 85: 'da derivative final', 105: 'derivative final alpha', 222: 'final alpha variable', 14: 'alpha variable respect', 152: 'dz', 44: 'called', 277: 'including', 199: 'explicitly', 497: 'turns dz', 154: 'dz called', 45: 'called variable', 517: 'variable derivative', 422: 'respect write', 561: 'write loss', 309: 'loss including', 278: 'including explicitly', 200: 'explicitly parameters', 498: 'turns dz called', 155: 'dz called variable', 46: 'called variable derivative', 518: 'variable derivative loss', 318: 'loss respect write', 423: 'respect write loss', 562: 'write loss including', 310: 'loss including explicitly', 279: 'including explicitly parameters', 501: 'type', 333: 'notation', 178: 'equally', 3: 'acceptable', 502: 'type notation', 334: 'notation equally', 179: 'equally acceptable', 503: 'type notation equally', 335: 'notation equally acceptable', 175: 'equal', 78: 'couple', 59: 'comments', 193: 'experts', 79: 'couple comments', 60: 'comments experts', 194: 'experts calculus', 41: 'calculus re', 381: 're expert', 80: 'couple comments experts', 61: 'comments experts calculus', 195: 'experts calculus re', 42: 'calculus re expert', 382: 're expert calculus', 192: 'expert calculus worry', 133: 'dl', 201: 'expressed', 139: 'dl_da_times_da_dz', 0: 'a_times_1', 554: 'worked', 366: 'quantities', 470: 'term', 329: 'multiply', 495: 'turns dl', 137: 'dl dz', 156: 'dz expressed', 202: 'expressed dl_da_times_da_dz', 140: 'dl_da_times_da_dz turns', 493: 'turns da', 86: 'da dz', 162: 'dz turns', 491: 'turns a_times_1', 1: 'a_times_1 dl', 134: 'dl da', 91: 'da worked', 555: 'worked quantities', 367: 'quantities dl', 89: 'da term', 471: 'term da', 160: 'dz term', 473: 'term multiply', 496: 'turns dl dz', 138: 'dl dz expressed', 157: 'dz expressed dl_da_times_da_dz', 203: 'expressed dl_da_times_da_dz turns', 141: 'dl_da_times_da_dz turns da', 494: 'turns da dz', 88: 'da dz turns', 163: 'dz turns a_times_1', 492: 'turns a_times_1 dl', 2: 'a_times_1 dl da', 136: 'dl da worked', 92: 'da worked quantities', 556: 'worked quantities dl', 368: 'quantities dl da', 135: 'dl da term', 90: 'da term da', 472: 'term da dz', 87: 'da dz term', 161: 'dz term multiply', 180: 'equation', 444: 'simplifies', 181: 'equation simplifies', 118: 'derive', 47: 'chain', 424: 'rule', 164: 'eluded', 234: 'form', 119: 'derive chain', 48: 'chain rule', 425: 'rule eluded', 165: 'eluded form', 120: 'derive chain rule', 49: 'chain rule eluded', 426: 'rule eluded form', 217: 'feel', 244: 'free', 27: 'calculation', 288: 'knowledgeable', 218: 'feel free', 245: 'free calculation', 28: 'calculation knowledgeable', 289: 'knowledgeable calculus', 35: 'calculus dz', 153: 'dz calculus', 219: 'feel free calculation', 246: 'free calculation knowledgeable', 29: 'calculation knowledgeable calculus', 290: 'knowledgeable calculus dz', 36: 'calculus dz calculus', 50: 'change', 223: 'final step', 63: 'computation change', 224: 'final step computation', 456: 'step computation change', 372: 'quotes', 142: 'dw1', 567: 'x1_times_dz', 420: 'respect w1', 534: 'w1 quotes', 373: 'quotes dw1', 145: 'dw1 equal', 177: 'equal x1_times_dz', 112: 'derivative respect w1', 421: 'respect w1 quotes', 535: 'w1 quotes dw1', 374: 'quotes dw1 equal', 146: 'dw1 equal x1_times_dz', 147: 'dw2', 571: 'x2_times_dz', 186: 'excuse', 93: 'db', 148: 'dw2 change', 51: 'change w2', 549: 'w2 x2_times_dz', 572: 'x2_times_dz excuse', 187: 'excuse db', 94: 'db equal', 176: 'equal dz', 149: 'dw2 change w2', 52: 'change w2 x2_times_dz', 550: 'w2 x2_times_dz excuse', 573: 'x2_times_dz excuse db', 188: 'excuse db equal', 95: 'db equal dz', 352: 'perform', 512: 'updates', 127: 'descent respect', 410: 'respect formula', 236: 'formula dz', 158: 'dz formulas', 242: 'formulas dw1', 143: 'dw1 dw2', 150: 'dw2 db', 96: 'db perform', 353: 'perform updates', 256: 'gradient descent respect', 128: 'descent respect formula', 411: 'respect formula dz', 237: 'formula dz formulas', 159: 'dz formulas dw1', 243: 'formulas dw1 dw2', 144: 'dw1 dw2 db', 151: 'dw2 db perform', 97: 'db perform updates', 507: 'updated', 292: 'learning', 375: 'rate', 474: 'times', 536: 'w1 updated', 510: 'updated w1', 532: 'w1 learning', 295: 'learning rate', 376: 'rate alpha', 11: 'alpha times', 476: 'times dw1', 537: 'w1 updated w1', 511: 'updated w1 learning', 533: 'w1 learning rate', 296: 'learning rate alpha', 377: 'rate alpha times', 12: 'alpha times dw1', 545: 'w2 updated', 508: 'updated set', 434: 'set learning', 378: 'rate times', 475: 'times db', 546: 'w2 updated set', 509: 'updated set learning', 435: 'set learning rate', 297: 'learning rate times', 379: 'rate times db', 250: 'grade', 458: 'step grade', 251: 'grade respect', 416: 'respect single', 459: 'step grade respect', 252: 'grade respect single', 401: 'regression respect', 304: 'logistic regression respect', 402: 'regression respect single', 417: 'respect single training', 323: 'model', 438: 'sets', 185: 'examples', 479: 'training logistic', 395: 'regression model', 324: 'model training', 486: 'training training', 482: 'training sets', 439: 'sets training', 478: 'training examples', 480: 'training logistic regression', 301: 'logistic regression model', 396: 'regression model training', 325: 'model training training', 487: 'training training sets', 483: 'training sets training', 440: 'sets training examples', 15: 'apply', 172: 'entire', 527: 'video ideas', 270: 'ideas apply', 16: 'apply learning', 293: 'learning entire', 173: 'entire training', 481: 'training set', 528: 'video ideas apply', 271: 'ideas apply learning', 17: 'apply learning entire', 294: 'learning entire training', 174: 'entire training set'}\n"
     ]
    }
   ],
   "source": [
    "word_graph, idx2word = build_word_graph(sents_after)\n",
    "print(word_graph.shape)\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(graph, d=0.85):\n",
    "    A = graph\n",
    "    matrix_size = A.shape[0]\n",
    "    for id in range(matrix_size):\n",
    "        A[id, id] = 0   # diagonal 부분 -> 0으로 바꿔줌(diagonal matrix)\n",
    "        link_sum = np.sum(A[:, id])\n",
    "        if link_sum != 0:\n",
    "            A[:, id] /= link_sum\n",
    "        A[:, id] *= -d\n",
    "        A[id, id] = 1\n",
    "        \n",
    "    B = (1-d) * np.ones((matrix_size, 1))\n",
    "    ranks = np.linalg.solve(A, B)\n",
    "    \n",
    "    return {idx: r[0] for idx, r in enumerate(ranks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.602526076662568,\n",
       " 1: 0.45481217716176786,\n",
       " 2: 1.3921844584315919,\n",
       " 3: 0.9465557087297984,\n",
       " 4: 1.0276633567654725,\n",
       " 5: 1.1863467102567133,\n",
       " 6: 0.8232136675453178,\n",
       " 7: 1.0029128590705292,\n",
       " 8: 0.9152092026275391,\n",
       " 9: 0.772167339593344,\n",
       " 10: 0.8699249956073611,\n",
       " 11: 0.8125927761589898,\n",
       " 12: 1.0067684950665285,\n",
       " 13: 0.9665485546415963,\n",
       " 14: 0.15000000000000002,\n",
       " 15: 1.53322263380662,\n",
       " 16: 0.5311469055073599,\n",
       " 17: 0.7364226931447745,\n",
       " 18: 1.658016019645117,\n",
       " 19: 1.0236637326102713,\n",
       " 20: 0.5095809130856517,\n",
       " 21: 1.491111657176732,\n",
       " 22: 1.113467516717841,\n",
       " 23: 1.3415121419000944,\n",
       " 24: 0.15000000000000002,\n",
       " 25: 0.4864607633484701,\n",
       " 26: 0.8886424016922253,\n",
       " 27: 0.6377794775880602,\n",
       " 28: 0.15000000000000002,\n",
       " 29: 0.15000000000000002,\n",
       " 30: 0.8050680809337288,\n",
       " 31: 0.8262730822324702,\n",
       " 32: 1.2366408838125782,\n",
       " 33: 0.9446726227991267,\n",
       " 34: 1.3272242297113557,\n",
       " 35: 0.8744352440296118,\n",
       " 36: 0.8435502919822894,\n",
       " 37: 0.8409078278485468,\n",
       " 38: 2.0091180810776197,\n",
       " 39: 0.7983923173879451,\n",
       " 40: 0.763264103642396}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_rank_idx = get_ranks(sent_graph)  # 문장 가중치 그래프\n",
    "sent_rank_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[38,\n",
       " 18,\n",
       " 0,\n",
       " 15,\n",
       " 21,\n",
       " 2,\n",
       " 23,\n",
       " 34,\n",
       " 32,\n",
       " 5,\n",
       " 22,\n",
       " 4,\n",
       " 19,\n",
       " 12,\n",
       " 7,\n",
       " 13,\n",
       " 3,\n",
       " 33,\n",
       " 8,\n",
       " 26,\n",
       " 35,\n",
       " 10,\n",
       " 36,\n",
       " 37,\n",
       " 31,\n",
       " 6,\n",
       " 11,\n",
       " 30,\n",
       " 39,\n",
       " 9,\n",
       " 40,\n",
       " 17,\n",
       " 27,\n",
       " 16,\n",
       " 20,\n",
       " 25,\n",
       " 1,\n",
       " 14,\n",
       " 24,\n",
       " 28,\n",
       " 29]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_sent_idx = sorted(sent_rank_idx, key=lambda k: sent_rank_idx[k], reverse=True)\n",
    "sorted_sent_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[405,\n",
       " 298,\n",
       " 299,\n",
       " 392,\n",
       " 103,\n",
       " 306,\n",
       " 152,\n",
       " 516,\n",
       " 257,\n",
       " 30,\n",
       " 124,\n",
       " 253,\n",
       " 254,\n",
       " 235,\n",
       " 542,\n",
       " 66,\n",
       " 531,\n",
       " 204,\n",
       " 315,\n",
       " 109,\n",
       " 93,\n",
       " 269,\n",
       " 142,\n",
       " 255,\n",
       " 126,\n",
       " 125,\n",
       " 465,\n",
       " 62,\n",
       " 490,\n",
       " 292,\n",
       " 568,\n",
       " 563,\n",
       " 81,\n",
       " 433,\n",
       " 330,\n",
       " 453,\n",
       " 7,\n",
       " 228,\n",
       " 305,\n",
       " 468,\n",
       " 229,\n",
       " 259,\n",
       " 272,\n",
       " 331,\n",
       " 332,\n",
       " 404,\n",
       " 469,\n",
       " 121,\n",
       " 123,\n",
       " 247,\n",
       " 249,\n",
       " 273,\n",
       " 431,\n",
       " 432,\n",
       " 452,\n",
       " 25,\n",
       " 122,\n",
       " 198,\n",
       " 230,\n",
       " 403,\n",
       " 8,\n",
       " 9,\n",
       " 21,\n",
       " 22,\n",
       " 24,\n",
       " 197,\n",
       " 209,\n",
       " 248,\n",
       " 347,\n",
       " 67,\n",
       " 196,\n",
       " 258,\n",
       " 430,\n",
       " 451,\n",
       " 23,\n",
       " 210,\n",
       " 346,\n",
       " 348,\n",
       " 336,\n",
       " 238,\n",
       " 113,\n",
       " 147,\n",
       " 189,\n",
       " 190,\n",
       " 477,\n",
       " 108,\n",
       " 107,\n",
       " 349,\n",
       " 10,\n",
       " 574,\n",
       " 557,\n",
       " 43,\n",
       " 98,\n",
       " 375,\n",
       " 507,\n",
       " 474,\n",
       " 295,\n",
       " 418,\n",
       " 558,\n",
       " 454,\n",
       " 274,\n",
       " 380,\n",
       " 566,\n",
       " 445,\n",
       " 205,\n",
       " 241,\n",
       " 202,\n",
       " 163,\n",
       " 135,\n",
       " 136,\n",
       " 156,\n",
       " 157,\n",
       " 161,\n",
       " 472,\n",
       " 0,\n",
       " 87,\n",
       " 91,\n",
       " 160,\n",
       " 201,\n",
       " 471,\n",
       " 1,\n",
       " 2,\n",
       " 88,\n",
       " 89,\n",
       " 92,\n",
       " 141,\n",
       " 329,\n",
       " 366,\n",
       " 470,\n",
       " 493,\n",
       " 86,\n",
       " 90,\n",
       " 134,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 162,\n",
       " 473,\n",
       " 491,\n",
       " 496,\n",
       " 137,\n",
       " 367,\n",
       " 494,\n",
       " 495,\n",
       " 555,\n",
       " 133,\n",
       " 492,\n",
       " 556,\n",
       " 554,\n",
       " 203,\n",
       " 368,\n",
       " 110,\n",
       " 167,\n",
       " 407,\n",
       " 32,\n",
       " 168,\n",
       " 311,\n",
       " 357,\n",
       " 31,\n",
       " 37,\n",
       " 38,\n",
       " 239,\n",
       " 312,\n",
       " 406,\n",
       " 448,\n",
       " 450,\n",
       " 166,\n",
       " 191,\n",
       " 358,\n",
       " 449,\n",
       " 359,\n",
       " 199,\n",
       " 318,\n",
       " 561,\n",
       " 562,\n",
       " 45,\n",
       " 155,\n",
       " 200,\n",
       " 423,\n",
       " 498,\n",
       " 517,\n",
       " 518,\n",
       " 44,\n",
       " 46,\n",
       " 277,\n",
       " 279,\n",
       " 309,\n",
       " 310,\n",
       " 497,\n",
       " 154,\n",
       " 278,\n",
       " 422,\n",
       " 455,\n",
       " 276,\n",
       " 275,\n",
       " 524,\n",
       " 256,\n",
       " 353,\n",
       " 512,\n",
       " 97,\n",
       " 150,\n",
       " 128,\n",
       " 352,\n",
       " 127,\n",
       " 151,\n",
       " 158,\n",
       " 159,\n",
       " 236,\n",
       " 243,\n",
       " 410,\n",
       " 96,\n",
       " 143,\n",
       " 144,\n",
       " 237,\n",
       " 242,\n",
       " 411,\n",
       " 77,\n",
       " 225,\n",
       " 551,\n",
       " 76,\n",
       " 442,\n",
       " 553,\n",
       " 576,\n",
       " 577,\n",
       " 69,\n",
       " 263,\n",
       " 443,\n",
       " 18,\n",
       " 19,\n",
       " 262,\n",
       " 441,\n",
       " 457,\n",
       " 552,\n",
       " 20,\n",
       " 226,\n",
       " 227,\n",
       " 175,\n",
       " 539,\n",
       " 339,\n",
       " 543,\n",
       " 514,\n",
       " 515,\n",
       " 513,\n",
       " 538,\n",
       " 544,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 280,\n",
       " 282,\n",
       " 340,\n",
       " 281,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 119,\n",
       " 164,\n",
       " 165,\n",
       " 234,\n",
       " 425,\n",
       " 426,\n",
       " 118,\n",
       " 120,\n",
       " 424,\n",
       " 506,\n",
       " 523,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 129,\n",
       " 504,\n",
       " 505,\n",
       " 522,\n",
       " 180,\n",
       " 181,\n",
       " 444,\n",
       " 334,\n",
       " 3,\n",
       " 333,\n",
       " 335,\n",
       " 178,\n",
       " 179,\n",
       " 501,\n",
       " 502,\n",
       " 503,\n",
       " 446,\n",
       " 50,\n",
       " 220,\n",
       " 409,\n",
       " 489,\n",
       " 232,\n",
       " 267,\n",
       " 291,\n",
       " 394,\n",
       " 99,\n",
       " 266,\n",
       " 268,\n",
       " 488,\n",
       " 100,\n",
       " 233,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 393,\n",
       " 408,\n",
       " 231,\n",
       " 300,\n",
       " 316,\n",
       " 466,\n",
       " 325,\n",
       " 440,\n",
       " 482,\n",
       " 323,\n",
       " 324,\n",
       " 396,\n",
       " 438,\n",
       " 185,\n",
       " 439,\n",
       " 478,\n",
       " 483,\n",
       " 301,\n",
       " 486,\n",
       " 479,\n",
       " 487,\n",
       " 480,\n",
       " 395,\n",
       " 389,\n",
       " 540,\n",
       " 264,\n",
       " 265,\n",
       " 70,\n",
       " 541,\n",
       " 548,\n",
       " 569,\n",
       " 26,\n",
       " 71,\n",
       " 72,\n",
       " 564,\n",
       " 570,\n",
       " 565,\n",
       " 388,\n",
       " 547,\n",
       " 94,\n",
       " 51,\n",
       " 149,\n",
       " 52,\n",
       " 95,\n",
       " 186,\n",
       " 549,\n",
       " 571,\n",
       " 550,\n",
       " 572,\n",
       " 573,\n",
       " 148,\n",
       " 188,\n",
       " 187,\n",
       " 176,\n",
       " 369,\n",
       " 222,\n",
       " 370,\n",
       " 85,\n",
       " 14,\n",
       " 73,\n",
       " 75,\n",
       " 84,\n",
       " 105,\n",
       " 13,\n",
       " 74,\n",
       " 104,\n",
       " 371,\n",
       " 521,\n",
       " 221,\n",
       " 35,\n",
       " 153,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 36,\n",
       " 246,\n",
       " 290,\n",
       " 217,\n",
       " 219,\n",
       " 245,\n",
       " 289,\n",
       " 218,\n",
       " 244,\n",
       " 288,\n",
       " 536,\n",
       " 12,\n",
       " 532,\n",
       " 11,\n",
       " 476,\n",
       " 377,\n",
       " 376,\n",
       " 510,\n",
       " 533,\n",
       " 511,\n",
       " 296,\n",
       " 537,\n",
       " 206,\n",
       " 322,\n",
       " 519,\n",
       " 314,\n",
       " 240,\n",
       " 33,\n",
       " 34,\n",
       " 111,\n",
       " 321,\n",
       " 419,\n",
       " 520,\n",
       " 313,\n",
       " 42,\n",
       " 60,\n",
       " 381,\n",
       " 41,\n",
       " 59,\n",
       " 79,\n",
       " 382,\n",
       " 61,\n",
       " 78,\n",
       " 195,\n",
       " 80,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 283,\n",
       " 528,\n",
       " 293,\n",
       " 15,\n",
       " 17,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 527,\n",
       " 16,\n",
       " 294,\n",
       " 481,\n",
       " 270,\n",
       " 271,\n",
       " 416,\n",
       " 384,\n",
       " 383,\n",
       " 114,\n",
       " 115,\n",
       " 475,\n",
       " 509,\n",
       " 546,\n",
       " 379,\n",
       " 508,\n",
       " 545,\n",
       " 435,\n",
       " 297,\n",
       " 378,\n",
       " 434,\n",
       " 177,\n",
       " 145,\n",
       " 420,\n",
       " 421,\n",
       " 535,\n",
       " 112,\n",
       " 373,\n",
       " 374,\n",
       " 534,\n",
       " 372,\n",
       " 146,\n",
       " 567,\n",
       " 429,\n",
       " 415,\n",
       " 116,\n",
       " 117,\n",
       " 414,\n",
       " 427,\n",
       " 308,\n",
       " 317,\n",
       " 413,\n",
       " 412,\n",
       " 307,\n",
       " 428,\n",
       " 303,\n",
       " 437,\n",
       " 354,\n",
       " 355,\n",
       " 385,\n",
       " 400,\n",
       " 386,\n",
       " 387,\n",
       " 356,\n",
       " 399,\n",
       " 436,\n",
       " 575,\n",
       " 484,\n",
       " 320,\n",
       " 447,\n",
       " 319,\n",
       " 361,\n",
       " 362,\n",
       " 461,\n",
       " 462,\n",
       " 485,\n",
       " 360,\n",
       " 460,\n",
       " 326,\n",
       " 342,\n",
       " 390,\n",
       " 327,\n",
       " 328,\n",
       " 391,\n",
       " 302,\n",
       " 398,\n",
       " 341,\n",
       " 351,\n",
       " 397,\n",
       " 350,\n",
       " 40,\n",
       " 171,\n",
       " 207,\n",
       " 170,\n",
       " 579,\n",
       " 499,\n",
       " 578,\n",
       " 169,\n",
       " 580,\n",
       " 39,\n",
       " 500,\n",
       " 102,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 101,\n",
       " 82,\n",
       " 83,\n",
       " 338,\n",
       " 184,\n",
       " 182,\n",
       " 284,\n",
       " 183,\n",
       " 337,\n",
       " 285,\n",
       " 417,\n",
       " 304,\n",
       " 401,\n",
       " 402,\n",
       " 215,\n",
       " 214,\n",
       " 260,\n",
       " 559,\n",
       " 216,\n",
       " 68,\n",
       " 560,\n",
       " 261,\n",
       " 530,\n",
       " 467,\n",
       " 529,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 458,\n",
       " 459,\n",
       " 464,\n",
       " 463,\n",
       " 286,\n",
       " 287,\n",
       " 106,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 456,\n",
       " 224,\n",
       " 223,\n",
       " 63,\n",
       " 525,\n",
       " 65,\n",
       " 64,\n",
       " 526,\n",
       " 130,\n",
       " 132,\n",
       " 131,\n",
       " 208]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rank_idx = get_ranks(word_graph)\n",
    "sorted_word_idx = sorted(word_rank_idx, key=lambda k: word_rank_idx[k], reverse=True)\n",
    "sorted_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords(sorted_word_idx, idx2word, word_num=5):\n",
    "    keywords = []\n",
    "    index = []\n",
    "    for idx in sorted_word_idx[:word_num]:\n",
    "        index.append(idx)      \n",
    "    for idx in index:\n",
    "        keywords.append(idx2word[idx])\n",
    "        \n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사만 추출할 수는 없나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['respect',\n",
       " 'logistic',\n",
       " 'logistic regression',\n",
       " 'regression',\n",
       " 'derivative',\n",
       " 'loss',\n",
       " 'dz',\n",
       " 'variable',\n",
       " 'graph',\n",
       " 'calculus']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = keywords(sorted_word_idx, idx2word, 10)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keysents(sorted_sent_idx, sent_num=2):\n",
    "    keysents=[]\n",
    "    index=[]\n",
    "    for idx in sorted_sent_idx[:sent_num]:\n",
    "        index.append(idx)\n",
    "    for idx in index:\n",
    "        keysents.append(sentences[idx])\n",
    "\n",
    "    return keysents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['And so, this will be one step of grade with respect to a single example.',\n",
       " 'It turns out that if you are familiar with calculus, you could show that this ends up being -Y_over_A+1-Y_over_1-A.',\n",
       " 'Welcome back.',\n",
       " \"Here's a cleaned-up version of the diagram.\",\n",
       " \"We'll provide the derivative formulas, what else you need, throughout this course.\",\n",
       " 'The key takeaways will be what you need to implement.',\n",
       " 'Now, having computed this quantity of DA and the derivative or your final alpha variable with respect to A, you can then go backwards.',\n",
       " 'Then, similarly, DW2, which is how much you want to change W2, is X2_times_DZ and B, excuse me, DB is equal to DZ.',\n",
       " 'Then, the final step in that computation is to go back to compute how much you need to change W and B.',\n",
       " 'I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about full-fledged neural networks.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysents = keysents(sorted_sent_idx, sent_num=10)\n",
    "keysents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keysents는 여러 후보 중에서 keywords가 많은 문장을 선택하는 방향으로?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. keyword \n",
    "    - return keywords, keyword 순위\n",
    "2. keysents\n",
    "    - return keysents, keysents with blank(keyword), keyword in blank\n",
    "    - keysents 1위부터 돌면서 weight=(각 keysents의 순위 + 갖고 있는 keyword 순위(다수라면 더 낮은 keyword 순위))\n",
    "    - weight가 같은 keysents라면 자체 keysents 순위가 더 높은 keysents 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keysents_blank(keywords:list, keysents:list):\n",
    "    keysent=''   # blank 만들 keysent\n",
    "    keysent_blank=''   # blank 만든 keysent\n",
    "    keyword_keysent=''   # keysent의 blank에 들어갈 keyword\n",
    "    lowest_weight=23   # 가장 작은 weight(초기값: 최대 weight+1)\n",
    "    \n",
    "    for sent in keysents:\n",
    "        sent_weight = keysents.index(sent) + 1 \n",
    "        \n",
    "        keyword=''\n",
    "        for word in keywords:\n",
    "            if word in sent:\n",
    "                keyword = word\n",
    "                break   # keywords 리스트는 앞의 index일수록 순위가 높은 키워드 -> 문장에 존재하면 break    \n",
    "        if keyword!='':\n",
    "            word_weight = keywords.index(keyword) + 1\n",
    "        else:\n",
    "            word_weight = 23\n",
    "            \n",
    "        weight = sent_weight + word_weight\n",
    "        if weight<lowest_weight:\n",
    "            lowest_weight = weight\n",
    "            keysent = sent\n",
    "            keyword_keysent = keyword\n",
    "    \n",
    "    keysent_blank = keysent.replace(keyword_keysent, '__________')\n",
    "    \n",
    "    return {'sentence_blank':keysent_blank, 'sentence':keysent, 'answer':keyword_keysent}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_blank': 'And so, this will be one step of grade with __________ to a single example.',\n",
       " 'sentence': 'And so, this will be one step of grade with respect to a single example.',\n",
       " 'answer': 'respect'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keysents_blank(keywords, keysents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
