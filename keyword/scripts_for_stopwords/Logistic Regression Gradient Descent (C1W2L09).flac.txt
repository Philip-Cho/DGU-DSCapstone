Welcome back in this video was talked about how to compute the rivet is for you to implement Drayden to send for the electric Russian. The key takeaways will be what you need to intimate. That is the key equation you need in order to implement trading to send photos of aggression, but I want to do this computation using the computation drop. I have to drop is a little bit of an overkill for deriving Brayden to send for logistic regression, but want to start explaining things, this way to catch up the mother of these ideas so that hopefully will make a bit more sense when we talked about. Your network, does dive into great into send for logistic regression. Heavy cough, we had set up logistic regression as follows. Your predictions. Y hats is defined as follows where Z is that. And if we focus on just one example for now, then the loss or expected, that one example is defined as follows. Where is the output of logistic regression and why is the ground troops able? So, let's write this out as a computation draft. And for this example, that say we have only two features X1 and X2. So in order to compute do you will need to input w1w to has be in addition to the Future values. X1 X2. So these things in a competition golf can't use the computers. You witches W 1 x 1 + W, 2 x 2, + B, draw a tackle box around that and then we can shoot y hats or a equals 6, Mugsy. That's the next step in the computation draw. And then finally we compute l a y and I will copy the phone with the game. So in logistic regression, what we want to do is to modify the parameters w&b in order to reduce this loss with this guy before propagation steps of how you ask, you compute the lost on a single Chinese Apple. Now, let's talk about how you can. Go backwards to talk to compute the derivative. She is a cleaned-up version of the diagram because what we want to do is compute the riveters respect to this loss. The first thing we want to do and going back with since to compute the derivative of this loss with respect to the script over there. And we respect to this terrible a and sew-in in the cold, you know, you just use the right to denote this variable and it turns out that if you are familiar with Calculus, you could show that this ends up being next to the Y over x + 1 - y over. 1 - 8 and the way you do that, if you take the formula for the loss and if you have to live in calculus, you can compute the derivative for this back to the airport, lowercase. A, and you get this phone's up. But if you know pin, your Calculus don't worry about it. We will provide the derivative formulas, you need throughout the school. So if you are, they think how cool is your than cause you to look up the formula for the lost from their previous slide in traffic and director for this recipe using, you know, calculus, but if you don't know enough. Don't worry about it. Now, having computed just going to the DA is a derivative of help available respect to a. You can then go backwards and it turns out that you can show Daz which this is how I think of the Apple named is going to be the derivative of the lost respect to D4L. You can read it right to the loss including and why explicit parameters are not eating either. Notation is equally acceptable TV show. That doesn't equal to a minus. Why just a couple comments only for those of you to expect in Calculus. If you're not explain calculus, don't worry about it. That turns out that this right d l t, z just can't be stressed as the l, d, a x t, a d z. And it turns out that the ADC, this turns out to be a * 1 - 8 and d l g a b. A previously worked out over here. And so if you take these two quantities, you do you, how do you say you should just tell him together with dadz? I'm just this time and just take these two things that multiply them. You can show that you to the equation, simplifies 28 - why. So, that's how you do. The chain rule that I briefed, you do this to him for a few for you to go through that. Calculation yourself, if you Knowledge about calculus, but if you aren't all, you need to know is that you can come shoot these. That's a - white and has already done that calculus for you. And then the final step in backpropagation is to go back to compute how much you need to change W and Pete. So in particular, you can show that the derivative respect to W1 and inco will call this dw1. That this is equal to x 1 x easy. And then some of these, ew, to me how much you want to change. W2 is x 2 x d, z & P C C, D, P equal to Dez. So if you want to do great in to send will respect you just as one example of what you would do is to follow. You would use this formula to compute easy. And then use these formulas to compute gw1, gw2 and DP. And then you perform these updates. Everyone gets updated. Tell me one. Mine. Is there any real fur x d w? 1 W to get updated similar, B&B getsetaz b-minus to learning rate. Times has to be one step equation with respect to a single example. So, you seen how to compute derivatives and Implement Brayden to Sanford. Logistic regression with respect to a single Chinese Apple, but touch any of this aggression, although you have not just one that trained example, given Thai training, set of pemmican examples. So in the next video, let's see how you can take these ideas and apply them to learning. Not just from one example, for 4 minutes, High training set. 